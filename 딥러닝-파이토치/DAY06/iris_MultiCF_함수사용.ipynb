{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN 기반 다중분류 모델 구현\n",
    "- 데이터셋 : iris.csv\n",
    "- Feature : 4개 Sepal_Length, Sepal_Width, Petal_Length, Petal_Width\n",
    "- Target : 1개 Variety\n",
    "- 학습-방법 : 지도학습 > 분류 > 다중분류 (클래스 3개)\n",
    "- 알고리즘 : 인공신경망(ANN) => MLP, DNN : 은닉층이 많은 구성\n",
    "- 프레임워크 : Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 및 데이터 준비 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 관련 모듈 로딩\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torchinfo import summary\n",
    "\n",
    "# Data 관련 모듈 로딩\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch V. 2.4.1\n",
      "Pandas V. 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# 활용 패키지 버전 체크 => 사용자 정의 함수로 구현하기\n",
    "print(f'Pytorch V. {torch.__version__}')\n",
    "print(f'Pandas V. {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width variety\n",
       "0           5.1          3.5           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로딩\n",
    "DATA_FILE='../data/iris.csv'\n",
    "\n",
    "# CSV => DataFrame\n",
    "irisDF=pd.read_csv(DATA_FILE)\n",
    "\n",
    "# 데이터 확인\n",
    "irisDF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Virginica'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타겟 변경 => 정수화, 클래스 3개\n",
    "irisDF['variety'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels => {'Setosa': 0, 'Versicolor': 1, 'Virginica': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0           5.1          3.5           1.4          0.2        0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=dict(zip(irisDF['variety'].unique().tolist(),range(3)))\n",
    "print(f'labels => {labels}')\n",
    "\n",
    "irisDF['variety']=irisDF['variety'].replace(labels)\n",
    "irisDF.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 모델 클래스 설계 및 정의<hr>\n",
    "- 클래스목적 : iris 데이터를 학습 및 추론 목적 \n",
    "- 클래스이름 : IrisMCFModel\n",
    "- 부모클래스 : nn.Module\n",
    "- 매개변수 : 층별 입출력 개수 고정하기때문에 필요 없음\n",
    "- 속성필드 : \n",
    "- 기능역할 : __init__() : 모델 구조, forward() : 순방향 학습 <= 오버라이딩\n",
    "- 클래스구조\n",
    "    * 입력층 : 입력  4개(피처)  출력 10개(퍼셉트론/뉴런 10개 존재)\n",
    "    * 은닉층 : 입력 10개        출력 5개(퍼셉트론/뉴런 5개 존재)\n",
    "    * 출력층 : 입력  5개        출력 1개(퍼셉트론/뉴런 1개 존재 : 2진분류)\n",
    "\n",
    "- 활성화함수\n",
    "    * 클래스형태 => nn.MESLoss, nn.ReLU => __init__() 메서드\n",
    "    * 함수형태 => torch.nn.fuctional 아래에 => forward() 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisMCFModel(nn.Module):\n",
    "\n",
    "    # 모델 구조 구성 및 인스턴스 생성 메서드\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer=nn.Linear(4,10)\n",
    "        self.hd_layer=nn.Linear(10,5)\n",
    "        self.out_layer=nn.Linear(5,3) # 다중분류 'Setosa', 'Versicolor', 'Virginica' \n",
    "\n",
    "    # 순방향 학습 진행 메서드\n",
    "    def forward(self, x):\n",
    "        y=F.relu(self.in_layer(x))\n",
    "        y=F.relu(self.hd_layer(y))\n",
    "        return self.out_layer(y) # 5개의 숫자 값 => 다중분류 : 손실함수 CrossEntrpyLoss가 내부에서 softmax 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisMCFModel(\n",
      "  (in_layer): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (hd_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=5, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스 생성\n",
    "model=IrisMCFModel()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "IrisMCFModel                             [1000, 3]                 --\n",
       "├─Linear: 1-1                            [1000, 10]                50\n",
       "├─Linear: 1-2                            [1000, 5]                 55\n",
       "├─Linear: 1-3                            [1000, 3]                 18\n",
       "==========================================================================================\n",
       "Total params: 123\n",
       "Trainable params: 123\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.12\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.14\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 사용 메모리 정보 확인\n",
    "summary(model, input_size=(1000,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 데이터셋 클래스 설계 및 정의<hr>\n",
    "- 데이터셋 : iris.csv\n",
    "- 피쳐개수 : 4개\n",
    "- 타겟개수 : 1개\n",
    "- 클래스이름 : IrisDataset\n",
    "- 부모클래스 : utils.data.Dataset\n",
    "- 속성필드 : featureDF, targetDF, n_rows, n_features\n",
    "- 필수메서드\n",
    "    * __init__(self) : 데이터셋 저장 및 전처리, 개발자가 필요한 속성 설정\n",
    "    * __len__(self) : 데이터의 개수 반환\n",
    "    * __getitem__(self, index) : 특정 인덱스의 피쳐와 타겟 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, featureDF, targetDF):\n",
    "        self.featureDF=featureDF\n",
    "        self.targetDF=targetDF\n",
    "        self.n_rows=featureDF.shape[0]\n",
    "        self.n_features=featureDF.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 넘파이를 텐서로\n",
    "        featureTS=torch.FloatTensor(self.featureDF.iloc[index].values)\n",
    "        targetTS=torch.FloatTensor(self.targetDF.iloc[index].values)        \n",
    "        # 피쳐와 타겟 반환\n",
    "        return featureTS, targetTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3-1] 데이터셋 인스턴스 생성 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureDF => (150, 4), targetDF => (150, 1)\n"
     ]
    }
   ],
   "source": [
    "# 피쳐, 타겟 추출\n",
    "featureDF, targetDF=irisDF[irisDF.columns[:-1]], irisDF[irisDF.columns[-1:]]\n",
    "print(f'featureDF => {featureDF.shape}, targetDF => {targetDF.shape}')\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "irisDS=IrisDataset(featureDF, targetDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 학습 준비\n",
    "- 학습 횟수 : EPOCH <= 처음부터 끝까지 학습하는 단위\n",
    "- 배치 크기 : BATCH_SIZE <= 한번에 학습할 데이터셋 양\n",
    "- 위치 지정 : DEVICE <= 텐서 저장 및 실행 위치 (GPU/CPU)\n",
    "- 학습률 : LR 가중치와 절편 업데이트 시 경사하강법으로 업데이트 간격 설정 0.001~0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 진행 관련 설정\n",
    "EPOCH=1000\n",
    "BATCH_SIZE=10\n",
    "BATCH_CNT=irisDF.shape[0]/BATCH_SIZE\n",
    "DEVICE= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인스턴스/객체 : 모델, 데이터셋, 최적화 (+ 손실함수, 성능지표)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 4) (38, 4) (28, 4)\n",
      "(84, 1) (38, 1) (28, 1)\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스\n",
    "model=IrisMCFModel()\n",
    "\n",
    "# 데이터셋 인스턴스\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터 분리\n",
    "X_train, X_test, y_train, y_test=train_test_split(featureDF, targetDF, random_state=1)\n",
    "X_train, X_val, y_train, y_val=train_test_split(X_train, y_train, random_state=1)\n",
    "print(f'{X_train.shape} {X_test.shape} {X_val.shape}')\n",
    "print(f'{y_train.shape} {y_test.shape} {y_val.shape}')\n",
    "\n",
    "trainDS=IrisDataset(X_train, y_train)\n",
    "valDS=IrisDataset(X_val, y_val)\n",
    "testDS=IrisDataset(X_test, y_test)\n",
    "\n",
    "# 데이터로드 인스턴스\n",
    "trainDL=DataLoader(trainDS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화, 손실함수 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 인스턴스 => W, b 텐서 즉, model.parameters() 전달\n",
    "optimizer=optim.Adam(model.parameters(),lr=LR)\n",
    "\n",
    "# 손실함수 인스턴스 => 분류 => 다중분류 CrossEntropyLoss\n",
    "#                            예측값은 선형식 결과값으로 전달 => AF 처리 X\n",
    "crossLoss=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 파일 관련\n",
    "### models 폴더 아래 프로젝트 폴더 아래 모델 파일 저장\n",
    "import os\n",
    "\n",
    "# 저장 경로\n",
    "SAVE_PATH = '../models/iris/MCF/'\n",
    "# 저장 파일명\n",
    "SAVE_FILE=SAVE_PATH+'model_train_wbs.pth'\n",
    "\n",
    "# 모델 구조 및 파라미터 모두 저장 파일명명\n",
    "SAVE_MODEL=SAVE_PATH+'model_all.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로상 폴더 존재 여부 체크\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)   # 폴더 / 폴더/.. 하위폴더까지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000]\n",
      "- [TRAIN] LOSS : 0.6295751730600992 [SCORE] : 0.1039793555935224\n",
      "[1/1000]\n",
      "- [VAL] LOSS : 1.0636123418807983 [SCORE] : 0.18803419172763824\n",
      "[2/1000]\n",
      "- [TRAIN] LOSS : 0.622826095422109 [SCORE] : 0.1039793555935224\n",
      "[2/1000]\n",
      "- [VAL] LOSS : 1.0555431842803955 [SCORE] : 0.18803419172763824\n",
      "[3/1000]\n",
      "- [TRAIN] LOSS : 0.6179781476656596 [SCORE] : 0.1039793555935224\n",
      "[3/1000]\n",
      "- [VAL] LOSS : 1.0464670658111572 [SCORE] : 0.18803419172763824\n",
      "[4/1000]\n",
      "- [TRAIN] LOSS : 0.6136249423027038 [SCORE] : 0.1039793555935224\n",
      "[4/1000]\n",
      "- [VAL] LOSS : 1.0373951196670532 [SCORE] : 0.18803419172763824\n",
      "[5/1000]\n",
      "- [TRAIN] LOSS : 0.6095952351888021 [SCORE] : 0.1039793555935224\n",
      "[5/1000]\n",
      "- [VAL] LOSS : 1.0290838479995728 [SCORE] : 0.18803419172763824\n",
      "[6/1000]\n",
      "- [TRAIN] LOSS : 0.60563036998113 [SCORE] : 0.1039793555935224\n",
      "[6/1000]\n",
      "- [VAL] LOSS : 1.021314263343811 [SCORE] : 0.18803419172763824\n",
      "[7/1000]\n",
      "- [TRAIN] LOSS : 0.6015904347101847 [SCORE] : 0.1039793555935224\n",
      "[7/1000]\n",
      "- [VAL] LOSS : 1.0132111310958862 [SCORE] : 0.18803419172763824\n",
      "[8/1000]\n",
      "- [TRAIN] LOSS : 0.597436233361562 [SCORE] : 0.1039793555935224\n",
      "[8/1000]\n",
      "- [VAL] LOSS : 1.0044575929641724 [SCORE] : 0.18803419172763824\n",
      "[9/1000]\n",
      "- [TRAIN] LOSS : 0.5932460268338521 [SCORE] : 0.1039793555935224\n",
      "[9/1000]\n",
      "- [VAL] LOSS : 0.9958635568618774 [SCORE] : 0.18803419172763824\n",
      "[10/1000]\n",
      "- [TRAIN] LOSS : 0.5888364473978679 [SCORE] : 0.1039793555935224\n",
      "[10/1000]\n",
      "- [VAL] LOSS : 0.9864888787269592 [SCORE] : 0.18803419172763824\n",
      "[11/1000]\n",
      "- [TRAIN] LOSS : 0.5842442353566487 [SCORE] : 0.1039793555935224\n",
      "[11/1000]\n",
      "- [VAL] LOSS : 0.976313054561615 [SCORE] : 0.18803419172763824\n",
      "[12/1000]\n",
      "- [TRAIN] LOSS : 0.5795841336250305 [SCORE] : 0.1039793555935224\n",
      "[12/1000]\n",
      "- [VAL] LOSS : 0.9661555290222168 [SCORE] : 0.18803419172763824\n",
      "[13/1000]\n",
      "- [TRAIN] LOSS : 0.5747152924537658 [SCORE] : 0.1039793555935224\n",
      "[13/1000]\n",
      "- [VAL] LOSS : 0.9555522799491882 [SCORE] : 0.18803419172763824\n",
      "[14/1000]\n",
      "- [TRAIN] LOSS : 0.5696359038352966 [SCORE] : 0.1039793555935224\n",
      "[14/1000]\n",
      "- [VAL] LOSS : 0.9441216588020325 [SCORE] : 0.18803419172763824\n",
      "[15/1000]\n",
      "- [TRAIN] LOSS : 0.5643471240997314 [SCORE] : 0.1039793555935224\n",
      "[15/1000]\n",
      "- [VAL] LOSS : 0.9317746162414551 [SCORE] : 0.18803419172763824\n",
      "[16/1000]\n",
      "- [TRAIN] LOSS : 0.5589089115460714 [SCORE] : 0.1039793555935224\n",
      "[16/1000]\n",
      "- [VAL] LOSS : 0.9196388125419617 [SCORE] : 0.18803419172763824\n",
      "[17/1000]\n",
      "- [TRAIN] LOSS : 0.5533356348673503 [SCORE] : 0.1039793555935224\n",
      "[17/1000]\n",
      "- [VAL] LOSS : 0.907805323600769 [SCORE] : 0.18803419172763824\n",
      "[18/1000]\n",
      "- [TRAIN] LOSS : 0.5474222739537556 [SCORE] : 0.12717838039000828\n",
      "[18/1000]\n",
      "- [VAL] LOSS : 0.8943314552307129 [SCORE] : 0.24426449835300446\n",
      "[19/1000]\n",
      "- [TRAIN] LOSS : 0.5411278446515401 [SCORE] : 0.15385318746169407\n",
      "[19/1000]\n",
      "- [VAL] LOSS : 0.8794611692428589 [SCORE] : 0.4444444477558136\n",
      "[20/1000]\n",
      "- [TRAIN] LOSS : 0.5345106363296509 [SCORE] : 0.23693677186965942\n",
      "[20/1000]\n",
      "- [VAL] LOSS : 0.8644313216209412 [SCORE] : 0.5301587581634521\n",
      "[21/1000]\n",
      "- [TRAIN] LOSS : 0.5273537198702495 [SCORE] : 0.29314933021863304\n",
      "[21/1000]\n",
      "- [VAL] LOSS : 0.8486136198043823 [SCORE] : 0.604938268661499\n",
      "[22/1000]\n",
      "- [TRAIN] LOSS : 0.5197900692621867 [SCORE] : 0.30988975167274474\n",
      "[22/1000]\n",
      "- [VAL] LOSS : 0.8319887518882751 [SCORE] : 0.604938268661499\n",
      "[23/1000]\n",
      "- [TRAIN] LOSS : 0.5114908377329509 [SCORE] : 0.31454583605130515\n",
      "[23/1000]\n",
      "- [VAL] LOSS : 0.8143361210823059 [SCORE] : 0.604938268661499\n",
      "[24/1000]\n",
      "- [TRAIN] LOSS : 0.5033758759498597 [SCORE] : 0.3278791705767314\n",
      "[24/1000]\n",
      "- [VAL] LOSS : 0.7967690229415894 [SCORE] : 0.604938268661499\n",
      "[25/1000]\n",
      "- [TRAIN] LOSS : 0.4954083998998006 [SCORE] : 0.33266993761062624\n",
      "[25/1000]\n",
      "- [VAL] LOSS : 0.7790561318397522 [SCORE] : 0.604938268661499\n",
      "[26/1000]\n",
      "- [TRAIN] LOSS : 0.4873580773671468 [SCORE] : 0.33266993761062624\n",
      "[26/1000]\n",
      "- [VAL] LOSS : 0.7608774900436401 [SCORE] : 0.604938268661499\n",
      "[27/1000]\n",
      "- [TRAIN] LOSS : 0.4791663646697998 [SCORE] : 0.33266993761062624\n",
      "[27/1000]\n",
      "- [VAL] LOSS : 0.7421776056289673 [SCORE] : 0.604938268661499\n",
      "[28/1000]\n",
      "- [TRAIN] LOSS : 0.47103340228398644 [SCORE] : 0.33266993761062624\n",
      "[28/1000]\n",
      "- [VAL] LOSS : 0.7237949371337891 [SCORE] : 0.604938268661499\n",
      "[29/1000]\n",
      "- [TRAIN] LOSS : 0.46296093463897703 [SCORE] : 0.33266993761062624\n",
      "[29/1000]\n",
      "- [VAL] LOSS : 0.7056255340576172 [SCORE] : 0.604938268661499\n",
      "[30/1000]\n",
      "- [TRAIN] LOSS : 0.45491791566212975 [SCORE] : 0.33266993761062624\n",
      "[30/1000]\n",
      "- [VAL] LOSS : 0.6877021193504333 [SCORE] : 0.604938268661499\n",
      "[31/1000]\n",
      "- [TRAIN] LOSS : 0.44693393309911095 [SCORE] : 0.33266993761062624\n",
      "[31/1000]\n",
      "- [VAL] LOSS : 0.6700113415718079 [SCORE] : 0.604938268661499\n",
      "[32/1000]\n",
      "- [TRAIN] LOSS : 0.4389771858851115 [SCORE] : 0.33266993761062624\n",
      "[32/1000]\n",
      "- [VAL] LOSS : 0.6524661183357239 [SCORE] : 0.604938268661499\n",
      "[33/1000]\n",
      "- [TRAIN] LOSS : 0.43109450737635296 [SCORE] : 0.33266993761062624\n",
      "[33/1000]\n",
      "- [VAL] LOSS : 0.6351286768913269 [SCORE] : 0.604938268661499\n",
      "[34/1000]\n",
      "- [TRAIN] LOSS : 0.4233511050542196 [SCORE] : 0.33266993761062624\n",
      "[34/1000]\n",
      "- [VAL] LOSS : 0.6182069778442383 [SCORE] : 0.604938268661499\n",
      "[35/1000]\n",
      "- [TRAIN] LOSS : 0.4157068888346354 [SCORE] : 0.33266993761062624\n",
      "[35/1000]\n",
      "- [VAL] LOSS : 0.60153728723526 [SCORE] : 0.604938268661499\n",
      "[36/1000]\n",
      "- [TRAIN] LOSS : 0.4082891225814819 [SCORE] : 0.33266993761062624\n",
      "[36/1000]\n",
      "- [VAL] LOSS : 0.585570752620697 [SCORE] : 0.604938268661499\n",
      "[37/1000]\n",
      "- [TRAIN] LOSS : 0.40108617146809894 [SCORE] : 0.33266993761062624\n",
      "[37/1000]\n",
      "- [VAL] LOSS : 0.5703402161598206 [SCORE] : 0.604938268661499\n",
      "[38/1000]\n",
      "- [TRAIN] LOSS : 0.3941120425860087 [SCORE] : 0.33266993761062624\n",
      "[38/1000]\n",
      "- [VAL] LOSS : 0.5558523535728455 [SCORE] : 0.604938268661499\n",
      "[39/1000]\n",
      "- [TRAIN] LOSS : 0.3873780171076457 [SCORE] : 0.33266993761062624\n",
      "[39/1000]\n",
      "- [VAL] LOSS : 0.5419968962669373 [SCORE] : 0.604938268661499\n",
      "[40/1000]\n",
      "- [TRAIN] LOSS : 0.3809393803278605 [SCORE] : 0.33266993761062624\n",
      "[40/1000]\n",
      "- [VAL] LOSS : 0.5289493799209595 [SCORE] : 0.604938268661499\n",
      "[41/1000]\n",
      "- [TRAIN] LOSS : 0.3748177409172058 [SCORE] : 0.33266993761062624\n",
      "[41/1000]\n",
      "- [VAL] LOSS : 0.5166792273521423 [SCORE] : 0.604938268661499\n",
      "[42/1000]\n",
      "- [TRAIN] LOSS : 0.36899352073669434 [SCORE] : 0.33266993761062624\n",
      "[42/1000]\n",
      "- [VAL] LOSS : 0.505268931388855 [SCORE] : 0.604938268661499\n",
      "[43/1000]\n",
      "- [TRAIN] LOSS : 0.3634558439254761 [SCORE] : 0.33266993761062624\n",
      "[43/1000]\n",
      "- [VAL] LOSS : 0.4945797026157379 [SCORE] : 0.604938268661499\n",
      "[44/1000]\n",
      "- [TRAIN] LOSS : 0.3582189162572225 [SCORE] : 0.33266993761062624\n",
      "[44/1000]\n",
      "- [VAL] LOSS : 0.48457425832748413 [SCORE] : 0.604938268661499\n",
      "[45/1000]\n",
      "- [TRAIN] LOSS : 0.3532868027687073 [SCORE] : 0.33266993761062624\n",
      "[45/1000]\n",
      "- [VAL] LOSS : 0.4752833843231201 [SCORE] : 0.604938268661499\n",
      "[46/1000]\n",
      "- [TRAIN] LOSS : 0.3486434737841288 [SCORE] : 0.33266993761062624\n",
      "[46/1000]\n",
      "- [VAL] LOSS : 0.46668317914009094 [SCORE] : 0.604938268661499\n",
      "[47/1000]\n",
      "- [TRAIN] LOSS : 0.3442709485689799 [SCORE] : 0.33266993761062624\n",
      "[47/1000]\n",
      "- [VAL] LOSS : 0.45870834589004517 [SCORE] : 0.604938268661499\n",
      "[48/1000]\n",
      "- [TRAIN] LOSS : 0.3401471197605133 [SCORE] : 0.33266993761062624\n",
      "[48/1000]\n",
      "- [VAL] LOSS : 0.451294869184494 [SCORE] : 0.604938268661499\n",
      "[49/1000]\n",
      "- [TRAIN] LOSS : 0.3362668474515279 [SCORE] : 0.33266993761062624\n",
      "[49/1000]\n",
      "- [VAL] LOSS : 0.44441747665405273 [SCORE] : 0.604938268661499\n",
      "[50/1000]\n",
      "- [TRAIN] LOSS : 0.3326062182585398 [SCORE] : 0.33266993761062624\n",
      "[50/1000]\n",
      "- [VAL] LOSS : 0.4380166530609131 [SCORE] : 0.604938268661499\n",
      "[51/1000]\n",
      "- [TRAIN] LOSS : 0.32914142608642577 [SCORE] : 0.33266993761062624\n",
      "[51/1000]\n",
      "- [VAL] LOSS : 0.4320685863494873 [SCORE] : 0.604938268661499\n",
      "[52/1000]\n",
      "- [TRAIN] LOSS : 0.3258828282356262 [SCORE] : 0.33266993761062624\n",
      "[52/1000]\n",
      "- [VAL] LOSS : 0.4264695644378662 [SCORE] : 0.604938268661499\n",
      "[53/1000]\n",
      "- [TRAIN] LOSS : 0.3227641761302948 [SCORE] : 0.33266993761062624\n",
      "[53/1000]\n",
      "- [VAL] LOSS : 0.42111238837242126 [SCORE] : 0.604938268661499\n",
      "[54/1000]\n",
      "- [TRAIN] LOSS : 0.31982948184013366 [SCORE] : 0.33266993761062624\n",
      "[54/1000]\n",
      "- [VAL] LOSS : 0.4161166250705719 [SCORE] : 0.604938268661499\n",
      "[55/1000]\n",
      "- [TRAIN] LOSS : 0.3170978407065074 [SCORE] : 0.33266993761062624\n",
      "[55/1000]\n",
      "- [VAL] LOSS : 0.4114890396595001 [SCORE] : 0.604938268661499\n",
      "[56/1000]\n",
      "- [TRAIN] LOSS : 0.31443639993667605 [SCORE] : 0.33266993761062624\n",
      "[56/1000]\n",
      "- [VAL] LOSS : 0.4071405827999115 [SCORE] : 0.604938268661499\n",
      "[57/1000]\n",
      "- [TRAIN] LOSS : 0.31191945672035215 [SCORE] : 0.33266993761062624\n",
      "[57/1000]\n",
      "- [VAL] LOSS : 0.4030010998249054 [SCORE] : 0.604938268661499\n",
      "[58/1000]\n",
      "- [TRAIN] LOSS : 0.3095188796520233 [SCORE] : 0.33266993761062624\n",
      "[58/1000]\n",
      "- [VAL] LOSS : 0.39906391501426697 [SCORE] : 0.604938268661499\n",
      "[59/1000]\n",
      "- [TRAIN] LOSS : 0.30722044308980306 [SCORE] : 0.33266993761062624\n",
      "[59/1000]\n",
      "- [VAL] LOSS : 0.39536190032958984 [SCORE] : 0.604938268661499\n",
      "[60/1000]\n",
      "- [TRAIN] LOSS : 0.30501524607340497 [SCORE] : 0.33266993761062624\n",
      "[60/1000]\n",
      "- [VAL] LOSS : 0.39187556505203247 [SCORE] : 0.604938268661499\n",
      "[61/1000]\n",
      "- [TRAIN] LOSS : 0.30289111534754437 [SCORE] : 0.33266993761062624\n",
      "[61/1000]\n",
      "- [VAL] LOSS : 0.3885703682899475 [SCORE] : 0.604938268661499\n",
      "[62/1000]\n",
      "- [TRAIN] LOSS : 0.3008340855439504 [SCORE] : 0.33266993761062624\n",
      "[62/1000]\n",
      "- [VAL] LOSS : 0.38535118103027344 [SCORE] : 0.604938268661499\n",
      "[63/1000]\n",
      "- [TRAIN] LOSS : 0.29884572823842365 [SCORE] : 0.33266993761062624\n",
      "[63/1000]\n",
      "- [VAL] LOSS : 0.3822372853755951 [SCORE] : 0.604938268661499\n",
      "[64/1000]\n",
      "- [TRAIN] LOSS : 0.2969200809796651 [SCORE] : 0.33266993761062624\n",
      "[64/1000]\n",
      "- [VAL] LOSS : 0.3792254328727722 [SCORE] : 0.604938268661499\n",
      "[65/1000]\n",
      "- [TRAIN] LOSS : 0.295062388976415 [SCORE] : 0.33266993761062624\n",
      "[65/1000]\n",
      "- [VAL] LOSS : 0.37638065218925476 [SCORE] : 0.604938268661499\n",
      "[66/1000]\n",
      "- [TRAIN] LOSS : 0.2932481070359548 [SCORE] : 0.33266993761062624\n",
      "[66/1000]\n",
      "- [VAL] LOSS : 0.37362271547317505 [SCORE] : 0.604938268661499\n",
      "[67/1000]\n",
      "- [TRAIN] LOSS : 0.2914647618929545 [SCORE] : 0.33266993761062624\n",
      "[67/1000]\n",
      "- [VAL] LOSS : 0.3709103763103485 [SCORE] : 0.604938268661499\n",
      "[68/1000]\n",
      "- [TRAIN] LOSS : 0.2897392193476359 [SCORE] : 0.33266993761062624\n",
      "[68/1000]\n",
      "- [VAL] LOSS : 0.36829838156700134 [SCORE] : 0.604938268661499\n",
      "[69/1000]\n",
      "- [TRAIN] LOSS : 0.28805036743481954 [SCORE] : 0.33266993761062624\n",
      "[69/1000]\n",
      "- [VAL] LOSS : 0.3657846748828888 [SCORE] : 0.604938268661499\n",
      "[70/1000]\n",
      "- [TRAIN] LOSS : 0.2863845189412435 [SCORE] : 0.33266993761062624\n",
      "[70/1000]\n",
      "- [VAL] LOSS : 0.36334821581840515 [SCORE] : 0.7264957427978516\n",
      "[71/1000]\n",
      "- [TRAIN] LOSS : 0.2847477674484253 [SCORE] : 0.3412169456481934\n",
      "[71/1000]\n",
      "- [VAL] LOSS : 0.3609861731529236 [SCORE] : 0.7264957427978516\n",
      "[72/1000]\n",
      "- [TRAIN] LOSS : 0.2831338822841644 [SCORE] : 0.3412169456481934\n",
      "[72/1000]\n",
      "- [VAL] LOSS : 0.3586796820163727 [SCORE] : 0.7264957427978516\n",
      "[73/1000]\n",
      "- [TRAIN] LOSS : 0.2815338353315989 [SCORE] : 0.35417990684509276\n",
      "[73/1000]\n",
      "- [VAL] LOSS : 0.3563927710056305 [SCORE] : 0.7264957427978516\n",
      "[74/1000]\n",
      "- [TRAIN] LOSS : 0.27995362877845764 [SCORE] : 0.35417990684509276\n",
      "[74/1000]\n",
      "- [VAL] LOSS : 0.35413476824760437 [SCORE] : 0.7264957427978516\n",
      "[75/1000]\n",
      "- [TRAIN] LOSS : 0.27838658293088275 [SCORE] : 0.3613227645556132\n",
      "[75/1000]\n",
      "- [VAL] LOSS : 0.35190650820732117 [SCORE] : 0.7264957427978516\n",
      "[76/1000]\n",
      "- [TRAIN] LOSS : 0.2768254578113556 [SCORE] : 0.3679605722427368\n",
      "[76/1000]\n",
      "- [VAL] LOSS : 0.3496937155723572 [SCORE] : 0.7264957427978516\n",
      "[77/1000]\n",
      "- [TRAIN] LOSS : 0.2752821346124013 [SCORE] : 0.37700819174448646\n",
      "[77/1000]\n",
      "- [VAL] LOSS : 0.3475237786769867 [SCORE] : 0.7264957427978516\n",
      "[78/1000]\n",
      "- [TRAIN] LOSS : 0.2737407565116882 [SCORE] : 0.40801347891489664\n",
      "[78/1000]\n",
      "- [VAL] LOSS : 0.3453679084777832 [SCORE] : 0.7264957427978516\n",
      "[79/1000]\n",
      "- [TRAIN] LOSS : 0.2722003360589345 [SCORE] : 0.41764310995737713\n",
      "[79/1000]\n",
      "- [VAL] LOSS : 0.3432202935218811 [SCORE] : 0.7264957427978516\n",
      "[80/1000]\n",
      "- [TRAIN] LOSS : 0.2706719756126404 [SCORE] : 0.41764310995737713\n",
      "[80/1000]\n",
      "- [VAL] LOSS : 0.34110555052757263 [SCORE] : 0.7264957427978516\n",
      "[81/1000]\n",
      "- [TRAIN] LOSS : 0.2691381653149923 [SCORE] : 0.41764310995737713\n",
      "[81/1000]\n",
      "- [VAL] LOSS : 0.33899396657943726 [SCORE] : 0.8171428442001343\n",
      "[82/1000]\n",
      "- [TRAIN] LOSS : 0.26759902834892274 [SCORE] : 0.41764310995737713\n",
      "[82/1000]\n",
      "- [VAL] LOSS : 0.3368852138519287 [SCORE] : 0.8171428442001343\n",
      "[83/1000]\n",
      "- [TRAIN] LOSS : 0.2660582363605499 [SCORE] : 0.4625108281771342\n",
      "[83/1000]\n",
      "- [VAL] LOSS : 0.3347760736942291 [SCORE] : 0.8171428442001343\n",
      "[84/1000]\n",
      "- [TRAIN] LOSS : 0.2645144502321879 [SCORE] : 0.48448613882064817\n",
      "[84/1000]\n",
      "- [VAL] LOSS : 0.33265420794487 [SCORE] : 0.8171428442001343\n",
      "[85/1000]\n",
      "- [TRAIN] LOSS : 0.2629574795564016 [SCORE] : 0.48967132568359373\n",
      "[85/1000]\n",
      "- [VAL] LOSS : 0.3305099606513977 [SCORE] : 0.8171428442001343\n",
      "[86/1000]\n",
      "- [TRAIN] LOSS : 0.2614145835240682 [SCORE] : 0.5013836820920309\n",
      "[86/1000]\n",
      "- [VAL] LOSS : 0.3283809721469879 [SCORE] : 0.8171428442001343\n",
      "[87/1000]\n",
      "- [TRAIN] LOSS : 0.2598578850428263 [SCORE] : 0.5013836820920309\n",
      "[87/1000]\n",
      "- [VAL] LOSS : 0.32624736428260803 [SCORE] : 0.888888955116272\n",
      "[88/1000]\n",
      "- [TRAIN] LOSS : 0.2582868874073029 [SCORE] : 0.5013836820920309\n",
      "[88/1000]\n",
      "- [VAL] LOSS : 0.3241264224052429 [SCORE] : 0.888888955116272\n",
      "[89/1000]\n",
      "- [TRAIN] LOSS : 0.25671157240867615 [SCORE] : 0.5064454078674316\n",
      "[89/1000]\n",
      "- [VAL] LOSS : 0.3220248222351074 [SCORE] : 0.888888955116272\n",
      "[90/1000]\n",
      "- [TRAIN] LOSS : 0.25512487689654034 [SCORE] : 0.5064454078674316\n",
      "[90/1000]\n",
      "- [VAL] LOSS : 0.31993216276168823 [SCORE] : 0.888888955116272\n",
      "[91/1000]\n",
      "- [TRAIN] LOSS : 0.25353302955627444 [SCORE] : 0.5232355316480001\n",
      "[91/1000]\n",
      "- [VAL] LOSS : 0.31784722208976746 [SCORE] : 0.888888955116272\n",
      "[92/1000]\n",
      "- [TRAIN] LOSS : 0.2519116441408793 [SCORE] : 0.5313674012819926\n",
      "[92/1000]\n",
      "- [VAL] LOSS : 0.3157338500022888 [SCORE] : 0.888888955116272\n",
      "[93/1000]\n",
      "- [TRAIN] LOSS : 0.25027610659599303 [SCORE] : 0.5313674012819926\n",
      "[93/1000]\n",
      "- [VAL] LOSS : 0.31356698274612427 [SCORE] : 0.888888955116272\n",
      "[94/1000]\n",
      "- [TRAIN] LOSS : 0.24863425890604654 [SCORE] : 0.5412439425786336\n",
      "[94/1000]\n",
      "- [VAL] LOSS : 0.3113741874694824 [SCORE] : 0.888888955116272\n",
      "[95/1000]\n",
      "- [TRAIN] LOSS : 0.2469817042350769 [SCORE] : 0.5412439425786336\n",
      "[95/1000]\n",
      "- [VAL] LOSS : 0.3091798424720764 [SCORE] : 0.888888955116272\n",
      "[96/1000]\n",
      "- [TRAIN] LOSS : 0.24531526962916056 [SCORE] : 0.5412439425786336\n",
      "[96/1000]\n",
      "- [VAL] LOSS : 0.3069934546947479 [SCORE] : 0.888888955116272\n",
      "[97/1000]\n",
      "- [TRAIN] LOSS : 0.24363084236780802 [SCORE] : 0.5412439425786336\n",
      "[97/1000]\n",
      "- [VAL] LOSS : 0.30480512976646423 [SCORE] : 0.888888955116272\n",
      "[98/1000]\n",
      "- [TRAIN] LOSS : 0.241927037636439 [SCORE] : 0.5412439425786336\n",
      "[98/1000]\n",
      "- [VAL] LOSS : 0.30260148644447327 [SCORE] : 0.888888955116272\n",
      "[99/1000]\n",
      "- [TRAIN] LOSS : 0.24019983212153118 [SCORE] : 0.5412439425786336\n",
      "[99/1000]\n",
      "- [VAL] LOSS : 0.3003672659397125 [SCORE] : 0.888888955116272\n",
      "[100/1000]\n",
      "- [TRAIN] LOSS : 0.2384614408016205 [SCORE] : 0.5412439425786336\n",
      "[100/1000]\n",
      "- [VAL] LOSS : 0.29810795187950134 [SCORE] : 0.888888955116272\n",
      "[101/1000]\n",
      "- [TRAIN] LOSS : 0.23670390049616497 [SCORE] : 0.5506715536117553\n",
      "[101/1000]\n",
      "- [VAL] LOSS : 0.29583510756492615 [SCORE] : 0.888888955116272\n",
      "[102/1000]\n",
      "- [TRAIN] LOSS : 0.2349240779876709 [SCORE] : 0.5556098262468974\n",
      "[102/1000]\n",
      "- [VAL] LOSS : 0.29354044795036316 [SCORE] : 0.888888955116272\n",
      "[103/1000]\n",
      "- [TRAIN] LOSS : 0.23313039938608807 [SCORE] : 0.5556098262468974\n",
      "[103/1000]\n",
      "- [VAL] LOSS : 0.2912028133869171 [SCORE] : 0.888888955116272\n",
      "[104/1000]\n",
      "- [TRAIN] LOSS : 0.23132305343945822 [SCORE] : 0.5556098262468974\n",
      "[104/1000]\n",
      "- [VAL] LOSS : 0.28880974650382996 [SCORE] : 0.888888955116272\n",
      "[105/1000]\n",
      "- [TRAIN] LOSS : 0.2295007904370626 [SCORE] : 0.5556098262468974\n",
      "[105/1000]\n",
      "- [VAL] LOSS : 0.28642553091049194 [SCORE] : 0.888888955116272\n",
      "[106/1000]\n",
      "- [TRAIN] LOSS : 0.22766225337982177 [SCORE] : 0.5617636720339457\n",
      "[106/1000]\n",
      "- [VAL] LOSS : 0.28403592109680176 [SCORE] : 0.888888955116272\n",
      "[107/1000]\n",
      "- [TRAIN] LOSS : 0.225815220673879 [SCORE] : 0.5693827191988627\n",
      "[107/1000]\n",
      "- [VAL] LOSS : 0.28164973855018616 [SCORE] : 0.888888955116272\n",
      "[108/1000]\n",
      "- [TRAIN] LOSS : 0.22394490440686543 [SCORE] : 0.5755908330281575\n",
      "[108/1000]\n",
      "- [VAL] LOSS : 0.27926141023635864 [SCORE] : 0.888888955116272\n",
      "[109/1000]\n",
      "- [TRAIN] LOSS : 0.22205652197202047 [SCORE] : 0.5755908330281575\n",
      "[109/1000]\n",
      "- [VAL] LOSS : 0.27685871720314026 [SCORE] : 0.888888955116272\n",
      "[110/1000]\n",
      "- [TRAIN] LOSS : 0.22015004158020018 [SCORE] : 0.5755908330281575\n",
      "[110/1000]\n",
      "- [VAL] LOSS : 0.27443185448646545 [SCORE] : 0.888888955116272\n",
      "[111/1000]\n",
      "- [TRAIN] LOSS : 0.218226816256841 [SCORE] : 0.5755908330281575\n",
      "[111/1000]\n",
      "- [VAL] LOSS : 0.27197927236557007 [SCORE] : 0.888888955116272\n",
      "[112/1000]\n",
      "- [TRAIN] LOSS : 0.21628802220026652 [SCORE] : 0.5755908330281575\n",
      "[112/1000]\n",
      "- [VAL] LOSS : 0.26950424909591675 [SCORE] : 0.888888955116272\n",
      "[113/1000]\n",
      "- [TRAIN] LOSS : 0.21433431307474773 [SCORE] : 0.5755908330281575\n",
      "[113/1000]\n",
      "- [VAL] LOSS : 0.26701077818870544 [SCORE] : 0.888888955116272\n",
      "[114/1000]\n",
      "- [TRAIN] LOSS : 0.21236280600229898 [SCORE] : 0.5755908330281575\n",
      "[114/1000]\n",
      "- [VAL] LOSS : 0.26447466015815735 [SCORE] : 0.888888955116272\n",
      "[115/1000]\n",
      "- [TRAIN] LOSS : 0.2103842337926229 [SCORE] : 0.5755908330281575\n",
      "[115/1000]\n",
      "- [VAL] LOSS : 0.2619263231754303 [SCORE] : 0.888888955116272\n",
      "[116/1000]\n",
      "- [TRAIN] LOSS : 0.20839647849400839 [SCORE] : 0.5755908330281575\n",
      "[116/1000]\n",
      "- [VAL] LOSS : 0.2593861222267151 [SCORE] : 0.888888955116272\n",
      "[117/1000]\n",
      "- [TRAIN] LOSS : 0.20639514327049255 [SCORE] : 0.5755908330281575\n",
      "[117/1000]\n",
      "- [VAL] LOSS : 0.2568458914756775 [SCORE] : 0.888888955116272\n",
      "[118/1000]\n",
      "- [TRAIN] LOSS : 0.20437985161940256 [SCORE] : 0.5755908330281575\n",
      "[118/1000]\n",
      "- [VAL] LOSS : 0.25429245829582214 [SCORE] : 0.888888955116272\n",
      "[119/1000]\n",
      "- [TRAIN] LOSS : 0.20235281785329182 [SCORE] : 0.5755908330281575\n",
      "[119/1000]\n",
      "- [VAL] LOSS : 0.2517213225364685 [SCORE] : 0.888888955116272\n",
      "[120/1000]\n",
      "- [TRAIN] LOSS : 0.20031643509864808 [SCORE] : 0.5755908330281575\n",
      "[120/1000]\n",
      "- [VAL] LOSS : 0.24913570284843445 [SCORE] : 0.888888955116272\n",
      "[121/1000]\n",
      "- [TRAIN] LOSS : 0.19827213287353515 [SCORE] : 0.5755908330281575\n",
      "[121/1000]\n",
      "- [VAL] LOSS : 0.24654076993465424 [SCORE] : 0.888888955116272\n",
      "[122/1000]\n",
      "- [TRAIN] LOSS : 0.19622064928213756 [SCORE] : 0.5755908330281575\n",
      "[122/1000]\n",
      "- [VAL] LOSS : 0.24393944442272186 [SCORE] : 0.888888955116272\n",
      "[123/1000]\n",
      "- [TRAIN] LOSS : 0.19416281779607136 [SCORE] : 0.5755908330281575\n",
      "[123/1000]\n",
      "- [VAL] LOSS : 0.24133281409740448 [SCORE] : 0.888888955116272\n",
      "[124/1000]\n",
      "- [TRAIN] LOSS : 0.1920997510353724 [SCORE] : 0.5755908330281575\n",
      "[124/1000]\n",
      "- [VAL] LOSS : 0.23872153460979462 [SCORE] : 0.9484702348709106\n",
      "[125/1000]\n",
      "- [TRAIN] LOSS : 0.1900328000386556 [SCORE] : 0.5755908330281575\n",
      "[125/1000]\n",
      "- [VAL] LOSS : 0.23610711097717285 [SCORE] : 0.9484702348709106\n",
      "[126/1000]\n",
      "- [TRAIN] LOSS : 0.18796329498291015 [SCORE] : 0.5755908330281575\n",
      "[126/1000]\n",
      "- [VAL] LOSS : 0.23349125683307648 [SCORE] : 0.9484702348709106\n",
      "[127/1000]\n",
      "- [TRAIN] LOSS : 0.1858925739924113 [SCORE] : 0.5755908330281575\n",
      "[127/1000]\n",
      "- [VAL] LOSS : 0.23087605834007263 [SCORE] : 0.9484702348709106\n",
      "[128/1000]\n",
      "- [TRAIN] LOSS : 0.18382188975811004 [SCORE] : 0.5755908330281575\n",
      "[128/1000]\n",
      "- [VAL] LOSS : 0.2282632440328598 [SCORE] : 0.9484702348709106\n",
      "[129/1000]\n",
      "- [TRAIN] LOSS : 0.18175258835156757 [SCORE] : 0.5755908330281575\n",
      "[129/1000]\n",
      "- [VAL] LOSS : 0.22565458714962006 [SCORE] : 0.9484702348709106\n",
      "[130/1000]\n",
      "- [TRAIN] LOSS : 0.17968597809473674 [SCORE] : 0.5755908330281575\n",
      "[130/1000]\n",
      "- [VAL] LOSS : 0.22305168211460114 [SCORE] : 0.9484702348709106\n",
      "[131/1000]\n",
      "- [TRAIN] LOSS : 0.17762341996033987 [SCORE] : 0.5755908330281575\n",
      "[131/1000]\n",
      "- [VAL] LOSS : 0.22045637667179108 [SCORE] : 0.9484702348709106\n",
      "[132/1000]\n",
      "- [TRAIN] LOSS : 0.17556628187497456 [SCORE] : 0.5755908330281575\n",
      "[132/1000]\n",
      "- [VAL] LOSS : 0.21787045896053314 [SCORE] : 0.9484702348709106\n",
      "[133/1000]\n",
      "- [TRAIN] LOSS : 0.17351586123307547 [SCORE] : 0.5755908330281575\n",
      "[133/1000]\n",
      "- [VAL] LOSS : 0.2152954787015915 [SCORE] : 0.9484702348709106\n",
      "[134/1000]\n",
      "- [TRAIN] LOSS : 0.171473491191864 [SCORE] : 0.5755908330281575\n",
      "[134/1000]\n",
      "- [VAL] LOSS : 0.2127334028482437 [SCORE] : 0.9484702348709106\n",
      "[135/1000]\n",
      "- [TRAIN] LOSS : 0.1694404939810435 [SCORE] : 0.5755908330281575\n",
      "[135/1000]\n",
      "- [VAL] LOSS : 0.2101856768131256 [SCORE] : 0.9484702348709106\n",
      "[136/1000]\n",
      "- [TRAIN] LOSS : 0.1674181928237279 [SCORE] : 0.5755908330281575\n",
      "[136/1000]\n",
      "- [VAL] LOSS : 0.20765413343906403 [SCORE] : 0.9484702348709106\n",
      "[137/1000]\n",
      "- [TRAIN] LOSS : 0.16540780067443847 [SCORE] : 0.5755908330281575\n",
      "[137/1000]\n",
      "- [VAL] LOSS : 0.20514026284217834 [SCORE] : 0.9484702348709106\n",
      "[138/1000]\n",
      "- [TRAIN] LOSS : 0.16341052850087484 [SCORE] : 0.5755908330281575\n",
      "[138/1000]\n",
      "- [VAL] LOSS : 0.20264558494091034 [SCORE] : 0.9484702348709106\n",
      "[139/1000]\n",
      "- [TRAIN] LOSS : 0.16142762800057728 [SCORE] : 0.5755908330281575\n",
      "[139/1000]\n",
      "- [VAL] LOSS : 0.20017175376415253 [SCORE] : 0.9484702348709106\n",
      "[140/1000]\n",
      "- [TRAIN] LOSS : 0.15946020980676015 [SCORE] : 0.5755908330281575\n",
      "[140/1000]\n",
      "- [VAL] LOSS : 0.19772008061408997 [SCORE] : 0.9484702348709106\n",
      "[141/1000]\n",
      "- [TRAIN] LOSS : 0.15750941038131713 [SCORE] : 0.5755908330281575\n",
      "[141/1000]\n",
      "- [VAL] LOSS : 0.19529201090335846 [SCORE] : 0.9484702348709106\n",
      "[142/1000]\n",
      "- [TRAIN] LOSS : 0.155576291680336 [SCORE] : 0.5755908330281575\n",
      "[142/1000]\n",
      "- [VAL] LOSS : 0.19288884103298187 [SCORE] : 0.9484702348709106\n",
      "[143/1000]\n",
      "- [TRAIN] LOSS : 0.1536618649959564 [SCORE] : 0.5755908330281575\n",
      "[143/1000]\n",
      "- [VAL] LOSS : 0.19051174819469452 [SCORE] : 0.9484702348709106\n",
      "[144/1000]\n",
      "- [TRAIN] LOSS : 0.1517671138048172 [SCORE] : 0.5755908330281575\n",
      "[144/1000]\n",
      "- [VAL] LOSS : 0.18816199898719788 [SCORE] : 0.9484702348709106\n",
      "[145/1000]\n",
      "- [TRAIN] LOSS : 0.14989291628201803 [SCORE] : 0.5755908330281575\n",
      "[145/1000]\n",
      "- [VAL] LOSS : 0.18584059178829193 [SCORE] : 0.9484702348709106\n",
      "[146/1000]\n",
      "- [TRAIN] LOSS : 0.1480401168266932 [SCORE] : 0.5755908330281575\n",
      "[146/1000]\n",
      "- [VAL] LOSS : 0.1835486739873886 [SCORE] : 0.9484702348709106\n",
      "[147/1000]\n",
      "- [TRAIN] LOSS : 0.14620961348215739 [SCORE] : 0.5755908330281575\n",
      "[147/1000]\n",
      "- [VAL] LOSS : 0.18128706514835358 [SCORE] : 0.9484702348709106\n",
      "[148/1000]\n",
      "- [TRAIN] LOSS : 0.1444020539522171 [SCORE] : 0.5755908330281575\n",
      "[148/1000]\n",
      "- [VAL] LOSS : 0.17905671894550323 [SCORE] : 0.9484702348709106\n",
      "[149/1000]\n",
      "- [TRAIN] LOSS : 0.14261817038059235 [SCORE] : 0.5755908330281575\n",
      "[149/1000]\n",
      "- [VAL] LOSS : 0.17685841023921967 [SCORE] : 0.9484702348709106\n",
      "[150/1000]\n",
      "- [TRAIN] LOSS : 0.14085856676101685 [SCORE] : 0.5755908330281575\n",
      "[150/1000]\n",
      "- [VAL] LOSS : 0.17469282448291779 [SCORE] : 0.9484702348709106\n",
      "[151/1000]\n",
      "- [TRAIN] LOSS : 0.13912381132443746 [SCORE] : 0.5755908330281575\n",
      "[151/1000]\n",
      "- [VAL] LOSS : 0.172560453414917 [SCORE] : 0.9484702348709106\n",
      "[152/1000]\n",
      "- [TRAIN] LOSS : 0.13741443951924642 [SCORE] : 0.5755908330281575\n",
      "[152/1000]\n",
      "- [VAL] LOSS : 0.17046213150024414 [SCORE] : 0.9484702348709106\n",
      "[153/1000]\n",
      "- [TRAIN] LOSS : 0.1357308432459831 [SCORE] : 0.5755908330281575\n",
      "[153/1000]\n",
      "- [VAL] LOSS : 0.16839812695980072 [SCORE] : 0.9484702348709106\n",
      "[154/1000]\n",
      "- [TRAIN] LOSS : 0.13407349238793057 [SCORE] : 0.5755908330281575\n",
      "[154/1000]\n",
      "- [VAL] LOSS : 0.16636891663074493 [SCORE] : 0.9484702348709106\n",
      "[155/1000]\n",
      "- [TRAIN] LOSS : 0.13244267453749975 [SCORE] : 0.5755908330281575\n",
      "[155/1000]\n",
      "- [VAL] LOSS : 0.1643746942281723 [SCORE] : 0.9484702348709106\n",
      "[156/1000]\n",
      "- [TRAIN] LOSS : 0.13083868771791457 [SCORE] : 0.5755908330281575\n",
      "[156/1000]\n",
      "- [VAL] LOSS : 0.16241590678691864 [SCORE] : 0.9484702348709106\n",
      "[157/1000]\n",
      "- [TRAIN] LOSS : 0.12926176885763804 [SCORE] : 0.5755908330281575\n",
      "[157/1000]\n",
      "- [VAL] LOSS : 0.16049255430698395 [SCORE] : 0.9484702348709106\n",
      "[158/1000]\n",
      "- [TRAIN] LOSS : 0.1277120659748713 [SCORE] : 0.5755908330281575\n",
      "[158/1000]\n",
      "- [VAL] LOSS : 0.1586049348115921 [SCORE] : 0.9484702348709106\n",
      "[159/1000]\n",
      "- [TRAIN] LOSS : 0.126189727584521 [SCORE] : 0.5755908330281575\n",
      "[159/1000]\n",
      "- [VAL] LOSS : 0.15675295889377594 [SCORE] : 0.9484702348709106\n",
      "[160/1000]\n",
      "- [TRAIN] LOSS : 0.12469483812650045 [SCORE] : 0.5755908330281575\n",
      "[160/1000]\n",
      "- [VAL] LOSS : 0.15493664145469666 [SCORE] : 0.9484702348709106\n",
      "[161/1000]\n",
      "- [TRAIN] LOSS : 0.12322744131088256 [SCORE] : 0.5755908330281575\n",
      "[161/1000]\n",
      "- [VAL] LOSS : 0.15315601229667664 [SCORE] : 0.9484702348709106\n",
      "[162/1000]\n",
      "- [TRAIN] LOSS : 0.12178754061460495 [SCORE] : 0.5755908330281575\n",
      "[162/1000]\n",
      "- [VAL] LOSS : 0.15141096711158752 [SCORE] : 0.9484702348709106\n",
      "[163/1000]\n",
      "- [TRAIN] LOSS : 0.12037503470977147 [SCORE] : 0.5755908330281575\n",
      "[163/1000]\n",
      "- [VAL] LOSS : 0.14970143139362335 [SCORE] : 0.9484702348709106\n",
      "[164/1000]\n",
      "- [TRAIN] LOSS : 0.11898989578088125 [SCORE] : 0.5755908330281575\n",
      "[164/1000]\n",
      "- [VAL] LOSS : 0.14802707731723785 [SCORE] : 0.9484702348709106\n",
      "[165/1000]\n",
      "- [TRAIN] LOSS : 0.11763197729984919 [SCORE] : 0.5755908330281575\n",
      "[165/1000]\n",
      "- [VAL] LOSS : 0.14638753235340118 [SCORE] : 0.9484702348709106\n",
      "[166/1000]\n",
      "- [TRAIN] LOSS : 0.11630109896262487 [SCORE] : 0.5755908330281575\n",
      "[166/1000]\n",
      "- [VAL] LOSS : 0.14478255808353424 [SCORE] : 0.9484702348709106\n",
      "[167/1000]\n",
      "- [TRAIN] LOSS : 0.1149971216917038 [SCORE] : 0.5755908330281575\n",
      "[167/1000]\n",
      "- [VAL] LOSS : 0.14321187138557434 [SCORE] : 0.9484702348709106\n",
      "[168/1000]\n",
      "- [TRAIN] LOSS : 0.11371982047955195 [SCORE] : 0.5755908330281575\n",
      "[168/1000]\n",
      "- [VAL] LOSS : 0.14167502522468567 [SCORE] : 0.9484702348709106\n",
      "[169/1000]\n",
      "- [TRAIN] LOSS : 0.11246890674034754 [SCORE] : 0.5755908330281575\n",
      "[169/1000]\n",
      "- [VAL] LOSS : 0.14017173647880554 [SCORE] : 0.9484702348709106\n",
      "[170/1000]\n",
      "- [TRAIN] LOSS : 0.11124415795008341 [SCORE] : 0.5755908330281575\n",
      "[170/1000]\n",
      "- [VAL] LOSS : 0.1387014091014862 [SCORE] : 0.9484702348709106\n",
      "[171/1000]\n",
      "- [TRAIN] LOSS : 0.11004526317119598 [SCORE] : 0.5755908330281575\n",
      "[171/1000]\n",
      "- [VAL] LOSS : 0.13726377487182617 [SCORE] : 0.9484702348709106\n",
      "[172/1000]\n",
      "- [TRAIN] LOSS : 0.10887191792329153 [SCORE] : 0.5755908330281575\n",
      "[172/1000]\n",
      "- [VAL] LOSS : 0.13585814833641052 [SCORE] : 0.9484702348709106\n",
      "[173/1000]\n",
      "- [TRAIN] LOSS : 0.10772379040718079 [SCORE] : 0.5755908330281575\n",
      "[173/1000]\n",
      "- [VAL] LOSS : 0.134484201669693 [SCORE] : 0.9484702348709106\n",
      "[174/1000]\n",
      "- [TRAIN] LOSS : 0.10660053392251333 [SCORE] : 0.5755908330281575\n",
      "[174/1000]\n",
      "- [VAL] LOSS : 0.1331413984298706 [SCORE] : 0.9484702348709106\n",
      "[175/1000]\n",
      "- [TRAIN] LOSS : 0.10550177097320557 [SCORE] : 0.5755908330281575\n",
      "[175/1000]\n",
      "- [VAL] LOSS : 0.1318291574716568 [SCORE] : 0.9484702348709106\n",
      "[176/1000]\n",
      "- [TRAIN] LOSS : 0.10442717572053274 [SCORE] : 0.5755908330281575\n",
      "[176/1000]\n",
      "- [VAL] LOSS : 0.13054700195789337 [SCORE] : 0.9484702348709106\n",
      "[177/1000]\n",
      "- [TRAIN] LOSS : 0.10337629069884618 [SCORE] : 0.5755908330281575\n",
      "[177/1000]\n",
      "- [VAL] LOSS : 0.12929430603981018 [SCORE] : 0.9484702348709106\n",
      "[178/1000]\n",
      "- [TRAIN] LOSS : 0.10234877268473307 [SCORE] : 0.5755908330281575\n",
      "[178/1000]\n",
      "- [VAL] LOSS : 0.12807056307792664 [SCORE] : 0.9484702348709106\n",
      "[179/1000]\n",
      "- [TRAIN] LOSS : 0.10134418408075968 [SCORE] : 0.5755908330281575\n",
      "[179/1000]\n",
      "- [VAL] LOSS : 0.12687523663043976 [SCORE] : 0.9484702348709106\n",
      "[180/1000]\n",
      "- [TRAIN] LOSS : 0.10036214292049409 [SCORE] : 0.5755908330281575\n",
      "[180/1000]\n",
      "- [VAL] LOSS : 0.12570767104625702 [SCORE] : 0.9484702348709106\n",
      "[181/1000]\n",
      "- [TRAIN] LOSS : 0.0994022364417712 [SCORE] : 0.5755908330281575\n",
      "[181/1000]\n",
      "- [VAL] LOSS : 0.12456736713647842 [SCORE] : 0.9484702348709106\n",
      "[182/1000]\n",
      "- [TRAIN] LOSS : 0.09846406628688177 [SCORE] : 0.5755908330281575\n",
      "[182/1000]\n",
      "- [VAL] LOSS : 0.1234537735581398 [SCORE] : 0.9484702348709106\n",
      "[183/1000]\n",
      "- [TRAIN] LOSS : 0.09754711737235387 [SCORE] : 0.5755908330281575\n",
      "[183/1000]\n",
      "- [VAL] LOSS : 0.1223662868142128 [SCORE] : 0.9484702348709106\n",
      "[184/1000]\n",
      "- [TRAIN] LOSS : 0.09665107677380244 [SCORE] : 0.5755908330281575\n",
      "[184/1000]\n",
      "- [VAL] LOSS : 0.12130419909954071 [SCORE] : 0.9484702348709106\n",
      "[185/1000]\n",
      "- [TRAIN] LOSS : 0.09577545622984568 [SCORE] : 0.5755908330281575\n",
      "[185/1000]\n",
      "- [VAL] LOSS : 0.12026719003915787 [SCORE] : 0.9484702348709106\n",
      "[186/1000]\n",
      "- [TRAIN] LOSS : 0.09491984893878301 [SCORE] : 0.5755908330281575\n",
      "[186/1000]\n",
      "- [VAL] LOSS : 0.11925448477268219 [SCORE] : 0.9484702348709106\n",
      "[187/1000]\n",
      "- [TRAIN] LOSS : 0.09408382425705591 [SCORE] : 0.5755908330281575\n",
      "[187/1000]\n",
      "- [VAL] LOSS : 0.11826564371585846 [SCORE] : 0.9484702348709106\n",
      "[188/1000]\n",
      "- [TRAIN] LOSS : 0.09326700369517009 [SCORE] : 0.5755908330281575\n",
      "[188/1000]\n",
      "- [VAL] LOSS : 0.11730007082223892 [SCORE] : 0.9484702348709106\n",
      "[189/1000]\n",
      "- [TRAIN] LOSS : 0.09246890222032865 [SCORE] : 0.5755908330281575\n",
      "[189/1000]\n",
      "- [VAL] LOSS : 0.1163572147488594 [SCORE] : 0.9484702348709106\n",
      "[190/1000]\n",
      "- [TRAIN] LOSS : 0.09168912917375564 [SCORE] : 0.5755908330281575\n",
      "[190/1000]\n",
      "- [VAL] LOSS : 0.11543647199869156 [SCORE] : 0.9484702348709106\n",
      "[191/1000]\n",
      "- [TRAIN] LOSS : 0.09092728694279989 [SCORE] : 0.5755908330281575\n",
      "[191/1000]\n",
      "- [VAL] LOSS : 0.11453736573457718 [SCORE] : 0.9484702348709106\n",
      "[192/1000]\n",
      "- [TRAIN] LOSS : 0.09018298164010048 [SCORE] : 0.5755908330281575\n",
      "[192/1000]\n",
      "- [VAL] LOSS : 0.11365938931703568 [SCORE] : 0.9484702348709106\n",
      "[193/1000]\n",
      "- [TRAIN] LOSS : 0.0894557940463225 [SCORE] : 0.5755908330281575\n",
      "[193/1000]\n",
      "- [VAL] LOSS : 0.11280196905136108 [SCORE] : 0.9484702348709106\n",
      "[194/1000]\n",
      "- [TRAIN] LOSS : 0.08874526595075925 [SCORE] : 0.5755908330281575\n",
      "[194/1000]\n",
      "- [VAL] LOSS : 0.11196460574865341 [SCORE] : 0.9484702348709106\n",
      "[195/1000]\n",
      "- [TRAIN] LOSS : 0.08805109784007073 [SCORE] : 0.5755908330281575\n",
      "[195/1000]\n",
      "- [VAL] LOSS : 0.11114675551652908 [SCORE] : 0.9484702348709106\n",
      "[196/1000]\n",
      "- [TRAIN] LOSS : 0.08737288862466812 [SCORE] : 0.5755908330281575\n",
      "[196/1000]\n",
      "- [VAL] LOSS : 0.11034797877073288 [SCORE] : 0.9484702348709106\n",
      "[197/1000]\n",
      "- [TRAIN] LOSS : 0.08671018555760383 [SCORE] : 0.5755908330281575\n",
      "[197/1000]\n",
      "- [VAL] LOSS : 0.1095677986741066 [SCORE] : 0.9484702348709106\n",
      "[198/1000]\n",
      "- [TRAIN] LOSS : 0.08606269781788191 [SCORE] : 0.5755908330281575\n",
      "[198/1000]\n",
      "- [VAL] LOSS : 0.10880571603775024 [SCORE] : 0.9484702348709106\n",
      "[199/1000]\n",
      "- [TRAIN] LOSS : 0.08543001512686411 [SCORE] : 0.5755908330281575\n",
      "[199/1000]\n",
      "- [VAL] LOSS : 0.10806118696928024 [SCORE] : 0.9484702348709106\n",
      "[200/1000]\n",
      "- [TRAIN] LOSS : 0.0848117470741272 [SCORE] : 0.5755908330281575\n",
      "[200/1000]\n",
      "- [VAL] LOSS : 0.10733391344547272 [SCORE] : 0.9484702348709106\n",
      "[201/1000]\n",
      "- [TRAIN] LOSS : 0.08420759961009025 [SCORE] : 0.5755908330281575\n",
      "[201/1000]\n",
      "- [VAL] LOSS : 0.10662331432104111 [SCORE] : 0.9484702348709106\n",
      "[202/1000]\n",
      "- [TRAIN] LOSS : 0.08361720542112987 [SCORE] : 0.5755908330281575\n",
      "[202/1000]\n",
      "- [VAL] LOSS : 0.10592900216579437 [SCORE] : 0.9484702348709106\n",
      "[203/1000]\n",
      "- [TRAIN] LOSS : 0.08304021830360095 [SCORE] : 0.5755908330281575\n",
      "[203/1000]\n",
      "- [VAL] LOSS : 0.1052505299448967 [SCORE] : 0.9484702348709106\n",
      "[204/1000]\n",
      "- [TRAIN] LOSS : 0.08247629428903262 [SCORE] : 0.5755908330281575\n",
      "[204/1000]\n",
      "- [VAL] LOSS : 0.10458748787641525 [SCORE] : 0.9484702348709106\n",
      "[205/1000]\n",
      "- [TRAIN] LOSS : 0.08192514032125472 [SCORE] : 0.5755908330281575\n",
      "[205/1000]\n",
      "- [VAL] LOSS : 0.10393945872783661 [SCORE] : 0.9484702348709106\n",
      "[206/1000]\n",
      "- [TRAIN] LOSS : 0.08138640448451043 [SCORE] : 0.5755908330281575\n",
      "[206/1000]\n",
      "- [VAL] LOSS : 0.1033061072230339 [SCORE] : 0.9484702348709106\n",
      "[207/1000]\n",
      "- [TRAIN] LOSS : 0.08085976491371791 [SCORE] : 0.5755908330281575\n",
      "[207/1000]\n",
      "- [VAL] LOSS : 0.10268702358007431 [SCORE] : 0.9484702348709106\n",
      "[208/1000]\n",
      "- [TRAIN] LOSS : 0.0803449700276057 [SCORE] : 0.5755908330281575\n",
      "[208/1000]\n",
      "- [VAL] LOSS : 0.1020817756652832 [SCORE] : 0.9484702348709106\n",
      "[209/1000]\n",
      "- [TRAIN] LOSS : 0.07984165325760842 [SCORE] : 0.5755908330281575\n",
      "[209/1000]\n",
      "- [VAL] LOSS : 0.10149001330137253 [SCORE] : 0.9484702348709106\n",
      "[210/1000]\n",
      "- [TRAIN] LOSS : 0.07934960027535756 [SCORE] : 0.5755908330281575\n",
      "[210/1000]\n",
      "- [VAL] LOSS : 0.10091139376163483 [SCORE] : 0.9484702348709106\n",
      "[211/1000]\n",
      "- [TRAIN] LOSS : 0.0788686066865921 [SCORE] : 0.5755908330281575\n",
      "[211/1000]\n",
      "- [VAL] LOSS : 0.10034563392400742 [SCORE] : 0.9484702348709106\n",
      "[212/1000]\n",
      "- [TRAIN] LOSS : 0.07839821428060531 [SCORE] : 0.5755908330281575\n",
      "[212/1000]\n",
      "- [VAL] LOSS : 0.09979234635829926 [SCORE] : 0.9484702348709106\n",
      "[213/1000]\n",
      "- [TRAIN] LOSS : 0.0779381200671196 [SCORE] : 0.5755908330281575\n",
      "[213/1000]\n",
      "- [VAL] LOSS : 0.09925113618373871 [SCORE] : 0.9484702348709106\n",
      "[214/1000]\n",
      "- [TRAIN] LOSS : 0.07748818099498749 [SCORE] : 0.5755908330281575\n",
      "[214/1000]\n",
      "- [VAL] LOSS : 0.09872175008058548 [SCORE] : 0.9484702348709106\n",
      "[215/1000]\n",
      "- [TRAIN] LOSS : 0.07704813505212466 [SCORE] : 0.5755908330281575\n",
      "[215/1000]\n",
      "- [VAL] LOSS : 0.09820383787155151 [SCORE] : 0.9484702348709106\n",
      "[216/1000]\n",
      "- [TRAIN] LOSS : 0.07661772569020589 [SCORE] : 0.5755908330281575\n",
      "[216/1000]\n",
      "- [VAL] LOSS : 0.09769707918167114 [SCORE] : 0.9484702348709106\n",
      "[217/1000]\n",
      "- [TRAIN] LOSS : 0.07619667599598566 [SCORE] : 0.5755908330281575\n",
      "[217/1000]\n",
      "- [VAL] LOSS : 0.09720127284526825 [SCORE] : 0.9484702348709106\n",
      "[218/1000]\n",
      "- [TRAIN] LOSS : 0.07578471650679906 [SCORE] : 0.5755908330281575\n",
      "[218/1000]\n",
      "- [VAL] LOSS : 0.0967160314321518 [SCORE] : 0.9484702348709106\n",
      "[219/1000]\n",
      "- [TRAIN] LOSS : 0.07538172751665115 [SCORE] : 0.5755908330281575\n",
      "[219/1000]\n",
      "- [VAL] LOSS : 0.09624110907316208 [SCORE] : 0.9484702348709106\n",
      "[220/1000]\n",
      "- [TRAIN] LOSS : 0.07498737772305807 [SCORE] : 0.5755908330281575\n",
      "[220/1000]\n",
      "- [VAL] LOSS : 0.09577617794275284 [SCORE] : 0.9484702348709106\n",
      "[221/1000]\n",
      "- [TRAIN] LOSS : 0.07460148930549622 [SCORE] : 0.5755908330281575\n",
      "[221/1000]\n",
      "- [VAL] LOSS : 0.09532112628221512 [SCORE] : 0.9484702348709106\n",
      "[222/1000]\n",
      "- [TRAIN] LOSS : 0.07422384098172188 [SCORE] : 0.5755908330281575\n",
      "[222/1000]\n",
      "- [VAL] LOSS : 0.0948755219578743 [SCORE] : 0.9484702348709106\n",
      "[223/1000]\n",
      "- [TRAIN] LOSS : 0.07385421693325042 [SCORE] : 0.5755908330281575\n",
      "[223/1000]\n",
      "- [VAL] LOSS : 0.09443922340869904 [SCORE] : 0.9484702348709106\n",
      "[224/1000]\n",
      "- [TRAIN] LOSS : 0.07349244207143783 [SCORE] : 0.5755908330281575\n",
      "[224/1000]\n",
      "- [VAL] LOSS : 0.09401195496320724 [SCORE] : 0.9484702348709106\n",
      "[225/1000]\n",
      "- [TRAIN] LOSS : 0.07313829213380814 [SCORE] : 0.5755908330281575\n",
      "[225/1000]\n",
      "- [VAL] LOSS : 0.09359340369701385 [SCORE] : 0.9484702348709106\n",
      "[226/1000]\n",
      "- [TRAIN] LOSS : 0.07279157191514969 [SCORE] : 0.5755908330281575\n",
      "[226/1000]\n",
      "- [VAL] LOSS : 0.0931835025548935 [SCORE] : 0.9484702348709106\n",
      "[227/1000]\n",
      "- [TRAIN] LOSS : 0.07245214954018593 [SCORE] : 0.5755908330281575\n",
      "[227/1000]\n",
      "- [VAL] LOSS : 0.0927819088101387 [SCORE] : 0.9484702348709106\n",
      "[228/1000]\n",
      "- [TRAIN] LOSS : 0.07211975182096163 [SCORE] : 0.5755908330281575\n",
      "[228/1000]\n",
      "- [VAL] LOSS : 0.09238846600055695 [SCORE] : 0.9484702348709106\n",
      "[229/1000]\n",
      "- [TRAIN] LOSS : 0.07179425060749053 [SCORE] : 0.5755908330281575\n",
      "[229/1000]\n",
      "- [VAL] LOSS : 0.09200294315814972 [SCORE] : 0.9484702348709106\n",
      "[230/1000]\n",
      "- [TRAIN] LOSS : 0.07147547701994578 [SCORE] : 0.5755908330281575\n",
      "[230/1000]\n",
      "- [VAL] LOSS : 0.09162508696317673 [SCORE] : 0.9484702348709106\n",
      "[231/1000]\n",
      "- [TRAIN] LOSS : 0.07116326441367467 [SCORE] : 0.5755908330281575\n",
      "[231/1000]\n",
      "- [VAL] LOSS : 0.09125477075576782 [SCORE] : 0.9484702348709106\n",
      "[232/1000]\n",
      "- [TRAIN] LOSS : 0.0708575990051031 [SCORE] : 0.5755908330281575\n",
      "[232/1000]\n",
      "- [VAL] LOSS : 0.09089171886444092 [SCORE] : 0.9484702348709106\n",
      "[233/1000]\n",
      "- [TRAIN] LOSS : 0.07055804108579954 [SCORE] : 0.5755908330281575\n",
      "[233/1000]\n",
      "- [VAL] LOSS : 0.09053587168455124 [SCORE] : 0.9484702348709106\n",
      "[234/1000]\n",
      "- [TRAIN] LOSS : 0.07026444760461649 [SCORE] : 0.5755908330281575\n",
      "[234/1000]\n",
      "- [VAL] LOSS : 0.0901869386434555 [SCORE] : 0.9484702348709106\n",
      "[235/1000]\n",
      "- [TRAIN] LOSS : 0.06997688636183738 [SCORE] : 0.5755908330281575\n",
      "[235/1000]\n",
      "- [VAL] LOSS : 0.08984484523534775 [SCORE] : 0.9484702348709106\n",
      "[236/1000]\n",
      "- [TRAIN] LOSS : 0.06969517096877098 [SCORE] : 0.5755908330281575\n",
      "[236/1000]\n",
      "- [VAL] LOSS : 0.08950933068990707 [SCORE] : 0.9484702348709106\n",
      "[237/1000]\n",
      "- [TRAIN] LOSS : 0.06941906511783599 [SCORE] : 0.5755908330281575\n",
      "[237/1000]\n",
      "- [VAL] LOSS : 0.08918025344610214 [SCORE] : 0.9484702348709106\n",
      "[238/1000]\n",
      "- [TRAIN] LOSS : 0.06914839732150237 [SCORE] : 0.5755908330281575\n",
      "[238/1000]\n",
      "- [VAL] LOSS : 0.08885744959115982 [SCORE] : 0.9484702348709106\n",
      "[239/1000]\n",
      "- [TRAIN] LOSS : 0.06888312697410584 [SCORE] : 0.5755908330281575\n",
      "[239/1000]\n",
      "- [VAL] LOSS : 0.08854078501462936 [SCORE] : 0.9484702348709106\n",
      "[240/1000]\n",
      "- [TRAIN] LOSS : 0.06862313896417618 [SCORE] : 0.5755908330281575\n",
      "[240/1000]\n",
      "- [VAL] LOSS : 0.08823006600141525 [SCORE] : 0.9484702348709106\n",
      "[241/1000]\n",
      "- [TRAIN] LOSS : 0.06836825075248877 [SCORE] : 0.5755908330281575\n",
      "[241/1000]\n",
      "- [VAL] LOSS : 0.08792520314455032 [SCORE] : 0.9484702348709106\n",
      "[242/1000]\n",
      "- [TRAIN] LOSS : 0.06811832909782728 [SCORE] : 0.5755908330281575\n",
      "[242/1000]\n",
      "- [VAL] LOSS : 0.08762599527835846 [SCORE] : 0.9484702348709106\n",
      "[243/1000]\n",
      "- [TRAIN] LOSS : 0.06787326882282893 [SCORE] : 0.5755908330281575\n",
      "[243/1000]\n",
      "- [VAL] LOSS : 0.08733230084180832 [SCORE] : 0.9484702348709106\n",
      "[244/1000]\n",
      "- [TRAIN] LOSS : 0.06763296363254388 [SCORE] : 0.5755908330281575\n",
      "[244/1000]\n",
      "- [VAL] LOSS : 0.08704405277967453 [SCORE] : 0.9484702348709106\n",
      "[245/1000]\n",
      "- [TRAIN] LOSS : 0.06739729469021162 [SCORE] : 0.5755908330281575\n",
      "[245/1000]\n",
      "- [VAL] LOSS : 0.08676104992628098 [SCORE] : 0.9484702348709106\n",
      "[246/1000]\n",
      "- [TRAIN] LOSS : 0.06716614750524362 [SCORE] : 0.5755908330281575\n",
      "[246/1000]\n",
      "- [VAL] LOSS : 0.08648324757814407 [SCORE] : 0.9484702348709106\n",
      "[247/1000]\n",
      "- [TRAIN] LOSS : 0.06693940063317617 [SCORE] : 0.5755908330281575\n",
      "[247/1000]\n",
      "- [VAL] LOSS : 0.08621038496494293 [SCORE] : 0.9484702348709106\n",
      "[248/1000]\n",
      "- [TRAIN] LOSS : 0.06671698540449142 [SCORE] : 0.5755908330281575\n",
      "[248/1000]\n",
      "- [VAL] LOSS : 0.08594243228435516 [SCORE] : 0.9484702348709106\n",
      "[249/1000]\n",
      "- [TRAIN] LOSS : 0.0664987945308288 [SCORE] : 0.5755908330281575\n",
      "[249/1000]\n",
      "- [VAL] LOSS : 0.08567932993173599 [SCORE] : 0.9484702348709106\n",
      "[250/1000]\n",
      "- [TRAIN] LOSS : 0.06628469812373321 [SCORE] : 0.5755908330281575\n",
      "[250/1000]\n",
      "- [VAL] LOSS : 0.08542080223560333 [SCORE] : 0.9484702348709106\n",
      "[251/1000]\n",
      "- [TRAIN] LOSS : 0.06607460069159667 [SCORE] : 0.5755908330281575\n",
      "[251/1000]\n",
      "- [VAL] LOSS : 0.08516687154769897 [SCORE] : 0.9484702348709106\n",
      "[252/1000]\n",
      "- [TRAIN] LOSS : 0.06586843952536584 [SCORE] : 0.5755908330281575\n",
      "[252/1000]\n",
      "- [VAL] LOSS : 0.08491736650466919 [SCORE] : 0.9484702348709106\n",
      "[253/1000]\n",
      "- [TRAIN] LOSS : 0.06566610286633173 [SCORE] : 0.5755908330281575\n",
      "[253/1000]\n",
      "- [VAL] LOSS : 0.08467220515012741 [SCORE] : 0.9484702348709106\n",
      "[254/1000]\n",
      "- [TRAIN] LOSS : 0.06546750627458095 [SCORE] : 0.5755908330281575\n",
      "[254/1000]\n",
      "- [VAL] LOSS : 0.08443128317594528 [SCORE] : 0.9484702348709106\n",
      "[255/1000]\n",
      "- [TRAIN] LOSS : 0.06527256382008394 [SCORE] : 0.5755908330281575\n",
      "[255/1000]\n",
      "- [VAL] LOSS : 0.08419458568096161 [SCORE] : 0.9484702348709106\n",
      "[256/1000]\n",
      "- [TRAIN] LOSS : 0.06508123005429904 [SCORE] : 0.5755908330281575\n",
      "[256/1000]\n",
      "- [VAL] LOSS : 0.0839618518948555 [SCORE] : 0.9484702348709106\n",
      "[257/1000]\n",
      "- [TRAIN] LOSS : 0.0648933673898379 [SCORE] : 0.5755908330281575\n",
      "[257/1000]\n",
      "- [VAL] LOSS : 0.08373308926820755 [SCORE] : 0.9484702348709106\n",
      "[258/1000]\n",
      "- [TRAIN] LOSS : 0.06470892081658046 [SCORE] : 0.5755908330281575\n",
      "[258/1000]\n",
      "- [VAL] LOSS : 0.08350811898708344 [SCORE] : 0.9484702348709106\n",
      "[259/1000]\n",
      "- [TRAIN] LOSS : 0.06452777261535327 [SCORE] : 0.5755908330281575\n",
      "[259/1000]\n",
      "- [VAL] LOSS : 0.08328697830438614 [SCORE] : 0.9484702348709106\n",
      "[260/1000]\n",
      "- [TRAIN] LOSS : 0.06434987833102544 [SCORE] : 0.5755908330281575\n",
      "[260/1000]\n",
      "- [VAL] LOSS : 0.0830695629119873 [SCORE] : 0.9484702348709106\n",
      "[261/1000]\n",
      "- [TRAIN] LOSS : 0.06417518630623817 [SCORE] : 0.5755908330281575\n",
      "[261/1000]\n",
      "- [VAL] LOSS : 0.08285573869943619 [SCORE] : 0.9484702348709106\n",
      "[262/1000]\n",
      "- [TRAIN] LOSS : 0.06400363569458326 [SCORE] : 0.5755908330281575\n",
      "[262/1000]\n",
      "- [VAL] LOSS : 0.08264535665512085 [SCORE] : 0.9484702348709106\n",
      "[263/1000]\n",
      "- [TRAIN] LOSS : 0.0638351115087668 [SCORE] : 0.5755908330281575\n",
      "[263/1000]\n",
      "- [VAL] LOSS : 0.08243842422962189 [SCORE] : 0.9484702348709106\n",
      "[264/1000]\n",
      "- [TRAIN] LOSS : 0.06366950819889704 [SCORE] : 0.5755908330281575\n",
      "[264/1000]\n",
      "- [VAL] LOSS : 0.08223491162061691 [SCORE] : 0.9484702348709106\n",
      "[265/1000]\n",
      "- [TRAIN] LOSS : 0.06350689157843589 [SCORE] : 0.5755908330281575\n",
      "[265/1000]\n",
      "- [VAL] LOSS : 0.08203471451997757 [SCORE] : 0.9484702348709106\n",
      "[266/1000]\n",
      "- [TRAIN] LOSS : 0.06334706669052442 [SCORE] : 0.5755908330281575\n",
      "[266/1000]\n",
      "- [VAL] LOSS : 0.08183766901493073 [SCORE] : 0.9484702348709106\n",
      "[267/1000]\n",
      "- [TRAIN] LOSS : 0.063190059363842 [SCORE] : 0.5755908330281575\n",
      "[267/1000]\n",
      "- [VAL] LOSS : 0.08164383471012115 [SCORE] : 0.9484702348709106\n",
      "[268/1000]\n",
      "- [TRAIN] LOSS : 0.06303572816153367 [SCORE] : 0.5755908330281575\n",
      "[268/1000]\n",
      "- [VAL] LOSS : 0.08145298063755035 [SCORE] : 0.9484702348709106\n",
      "[269/1000]\n",
      "- [TRAIN] LOSS : 0.06288410400350888 [SCORE] : 0.5755908330281575\n",
      "[269/1000]\n",
      "- [VAL] LOSS : 0.0812651589512825 [SCORE] : 0.9484702348709106\n",
      "[270/1000]\n",
      "- [TRAIN] LOSS : 0.06273506432771683 [SCORE] : 0.5755908330281575\n",
      "[270/1000]\n",
      "- [VAL] LOSS : 0.08108029514551163 [SCORE] : 0.9484702348709106\n",
      "[271/1000]\n",
      "- [TRAIN] LOSS : 0.06258853723605474 [SCORE] : 0.5755908330281575\n",
      "[271/1000]\n",
      "- [VAL] LOSS : 0.08089828491210938 [SCORE] : 0.9484702348709106\n",
      "[272/1000]\n",
      "- [TRAIN] LOSS : 0.06244452508787314 [SCORE] : 0.5755908330281575\n",
      "[272/1000]\n",
      "- [VAL] LOSS : 0.08071919530630112 [SCORE] : 0.9484702348709106\n",
      "[273/1000]\n",
      "- [TRAIN] LOSS : 0.06230298280715942 [SCORE] : 0.5755908330281575\n",
      "[273/1000]\n",
      "- [VAL] LOSS : 0.08054278790950775 [SCORE] : 0.9484702348709106\n",
      "[274/1000]\n",
      "- [TRAIN] LOSS : 0.062163780877987546 [SCORE] : 0.5755908330281575\n",
      "[274/1000]\n",
      "- [VAL] LOSS : 0.08036909997463226 [SCORE] : 0.9484702348709106\n",
      "[275/1000]\n",
      "- [TRAIN] LOSS : 0.06202695469061534 [SCORE] : 0.5755908330281575\n",
      "[275/1000]\n",
      "- [VAL] LOSS : 0.0801980048418045 [SCORE] : 0.9484702348709106\n",
      "[276/1000]\n",
      "- [TRAIN] LOSS : 0.061892382552226385 [SCORE] : 0.5755908330281575\n",
      "[276/1000]\n",
      "- [VAL] LOSS : 0.08002950251102448 [SCORE] : 0.9484702348709106\n",
      "[277/1000]\n",
      "- [TRAIN] LOSS : 0.06176004807154337 [SCORE] : 0.5755908330281575\n",
      "[277/1000]\n",
      "- [VAL] LOSS : 0.0798635482788086 [SCORE] : 0.9484702348709106\n",
      "[278/1000]\n",
      "- [TRAIN] LOSS : 0.06162989350656668 [SCORE] : 0.5755908330281575\n",
      "[278/1000]\n",
      "- [VAL] LOSS : 0.07970006763935089 [SCORE] : 0.9484702348709106\n",
      "[279/1000]\n",
      "- [TRAIN] LOSS : 0.06150193090240161 [SCORE] : 0.5755908330281575\n",
      "[279/1000]\n",
      "- [VAL] LOSS : 0.07953904569149017 [SCORE] : 0.9484702348709106\n",
      "[280/1000]\n",
      "- [TRAIN] LOSS : 0.06137603260576725 [SCORE] : 0.5755908330281575\n",
      "[280/1000]\n",
      "- [VAL] LOSS : 0.07938029617071152 [SCORE] : 0.9484702348709106\n",
      "[281/1000]\n",
      "- [TRAIN] LOSS : 0.06125219240784645 [SCORE] : 0.5755908330281575\n",
      "[281/1000]\n",
      "- [VAL] LOSS : 0.07922391593456268 [SCORE] : 0.9484702348709106\n",
      "[282/1000]\n",
      "- [TRAIN] LOSS : 0.061130365977684654 [SCORE] : 0.5755908330281575\n",
      "[282/1000]\n",
      "- [VAL] LOSS : 0.07906977832317352 [SCORE] : 0.9484702348709106\n",
      "[283/1000]\n",
      "- [TRAIN] LOSS : 0.06101051022609075 [SCORE] : 0.5755908330281575\n",
      "[283/1000]\n",
      "- [VAL] LOSS : 0.0789179876446724 [SCORE] : 0.9484702348709106\n",
      "[284/1000]\n",
      "- [TRAIN] LOSS : 0.06089257821440697 [SCORE] : 0.5755908330281575\n",
      "[284/1000]\n",
      "- [VAL] LOSS : 0.078768290579319 [SCORE] : 0.9484702348709106\n",
      "[285/1000]\n",
      "- [TRAIN] LOSS : 0.06077652300397555 [SCORE] : 0.5755908330281575\n",
      "[285/1000]\n",
      "- [VAL] LOSS : 0.0786207765340805 [SCORE] : 0.9484702348709106\n",
      "[286/1000]\n",
      "- [TRAIN] LOSS : 0.06066236992677053 [SCORE] : 0.5755908330281575\n",
      "[286/1000]\n",
      "- [VAL] LOSS : 0.07847529649734497 [SCORE] : 0.9484702348709106\n",
      "[287/1000]\n",
      "- [TRAIN] LOSS : 0.060549963265657425 [SCORE] : 0.5755908330281575\n",
      "[287/1000]\n",
      "- [VAL] LOSS : 0.07833191007375717 [SCORE] : 0.9484702348709106\n",
      "[288/1000]\n",
      "- [TRAIN] LOSS : 0.06043941304087639 [SCORE] : 0.5755908330281575\n",
      "[288/1000]\n",
      "- [VAL] LOSS : 0.07819060981273651 [SCORE] : 0.9484702348709106\n",
      "[289/1000]\n",
      "- [TRAIN] LOSS : 0.06033056279023488 [SCORE] : 0.5755908330281575\n",
      "[289/1000]\n",
      "- [VAL] LOSS : 0.07805115729570389 [SCORE] : 0.9484702348709106\n",
      "[290/1000]\n",
      "- [TRAIN] LOSS : 0.06022343027095 [SCORE] : 0.5755908330281575\n",
      "[290/1000]\n",
      "- [VAL] LOSS : 0.07791365683078766 [SCORE] : 0.9484702348709106\n",
      "[291/1000]\n",
      "- [TRAIN] LOSS : 0.06011793004969756 [SCORE] : 0.5755908330281575\n",
      "[291/1000]\n",
      "- [VAL] LOSS : 0.07777805626392365 [SCORE] : 0.9484702348709106\n",
      "[292/1000]\n",
      "- [TRAIN] LOSS : 0.06001409590244293 [SCORE] : 0.5755908330281575\n",
      "[292/1000]\n",
      "- [VAL] LOSS : 0.07764441519975662 [SCORE] : 0.9484702348709106\n",
      "[293/1000]\n",
      "- [TRAIN] LOSS : 0.05991189529498418 [SCORE] : 0.5755908330281575\n",
      "[293/1000]\n",
      "- [VAL] LOSS : 0.0775124654173851 [SCORE] : 0.9484702348709106\n",
      "[294/1000]\n",
      "- [TRAIN] LOSS : 0.05981125968197982 [SCORE] : 0.5755908330281575\n",
      "[294/1000]\n",
      "- [VAL] LOSS : 0.07738234102725983 [SCORE] : 0.9484702348709106\n",
      "[295/1000]\n",
      "- [TRAIN] LOSS : 0.05971215305229028 [SCORE] : 0.5755908330281575\n",
      "[295/1000]\n",
      "- [VAL] LOSS : 0.0772540494799614 [SCORE] : 0.9484702348709106\n",
      "[296/1000]\n",
      "- [TRAIN] LOSS : 0.05961457851032416 [SCORE] : 0.5755908330281575\n",
      "[296/1000]\n",
      "- [VAL] LOSS : 0.0771273821592331 [SCORE] : 0.9484702348709106\n",
      "[297/1000]\n",
      "- [TRAIN] LOSS : 0.05951848551630974 [SCORE] : 0.5755908330281575\n",
      "[297/1000]\n",
      "- [VAL] LOSS : 0.07700234651565552 [SCORE] : 0.9484702348709106\n",
      "[298/1000]\n",
      "- [TRAIN] LOSS : 0.05942383520305157 [SCORE] : 0.5755908330281575\n",
      "[298/1000]\n",
      "- [VAL] LOSS : 0.0768790915608406 [SCORE] : 0.9484702348709106\n",
      "[299/1000]\n",
      "- [TRAIN] LOSS : 0.05933062229305506 [SCORE] : 0.5755908330281575\n",
      "[299/1000]\n",
      "- [VAL] LOSS : 0.07675744593143463 [SCORE] : 0.9484702348709106\n",
      "[300/1000]\n",
      "- [TRAIN] LOSS : 0.05923880723615487 [SCORE] : 0.5755908330281575\n",
      "[300/1000]\n",
      "- [VAL] LOSS : 0.07663726061582565 [SCORE] : 0.9484702348709106\n",
      "[301/1000]\n",
      "- [TRAIN] LOSS : 0.059148367928961915 [SCORE] : 0.5755908330281575\n",
      "[301/1000]\n",
      "- [VAL] LOSS : 0.07651879638433456 [SCORE] : 0.9484702348709106\n",
      "[302/1000]\n",
      "- [TRAIN] LOSS : 0.05905926488339901 [SCORE] : 0.5755908330281575\n",
      "[302/1000]\n",
      "- [VAL] LOSS : 0.07640177756547928 [SCORE] : 0.9484702348709106\n",
      "[303/1000]\n",
      "- [TRAIN] LOSS : 0.05897150579839945 [SCORE] : 0.5755908330281575\n",
      "[303/1000]\n",
      "- [VAL] LOSS : 0.07628627866506577 [SCORE] : 0.9484702348709106\n",
      "[304/1000]\n",
      "- [TRAIN] LOSS : 0.0588850237429142 [SCORE] : 0.5755908330281575\n",
      "[304/1000]\n",
      "- [VAL] LOSS : 0.07617224752902985 [SCORE] : 0.9484702348709106\n",
      "[305/1000]\n",
      "- [TRAIN] LOSS : 0.058799800214668116 [SCORE] : 0.5755908330281575\n",
      "[305/1000]\n",
      "- [VAL] LOSS : 0.07605963200330734 [SCORE] : 0.9484702348709106\n",
      "[306/1000]\n",
      "- [TRAIN] LOSS : 0.058715828508138654 [SCORE] : 0.5755908330281575\n",
      "[306/1000]\n",
      "- [VAL] LOSS : 0.07594849169254303 [SCORE] : 0.9484702348709106\n",
      "[307/1000]\n",
      "- [TRAIN] LOSS : 0.05863312849154075 [SCORE] : 0.5755908330281575\n",
      "[307/1000]\n",
      "- [VAL] LOSS : 0.07583869248628616 [SCORE] : 0.9484702348709106\n",
      "[308/1000]\n",
      "- [TRAIN] LOSS : 0.0585516123721997 [SCORE] : 0.5755908330281575\n",
      "[308/1000]\n",
      "- [VAL] LOSS : 0.07573024928569794 [SCORE] : 0.9484702348709106\n",
      "[309/1000]\n",
      "- [TRAIN] LOSS : 0.058471230479578175 [SCORE] : 0.5755908330281575\n",
      "[309/1000]\n",
      "- [VAL] LOSS : 0.07562323659658432 [SCORE] : 0.9484702348709106\n",
      "[310/1000]\n",
      "- [TRAIN] LOSS : 0.0583920277034243 [SCORE] : 0.5755908330281575\n",
      "[310/1000]\n",
      "- [VAL] LOSS : 0.07551746815443039 [SCORE] : 0.9484702348709106\n",
      "[311/1000]\n",
      "- [TRAIN] LOSS : 0.058313998579978946 [SCORE] : 0.5755908330281575\n",
      "[311/1000]\n",
      "- [VAL] LOSS : 0.07541298866271973 [SCORE] : 0.9484702348709106\n",
      "[312/1000]\n",
      "- [TRAIN] LOSS : 0.05823707083861033 [SCORE] : 0.5755908330281575\n",
      "[312/1000]\n",
      "- [VAL] LOSS : 0.07530982792377472 [SCORE] : 0.9484702348709106\n",
      "[313/1000]\n",
      "- [TRAIN] LOSS : 0.058161233613888426 [SCORE] : 0.5755908330281575\n",
      "[313/1000]\n",
      "- [VAL] LOSS : 0.07520784437656403 [SCORE] : 0.9484702348709106\n",
      "[314/1000]\n",
      "- [TRAIN] LOSS : 0.05808645722766717 [SCORE] : 0.5755908330281575\n",
      "[314/1000]\n",
      "- [VAL] LOSS : 0.07510726153850555 [SCORE] : 0.9484702348709106\n",
      "[315/1000]\n",
      "- [TRAIN] LOSS : 0.058012735471129416 [SCORE] : 0.5755908330281575\n",
      "[315/1000]\n",
      "- [VAL] LOSS : 0.07500767707824707 [SCORE] : 0.9484702348709106\n",
      "[316/1000]\n",
      "- [TRAIN] LOSS : 0.05794007331132889 [SCORE] : 0.5755908330281575\n",
      "[316/1000]\n",
      "- [VAL] LOSS : 0.07490936666727066 [SCORE] : 0.9484702348709106\n",
      "[317/1000]\n",
      "- [TRAIN] LOSS : 0.05786839903642734 [SCORE] : 0.5755908330281575\n",
      "[317/1000]\n",
      "- [VAL] LOSS : 0.07481225579977036 [SCORE] : 0.9484702348709106\n",
      "[318/1000]\n",
      "- [TRAIN] LOSS : 0.05779775095482667 [SCORE] : 0.5755908330281575\n",
      "[318/1000]\n",
      "- [VAL] LOSS : 0.07471621781587601 [SCORE] : 0.9484702348709106\n",
      "[319/1000]\n",
      "- [TRAIN] LOSS : 0.05772807324926058 [SCORE] : 0.5755908330281575\n",
      "[319/1000]\n",
      "- [VAL] LOSS : 0.0746212974190712 [SCORE] : 0.9484702348709106\n",
      "[320/1000]\n",
      "- [TRAIN] LOSS : 0.05765934915592273 [SCORE] : 0.5755908330281575\n",
      "[320/1000]\n",
      "- [VAL] LOSS : 0.07452749460935593 [SCORE] : 0.9484702348709106\n",
      "[321/1000]\n",
      "- [TRAIN] LOSS : 0.057591613320012884 [SCORE] : 0.5755908330281575\n",
      "[321/1000]\n",
      "- [VAL] LOSS : 0.07443483173847198 [SCORE] : 0.9484702348709106\n",
      "[322/1000]\n",
      "- [TRAIN] LOSS : 0.05752477298180262 [SCORE] : 0.5755908330281575\n",
      "[322/1000]\n",
      "- [VAL] LOSS : 0.07434319704771042 [SCORE] : 0.9484702348709106\n",
      "[323/1000]\n",
      "- [TRAIN] LOSS : 0.05745884838203589 [SCORE] : 0.5755908330281575\n",
      "[323/1000]\n",
      "- [VAL] LOSS : 0.07425258308649063 [SCORE] : 0.9484702348709106\n",
      "[324/1000]\n",
      "- [TRAIN] LOSS : 0.05739383207013209 [SCORE] : 0.5755908330281575\n",
      "[324/1000]\n",
      "- [VAL] LOSS : 0.0741630271077156 [SCORE] : 0.9484702348709106\n",
      "[325/1000]\n",
      "- [TRAIN] LOSS : 0.0573296993970871 [SCORE] : 0.5755908330281575\n",
      "[325/1000]\n",
      "- [VAL] LOSS : 0.07407450675964355 [SCORE] : 0.9484702348709106\n",
      "[326/1000]\n",
      "- [TRAIN] LOSS : 0.05726642683148384 [SCORE] : 0.5755908330281575\n",
      "[326/1000]\n",
      "- [VAL] LOSS : 0.0739869624376297 [SCORE] : 0.9484702348709106\n",
      "[327/1000]\n",
      "- [TRAIN] LOSS : 0.057204001334806284 [SCORE] : 0.5755908330281575\n",
      "[327/1000]\n",
      "- [VAL] LOSS : 0.07390036433935165 [SCORE] : 0.9484702348709106\n",
      "[328/1000]\n",
      "- [TRAIN] LOSS : 0.057142442154387636 [SCORE] : 0.5755908330281575\n",
      "[328/1000]\n",
      "- [VAL] LOSS : 0.0738147497177124 [SCORE] : 0.9484702348709106\n",
      "[329/1000]\n",
      "- [TRAIN] LOSS : 0.05708167292177677 [SCORE] : 0.5755908330281575\n",
      "[329/1000]\n",
      "- [VAL] LOSS : 0.07373009622097015 [SCORE] : 0.9484702348709106\n",
      "[330/1000]\n",
      "- [TRAIN] LOSS : 0.05702171052495639 [SCORE] : 0.5755908330281575\n",
      "[330/1000]\n",
      "- [VAL] LOSS : 0.07364632189273834 [SCORE] : 0.9484702348709106\n",
      "[331/1000]\n",
      "- [TRAIN] LOSS : 0.05696252112587293 [SCORE] : 0.5755908330281575\n",
      "[331/1000]\n",
      "- [VAL] LOSS : 0.07356343418359756 [SCORE] : 0.9484702348709106\n",
      "[332/1000]\n",
      "- [TRAIN] LOSS : 0.056903819118936855 [SCORE] : 0.5755908330281575\n",
      "[332/1000]\n",
      "- [VAL] LOSS : 0.07348160445690155 [SCORE] : 0.9484702348709106\n",
      "[333/1000]\n",
      "- [TRAIN] LOSS : 0.0568464865287145 [SCORE] : 0.5755908330281575\n",
      "[333/1000]\n",
      "- [VAL] LOSS : 0.0734005942940712 [SCORE] : 0.9484702348709106\n",
      "[334/1000]\n",
      "- [TRAIN] LOSS : 0.05678981213519971 [SCORE] : 0.5755908330281575\n",
      "[334/1000]\n",
      "- [VAL] LOSS : 0.0733204334974289 [SCORE] : 0.9484702348709106\n",
      "[335/1000]\n",
      "- [TRAIN] LOSS : 0.05673379134386778 [SCORE] : 0.5755908330281575\n",
      "[335/1000]\n",
      "- [VAL] LOSS : 0.0732412189245224 [SCORE] : 0.9484702348709106\n",
      "[336/1000]\n",
      "- [TRAIN] LOSS : 0.056678077888985476 [SCORE] : 0.5755908330281575\n",
      "[336/1000]\n",
      "- [VAL] LOSS : 0.07316281646490097 [SCORE] : 0.9484702348709106\n",
      "[337/1000]\n",
      "- [TRAIN] LOSS : 0.05662372770408789 [SCORE] : 0.5755908330281575\n",
      "[337/1000]\n",
      "- [VAL] LOSS : 0.0730852410197258 [SCORE] : 0.9484702348709106\n",
      "[338/1000]\n",
      "- [TRAIN] LOSS : 0.056569937616586685 [SCORE] : 0.5755908330281575\n",
      "[338/1000]\n",
      "- [VAL] LOSS : 0.07300851494073868 [SCORE] : 0.9484702348709106\n",
      "[339/1000]\n",
      "- [TRAIN] LOSS : 0.0565167640025417 [SCORE] : 0.5755908330281575\n",
      "[339/1000]\n",
      "- [VAL] LOSS : 0.07293245941400528 [SCORE] : 0.9484702348709106\n",
      "[340/1000]\n",
      "- [TRAIN] LOSS : 0.05646392684429884 [SCORE] : 0.5755908330281575\n",
      "[340/1000]\n",
      "- [VAL] LOSS : 0.07285740226507187 [SCORE] : 0.9484702348709106\n",
      "[341/1000]\n",
      "- [TRAIN] LOSS : 0.05641223080456257 [SCORE] : 0.5755908330281575\n",
      "[341/1000]\n",
      "- [VAL] LOSS : 0.07278300821781158 [SCORE] : 0.9484702348709106\n",
      "[342/1000]\n",
      "- [TRAIN] LOSS : 0.05636076324929794 [SCORE] : 0.5755908330281575\n",
      "[342/1000]\n",
      "- [VAL] LOSS : 0.07270948588848114 [SCORE] : 0.9484702348709106\n",
      "[343/1000]\n",
      "- [TRAIN] LOSS : 0.056310595882435636 [SCORE] : 0.5755908330281575\n",
      "[343/1000]\n",
      "- [VAL] LOSS : 0.07263670116662979 [SCORE] : 0.9484702348709106\n",
      "[344/1000]\n",
      "- [TRAIN] LOSS : 0.05626083798706531 [SCORE] : 0.5755908330281575\n",
      "[344/1000]\n",
      "- [VAL] LOSS : 0.072564497590065 [SCORE] : 0.9484702348709106\n",
      "[345/1000]\n",
      "- [TRAIN] LOSS : 0.056211179681122304 [SCORE] : 0.5755908330281575\n",
      "[345/1000]\n",
      "- [VAL] LOSS : 0.07249327749013901 [SCORE] : 0.9484702348709106\n",
      "[346/1000]\n",
      "- [TRAIN] LOSS : 0.05616280424098174 [SCORE] : 0.5755908330281575\n",
      "[346/1000]\n",
      "- [VAL] LOSS : 0.07242266088724136 [SCORE] : 0.9484702348709106\n",
      "[347/1000]\n",
      "- [TRAIN] LOSS : 0.056114852676788965 [SCORE] : 0.5755908330281575\n",
      "[347/1000]\n",
      "- [VAL] LOSS : 0.07235267013311386 [SCORE] : 0.9484702348709106\n",
      "[348/1000]\n",
      "- [TRAIN] LOSS : 0.05606720969080925 [SCORE] : 0.5755908330281575\n",
      "[348/1000]\n",
      "- [VAL] LOSS : 0.07228343933820724 [SCORE] : 0.9484702348709106\n",
      "[349/1000]\n",
      "- [TRAIN] LOSS : 0.056020115564266844 [SCORE] : 0.5755908330281575\n",
      "[349/1000]\n",
      "- [VAL] LOSS : 0.07221509516239166 [SCORE] : 0.9484702348709106\n",
      "[350/1000]\n",
      "- [TRAIN] LOSS : 0.055974265250066914 [SCORE] : 0.5755908330281575\n",
      "[350/1000]\n",
      "- [VAL] LOSS : 0.07214727252721786 [SCORE] : 0.9484702348709106\n",
      "[351/1000]\n",
      "- [TRAIN] LOSS : 0.05592875238507986 [SCORE] : 0.5755908330281575\n",
      "[351/1000]\n",
      "- [VAL] LOSS : 0.07208001613616943 [SCORE] : 0.9484702348709106\n",
      "[352/1000]\n",
      "- [TRAIN] LOSS : 0.055883259139955045 [SCORE] : 0.5755908330281575\n",
      "[352/1000]\n",
      "- [VAL] LOSS : 0.0720134899020195 [SCORE] : 0.9484702348709106\n",
      "[353/1000]\n",
      "- [TRAIN] LOSS : 0.05583888155718644 [SCORE] : 0.5755908330281575\n",
      "[353/1000]\n",
      "- [VAL] LOSS : 0.07194756716489792 [SCORE] : 0.9484702348709106\n",
      "[354/1000]\n",
      "- [TRAIN] LOSS : 0.055794665279487766 [SCORE] : 0.5755908330281575\n",
      "[354/1000]\n",
      "- [VAL] LOSS : 0.07188242673873901 [SCORE] : 0.9484702348709106\n",
      "[355/1000]\n",
      "- [TRAIN] LOSS : 0.05575137548148632 [SCORE] : 0.5755908330281575\n",
      "[355/1000]\n",
      "- [VAL] LOSS : 0.07181774824857712 [SCORE] : 0.9484702348709106\n",
      "[356/1000]\n",
      "- [TRAIN] LOSS : 0.05570822569231192 [SCORE] : 0.5755908330281575\n",
      "[356/1000]\n",
      "- [VAL] LOSS : 0.07175380736589432 [SCORE] : 0.9484702348709106\n",
      "[357/1000]\n",
      "- [TRAIN] LOSS : 0.05566562091310819 [SCORE] : 0.5755908330281575\n",
      "[357/1000]\n",
      "- [VAL] LOSS : 0.07169060409069061 [SCORE] : 0.9484702348709106\n",
      "[358/1000]\n",
      "- [TRAIN] LOSS : 0.055624149305125076 [SCORE] : 0.5755908330281575\n",
      "[358/1000]\n",
      "- [VAL] LOSS : 0.07162780314683914 [SCORE] : 0.9484702348709106\n",
      "[359/1000]\n",
      "- [TRAIN] LOSS : 0.055582782439887526 [SCORE] : 0.5755908330281575\n",
      "[359/1000]\n",
      "- [VAL] LOSS : 0.07156554609537125 [SCORE] : 0.9484702348709106\n",
      "[360/1000]\n",
      "- [TRAIN] LOSS : 0.05554161431888739 [SCORE] : 0.5755908330281575\n",
      "[360/1000]\n",
      "- [VAL] LOSS : 0.07150401175022125 [SCORE] : 0.9484702348709106\n",
      "[361/1000]\n",
      "- [TRAIN] LOSS : 0.055501067886749904 [SCORE] : 0.5755908330281575\n",
      "[361/1000]\n",
      "- [VAL] LOSS : 0.07144305855035782 [SCORE] : 0.9484702348709106\n",
      "[362/1000]\n",
      "- [TRAIN] LOSS : 0.05546133648604155 [SCORE] : 0.5755908330281575\n",
      "[362/1000]\n",
      "- [VAL] LOSS : 0.07138246297836304 [SCORE] : 0.9484702348709106\n",
      "[363/1000]\n",
      "- [TRAIN] LOSS : 0.05542166698724031 [SCORE] : 0.5755908330281575\n",
      "[363/1000]\n",
      "- [VAL] LOSS : 0.07132261246442795 [SCORE] : 0.9484702348709106\n",
      "[364/1000]\n",
      "- [TRAIN] LOSS : 0.05538250878453255 [SCORE] : 0.5755908330281575\n",
      "[364/1000]\n",
      "- [VAL] LOSS : 0.07126344740390778 [SCORE] : 0.9484702348709106\n",
      "[365/1000]\n",
      "- [TRAIN] LOSS : 0.055344318971037865 [SCORE] : 0.5755908330281575\n",
      "[365/1000]\n",
      "- [VAL] LOSS : 0.0712045356631279 [SCORE] : 0.9484702348709106\n",
      "[366/1000]\n",
      "- [TRAIN] LOSS : 0.055305998275677365 [SCORE] : 0.5755908330281575\n",
      "[366/1000]\n",
      "- [VAL] LOSS : 0.0711464136838913 [SCORE] : 0.9484702348709106\n",
      "[367/1000]\n",
      "- [TRAIN] LOSS : 0.05526853886743387 [SCORE] : 0.5755908330281575\n",
      "[367/1000]\n",
      "- [VAL] LOSS : 0.07108860462903976 [SCORE] : 0.9484702348709106\n",
      "[368/1000]\n",
      "- [TRAIN] LOSS : 0.055231187989314395 [SCORE] : 0.5755908330281575\n",
      "[368/1000]\n",
      "- [VAL] LOSS : 0.07103138417005539 [SCORE] : 0.9484702348709106\n",
      "[369/1000]\n",
      "- [TRAIN] LOSS : 0.055194263346493244 [SCORE] : 0.5755908330281575\n",
      "[369/1000]\n",
      "- [VAL] LOSS : 0.07097483426332474 [SCORE] : 0.9484702348709106\n",
      "[370/1000]\n",
      "- [TRAIN] LOSS : 0.05515789532413085 [SCORE] : 0.5755908330281575\n",
      "[370/1000]\n",
      "- [VAL] LOSS : 0.07091876864433289 [SCORE] : 0.9484702348709106\n",
      "[371/1000]\n",
      "- [TRAIN] LOSS : 0.05512229812641938 [SCORE] : 0.5755908330281575\n",
      "[371/1000]\n",
      "- [VAL] LOSS : 0.07086291909217834 [SCORE] : 0.9484702348709106\n",
      "[372/1000]\n",
      "- [TRAIN] LOSS : 0.05508658370623986 [SCORE] : 0.5755908330281575\n",
      "[372/1000]\n",
      "- [VAL] LOSS : 0.07080765813589096 [SCORE] : 0.9484702348709106\n",
      "[373/1000]\n",
      "- [TRAIN] LOSS : 0.05505134351551533 [SCORE] : 0.5755908330281575\n",
      "[373/1000]\n",
      "- [VAL] LOSS : 0.0707530602812767 [SCORE] : 0.9484702348709106\n",
      "[374/1000]\n",
      "- [TRAIN] LOSS : 0.05501689991603295 [SCORE] : 0.5755908330281575\n",
      "[374/1000]\n",
      "- [VAL] LOSS : 0.07069866359233856 [SCORE] : 0.9484702348709106\n",
      "[375/1000]\n",
      "- [TRAIN] LOSS : 0.054982399195432664 [SCORE] : 0.5755908330281575\n",
      "[375/1000]\n",
      "- [VAL] LOSS : 0.07064490020275116 [SCORE] : 0.9484702348709106\n",
      "[376/1000]\n",
      "- [TRAIN] LOSS : 0.05494839294503132 [SCORE] : 0.5755908330281575\n",
      "[376/1000]\n",
      "- [VAL] LOSS : 0.07059169560670853 [SCORE] : 0.9484702348709106\n",
      "[377/1000]\n",
      "- [TRAIN] LOSS : 0.054914804299672444 [SCORE] : 0.5755908330281575\n",
      "[377/1000]\n",
      "- [VAL] LOSS : 0.07053885608911514 [SCORE] : 0.9484702348709106\n",
      "[378/1000]\n",
      "- [TRAIN] LOSS : 0.05488196810086568 [SCORE] : 0.5755908330281575\n",
      "[378/1000]\n",
      "- [VAL] LOSS : 0.07048622518777847 [SCORE] : 0.9484702348709106\n",
      "[379/1000]\n",
      "- [TRAIN] LOSS : 0.05484901530047258 [SCORE] : 0.5755908330281575\n",
      "[379/1000]\n",
      "- [VAL] LOSS : 0.07043422758579254 [SCORE] : 0.9484702348709106\n",
      "[380/1000]\n",
      "- [TRAIN] LOSS : 0.05481652691960335 [SCORE] : 0.5755908330281575\n",
      "[380/1000]\n",
      "- [VAL] LOSS : 0.07038278132677078 [SCORE] : 0.9484702348709106\n",
      "[381/1000]\n",
      "- [TRAIN] LOSS : 0.0547844377035896 [SCORE] : 0.5755908330281575\n",
      "[381/1000]\n",
      "- [VAL] LOSS : 0.07033172249794006 [SCORE] : 0.9484702348709106\n",
      "[382/1000]\n",
      "- [TRAIN] LOSS : 0.05475266519933939 [SCORE] : 0.5755908330281575\n",
      "[382/1000]\n",
      "- [VAL] LOSS : 0.07028110325336456 [SCORE] : 0.9484702348709106\n",
      "[383/1000]\n",
      "- [TRAIN] LOSS : 0.05472162185857694 [SCORE] : 0.5755908330281575\n",
      "[383/1000]\n",
      "- [VAL] LOSS : 0.07023067772388458 [SCORE] : 0.9484702348709106\n",
      "[384/1000]\n",
      "- [TRAIN] LOSS : 0.054690493767460185 [SCORE] : 0.5755908330281575\n",
      "[384/1000]\n",
      "- [VAL] LOSS : 0.07018069177865982 [SCORE] : 0.9484702348709106\n",
      "[385/1000]\n",
      "- [TRAIN] LOSS : 0.05465973162402709 [SCORE] : 0.5755908330281575\n",
      "[385/1000]\n",
      "- [VAL] LOSS : 0.0701313465833664 [SCORE] : 0.9484702348709106\n",
      "[386/1000]\n",
      "- [TRAIN] LOSS : 0.054629381746053696 [SCORE] : 0.5755908330281575\n",
      "[386/1000]\n",
      "- [VAL] LOSS : 0.07008232921361923 [SCORE] : 0.9484702348709106\n",
      "[387/1000]\n",
      "- [TRAIN] LOSS : 0.054599396077295144 [SCORE] : 0.5755908330281575\n",
      "[387/1000]\n",
      "- [VAL] LOSS : 0.07003365457057953 [SCORE] : 0.9484702348709106\n",
      "[388/1000]\n",
      "- [TRAIN] LOSS : 0.0545697142680486 [SCORE] : 0.5755908330281575\n",
      "[388/1000]\n",
      "- [VAL] LOSS : 0.06998548656702042 [SCORE] : 0.9484702348709106\n",
      "[389/1000]\n",
      "- [TRAIN] LOSS : 0.05454035295794408 [SCORE] : 0.5755908330281575\n",
      "[389/1000]\n",
      "- [VAL] LOSS : 0.06993760913610458 [SCORE] : 0.9484702348709106\n",
      "[390/1000]\n",
      "- [TRAIN] LOSS : 0.054511629541714984 [SCORE] : 0.5755908330281575\n",
      "[390/1000]\n",
      "- [VAL] LOSS : 0.06988993287086487 [SCORE] : 0.9484702348709106\n",
      "[391/1000]\n",
      "- [TRAIN] LOSS : 0.054482764874895416 [SCORE] : 0.5755908330281575\n",
      "[391/1000]\n",
      "- [VAL] LOSS : 0.06984277069568634 [SCORE] : 0.9484702348709106\n",
      "[392/1000]\n",
      "- [TRAIN] LOSS : 0.05445429539928834 [SCORE] : 0.5755908330281575\n",
      "[392/1000]\n",
      "- [VAL] LOSS : 0.06979597359895706 [SCORE] : 0.9484702348709106\n",
      "[393/1000]\n",
      "- [TRAIN] LOSS : 0.054426151948670544 [SCORE] : 0.5755908330281575\n",
      "[393/1000]\n",
      "- [VAL] LOSS : 0.069749616086483 [SCORE] : 0.9484702348709106\n",
      "[394/1000]\n",
      "- [TRAIN] LOSS : 0.05439836122095585 [SCORE] : 0.5755908330281575\n",
      "[394/1000]\n",
      "- [VAL] LOSS : 0.06970366090536118 [SCORE] : 0.9484702348709106\n",
      "[395/1000]\n",
      "- [TRAIN] LOSS : 0.054370913716653986 [SCORE] : 0.5755908330281575\n",
      "[395/1000]\n",
      "- [VAL] LOSS : 0.06965798884630203 [SCORE] : 0.9484702348709106\n",
      "[396/1000]\n",
      "- [TRAIN] LOSS : 0.054343759827315805 [SCORE] : 0.5755908330281575\n",
      "[396/1000]\n",
      "- [VAL] LOSS : 0.06961255520582199 [SCORE] : 0.9484702348709106\n",
      "[397/1000]\n",
      "- [TRAIN] LOSS : 0.0543168759594361 [SCORE] : 0.5755908330281575\n",
      "[397/1000]\n",
      "- [VAL] LOSS : 0.06956765800714493 [SCORE] : 0.9484702348709106\n",
      "[398/1000]\n",
      "- [TRAIN] LOSS : 0.054290248764057955 [SCORE] : 0.5755908330281575\n",
      "[398/1000]\n",
      "- [VAL] LOSS : 0.06952305883169174 [SCORE] : 0.9484702348709106\n",
      "[399/1000]\n",
      "- [TRAIN] LOSS : 0.054263900965452194 [SCORE] : 0.5755908330281575\n",
      "[399/1000]\n",
      "- [VAL] LOSS : 0.0694788470864296 [SCORE] : 0.9484702348709106\n",
      "[400/1000]\n",
      "- [TRAIN] LOSS : 0.054237888442973295 [SCORE] : 0.5755908330281575\n",
      "[400/1000]\n",
      "- [VAL] LOSS : 0.06943493336439133 [SCORE] : 0.9484702348709106\n",
      "[401/1000]\n",
      "- [TRAIN] LOSS : 0.05421214283754428 [SCORE] : 0.5755908330281575\n",
      "[401/1000]\n",
      "- [VAL] LOSS : 0.06939135491847992 [SCORE] : 0.9484702348709106\n",
      "[402/1000]\n",
      "- [TRAIN] LOSS : 0.0541866443430384 [SCORE] : 0.5755908330281575\n",
      "[402/1000]\n",
      "- [VAL] LOSS : 0.06934802234172821 [SCORE] : 0.9484702348709106\n",
      "[403/1000]\n",
      "- [TRAIN] LOSS : 0.05416140947490931 [SCORE] : 0.5755908330281575\n",
      "[403/1000]\n",
      "- [VAL] LOSS : 0.06930504739284515 [SCORE] : 0.9484702348709106\n",
      "[404/1000]\n",
      "- [TRAIN] LOSS : 0.05413643730183442 [SCORE] : 0.5755908330281575\n",
      "[404/1000]\n",
      "- [VAL] LOSS : 0.0692623108625412 [SCORE] : 0.9484702348709106\n",
      "[405/1000]\n",
      "- [TRAIN] LOSS : 0.054111735957364245 [SCORE] : 0.5755908330281575\n",
      "[405/1000]\n",
      "- [VAL] LOSS : 0.06921989470720291 [SCORE] : 0.9484702348709106\n",
      "[406/1000]\n",
      "- [TRAIN] LOSS : 0.05408726328363021 [SCORE] : 0.5755908330281575\n",
      "[406/1000]\n",
      "- [VAL] LOSS : 0.06917790323495865 [SCORE] : 0.9484702348709106\n",
      "[407/1000]\n",
      "- [TRAIN] LOSS : 0.05406308577706417 [SCORE] : 0.5755908330281575\n",
      "[407/1000]\n",
      "- [VAL] LOSS : 0.06913622468709946 [SCORE] : 0.9484702348709106\n",
      "[408/1000]\n",
      "- [TRAIN] LOSS : 0.05403913417831063 [SCORE] : 0.5755908330281575\n",
      "[408/1000]\n",
      "- [VAL] LOSS : 0.06909479945898056 [SCORE] : 0.9484702348709106\n",
      "[409/1000]\n",
      "- [TRAIN] LOSS : 0.05401543900370598 [SCORE] : 0.5755908330281575\n",
      "[409/1000]\n",
      "- [VAL] LOSS : 0.06905367225408554 [SCORE] : 0.9484702348709106\n",
      "[410/1000]\n",
      "- [TRAIN] LOSS : 0.053991979857285814 [SCORE] : 0.5755908330281575\n",
      "[410/1000]\n",
      "- [VAL] LOSS : 0.06901288777589798 [SCORE] : 0.9484702348709106\n",
      "[411/1000]\n",
      "- [TRAIN] LOSS : 0.05396878908698757 [SCORE] : 0.5755908330281575\n",
      "[411/1000]\n",
      "- [VAL] LOSS : 0.06897237151861191 [SCORE] : 0.9484702348709106\n",
      "[412/1000]\n",
      "- [TRAIN] LOSS : 0.05394576502343019 [SCORE] : 0.5755908330281575\n",
      "[412/1000]\n",
      "- [VAL] LOSS : 0.06893213838338852 [SCORE] : 0.9484702348709106\n",
      "[413/1000]\n",
      "- [TRAIN] LOSS : 0.053923036561657985 [SCORE] : 0.5755908330281575\n",
      "[413/1000]\n",
      "- [VAL] LOSS : 0.06889217346906662 [SCORE] : 0.9484702348709106\n",
      "[414/1000]\n",
      "- [TRAIN] LOSS : 0.05390051600212852 [SCORE] : 0.5755908330281575\n",
      "[414/1000]\n",
      "- [VAL] LOSS : 0.068852499127388 [SCORE] : 0.9484702348709106\n",
      "[415/1000]\n",
      "- [TRAIN] LOSS : 0.05387822321305672 [SCORE] : 0.5755908330281575\n",
      "[415/1000]\n",
      "- [VAL] LOSS : 0.0688130185008049 [SCORE] : 0.9484702348709106\n",
      "[416/1000]\n",
      "- [TRAIN] LOSS : 0.053856151768316825 [SCORE] : 0.5755908330281575\n",
      "[416/1000]\n",
      "- [VAL] LOSS : 0.06877388805150986 [SCORE] : 0.9484702348709106\n",
      "[417/1000]\n",
      "- [TRAIN] LOSS : 0.05383434882387519 [SCORE] : 0.5755908330281575\n",
      "[417/1000]\n",
      "- [VAL] LOSS : 0.06873489171266556 [SCORE] : 0.9484702348709106\n",
      "[418/1000]\n",
      "- [TRAIN] LOSS : 0.053812726059307654 [SCORE] : 0.5755908330281575\n",
      "[418/1000]\n",
      "- [VAL] LOSS : 0.0686962679028511 [SCORE] : 0.9484702348709106\n",
      "[419/1000]\n",
      "- [TRAIN] LOSS : 0.053791342706729965 [SCORE] : 0.5755908330281575\n",
      "[419/1000]\n",
      "- [VAL] LOSS : 0.06865791231393814 [SCORE] : 0.9484702348709106\n",
      "[420/1000]\n",
      "- [TRAIN] LOSS : 0.05377014273156722 [SCORE] : 0.5755908330281575\n",
      "[420/1000]\n",
      "- [VAL] LOSS : 0.06861983239650726 [SCORE] : 0.9484702348709106\n",
      "[421/1000]\n",
      "- [TRAIN] LOSS : 0.05374916388342778 [SCORE] : 0.5755908330281575\n",
      "[421/1000]\n",
      "- [VAL] LOSS : 0.0685819536447525 [SCORE] : 0.9484702348709106\n",
      "[422/1000]\n",
      "- [TRAIN] LOSS : 0.053728393651545046 [SCORE] : 0.5755908330281575\n",
      "[422/1000]\n",
      "- [VAL] LOSS : 0.06854446977376938 [SCORE] : 0.9484702348709106\n",
      "[423/1000]\n",
      "- [TRAIN] LOSS : 0.05370781548942129 [SCORE] : 0.5755908330281575\n",
      "[423/1000]\n",
      "- [VAL] LOSS : 0.06850718706846237 [SCORE] : 0.9484702348709106\n",
      "[424/1000]\n",
      "- [TRAIN] LOSS : 0.053687463887035845 [SCORE] : 0.5755908330281575\n",
      "[424/1000]\n",
      "- [VAL] LOSS : 0.06847011297941208 [SCORE] : 0.9484702348709106\n",
      "[425/1000]\n",
      "- [TRAIN] LOSS : 0.05366729168842236 [SCORE] : 0.5755908330281575\n",
      "[425/1000]\n",
      "- [VAL] LOSS : 0.06843329221010208 [SCORE] : 0.9484702348709106\n",
      "[426/1000]\n",
      "- [TRAIN] LOSS : 0.05364734884351492 [SCORE] : 0.5755908330281575\n",
      "[426/1000]\n",
      "- [VAL] LOSS : 0.06839672476053238 [SCORE] : 0.9484702348709106\n",
      "[427/1000]\n",
      "- [TRAIN] LOSS : 0.0536275510986646 [SCORE] : 0.5755908330281575\n",
      "[427/1000]\n",
      "- [VAL] LOSS : 0.0683603584766388 [SCORE] : 0.9484702348709106\n",
      "[428/1000]\n",
      "- [TRAIN] LOSS : 0.05360796190798282 [SCORE] : 0.5755908330281575\n",
      "[428/1000]\n",
      "- [VAL] LOSS : 0.06832430511713028 [SCORE] : 0.9484702348709106\n",
      "[429/1000]\n",
      "- [TRAIN] LOSS : 0.05358855190376441 [SCORE] : 0.5755908330281575\n",
      "[429/1000]\n",
      "- [VAL] LOSS : 0.06828843802213669 [SCORE] : 0.9484702348709106\n",
      "[430/1000]\n",
      "- [TRAIN] LOSS : 0.05356936100870371 [SCORE] : 0.5755908330281575\n",
      "[430/1000]\n",
      "- [VAL] LOSS : 0.06825282424688339 [SCORE] : 0.9484702348709106\n",
      "[431/1000]\n",
      "- [TRAIN] LOSS : 0.05355032384395599 [SCORE] : 0.5755908330281575\n",
      "[431/1000]\n",
      "- [VAL] LOSS : 0.06821740418672562 [SCORE] : 0.9484702348709106\n",
      "[432/1000]\n",
      "- [TRAIN] LOSS : 0.05353147583082318 [SCORE] : 0.5755908330281575\n",
      "[432/1000]\n",
      "- [VAL] LOSS : 0.06818223744630814 [SCORE] : 0.9484702348709106\n",
      "[433/1000]\n",
      "- [TRAIN] LOSS : 0.053512786670277514 [SCORE] : 0.5755908330281575\n",
      "[433/1000]\n",
      "- [VAL] LOSS : 0.06814733892679214 [SCORE] : 0.9484702348709106\n",
      "[434/1000]\n",
      "- [TRAIN] LOSS : 0.05349431947494546 [SCORE] : 0.5755908330281575\n",
      "[434/1000]\n",
      "- [VAL] LOSS : 0.0681125745177269 [SCORE] : 0.9484702348709106\n",
      "[435/1000]\n",
      "- [TRAIN] LOSS : 0.0534759609028697 [SCORE] : 0.5755908330281575\n",
      "[435/1000]\n",
      "- [VAL] LOSS : 0.06807797402143478 [SCORE] : 0.9484702348709106\n",
      "[436/1000]\n",
      "- [TRAIN] LOSS : 0.05345786161099871 [SCORE] : 0.5755908330281575\n",
      "[436/1000]\n",
      "- [VAL] LOSS : 0.06804365664720535 [SCORE] : 0.9484702348709106\n",
      "[437/1000]\n",
      "- [TRAIN] LOSS : 0.05343995398531357 [SCORE] : 0.5755908330281575\n",
      "[437/1000]\n",
      "- [VAL] LOSS : 0.06800948083400726 [SCORE] : 0.9484702348709106\n",
      "[438/1000]\n",
      "- [TRAIN] LOSS : 0.05342220657815536 [SCORE] : 0.5755908330281575\n",
      "[438/1000]\n",
      "- [VAL] LOSS : 0.06797546148300171 [SCORE] : 0.9484702348709106\n",
      "[439/1000]\n",
      "- [TRAIN] LOSS : 0.05340457806984584 [SCORE] : 0.5755908330281575\n",
      "[439/1000]\n",
      "- [VAL] LOSS : 0.06794171035289764 [SCORE] : 0.9484702348709106\n",
      "[440/1000]\n",
      "- [TRAIN] LOSS : 0.05338712874799967 [SCORE] : 0.5755908330281575\n",
      "[440/1000]\n",
      "- [VAL] LOSS : 0.06790822744369507 [SCORE] : 0.9484702348709106\n",
      "[441/1000]\n",
      "- [TRAIN] LOSS : 0.05336984811971585 [SCORE] : 0.5755908330281575\n",
      "[441/1000]\n",
      "- [VAL] LOSS : 0.0678749829530716 [SCORE] : 0.9484702348709106\n",
      "[442/1000]\n",
      "- [TRAIN] LOSS : 0.05335276837771138 [SCORE] : 0.5755908330281575\n",
      "[442/1000]\n",
      "- [VAL] LOSS : 0.06784192472696304 [SCORE] : 0.9484702348709106\n",
      "[443/1000]\n",
      "- [TRAIN] LOSS : 0.053335837771495184 [SCORE] : 0.5755908330281575\n",
      "[443/1000]\n",
      "- [VAL] LOSS : 0.06780938059091568 [SCORE] : 0.9484702348709106\n",
      "[444/1000]\n",
      "- [TRAIN] LOSS : 0.053319051116704944 [SCORE] : 0.5755908330281575\n",
      "[444/1000]\n",
      "- [VAL] LOSS : 0.06777714937925339 [SCORE] : 0.9484702348709106\n",
      "[445/1000]\n",
      "- [TRAIN] LOSS : 0.0533024488016963 [SCORE] : 0.5755908330281575\n",
      "[445/1000]\n",
      "- [VAL] LOSS : 0.06774509698152542 [SCORE] : 0.9484702348709106\n",
      "[446/1000]\n",
      "- [TRAIN] LOSS : 0.05328593890493115 [SCORE] : 0.5755908330281575\n",
      "[446/1000]\n",
      "- [VAL] LOSS : 0.06771325320005417 [SCORE] : 0.9484702348709106\n",
      "[447/1000]\n",
      "- [TRAIN] LOSS : 0.053269600650916495 [SCORE] : 0.5755908330281575\n",
      "[447/1000]\n",
      "- [VAL] LOSS : 0.06768171489238739 [SCORE] : 0.9484702348709106\n",
      "[448/1000]\n",
      "- [TRAIN] LOSS : 0.0532534461778899 [SCORE] : 0.5755908330281575\n",
      "[448/1000]\n",
      "- [VAL] LOSS : 0.0676501989364624 [SCORE] : 0.9484702348709106\n",
      "[449/1000]\n",
      "- [TRAIN] LOSS : 0.05323743239666025 [SCORE] : 0.5755908330281575\n",
      "[449/1000]\n",
      "- [VAL] LOSS : 0.0676189586520195 [SCORE] : 0.9484702348709106\n",
      "[450/1000]\n",
      "- [TRAIN] LOSS : 0.05322151177873214 [SCORE] : 0.5755908330281575\n",
      "[450/1000]\n",
      "- [VAL] LOSS : 0.06758774816989899 [SCORE] : 0.9484702348709106\n",
      "[451/1000]\n",
      "- [TRAIN] LOSS : 0.05320581666504343 [SCORE] : 0.5755908330281575\n",
      "[451/1000]\n",
      "- [VAL] LOSS : 0.06755682080984116 [SCORE] : 0.9484702348709106\n",
      "[452/1000]\n",
      "- [TRAIN] LOSS : 0.05319021393855413 [SCORE] : 0.5755908330281575\n",
      "[452/1000]\n",
      "- [VAL] LOSS : 0.06752606481313705 [SCORE] : 0.9484702348709106\n",
      "[453/1000]\n",
      "- [TRAIN] LOSS : 0.05317483389129241 [SCORE] : 0.5755908330281575\n",
      "[453/1000]\n",
      "- [VAL] LOSS : 0.0674954205751419 [SCORE] : 0.9484702348709106\n",
      "[454/1000]\n",
      "- [TRAIN] LOSS : 0.05315957212199767 [SCORE] : 0.5755908330281575\n",
      "[454/1000]\n",
      "- [VAL] LOSS : 0.06746494024991989 [SCORE] : 0.9484702348709106\n",
      "[455/1000]\n",
      "- [TRAIN] LOSS : 0.053144334629178044 [SCORE] : 0.5755908330281575\n",
      "[455/1000]\n",
      "- [VAL] LOSS : 0.06743476539850235 [SCORE] : 0.9484702348709106\n",
      "[456/1000]\n",
      "- [TRAIN] LOSS : 0.05312933086728056 [SCORE] : 0.5755908330281575\n",
      "[456/1000]\n",
      "- [VAL] LOSS : 0.06740491092205048 [SCORE] : 0.9484702348709106\n",
      "[457/1000]\n",
      "- [TRAIN] LOSS : 0.05311445898065965 [SCORE] : 0.5755908330281575\n",
      "[457/1000]\n",
      "- [VAL] LOSS : 0.06737519055604935 [SCORE] : 0.9484702348709106\n",
      "[458/1000]\n",
      "- [TRAIN] LOSS : 0.05309969730054339 [SCORE] : 0.5755908330281575\n",
      "[458/1000]\n",
      "- [VAL] LOSS : 0.06734554469585419 [SCORE] : 0.9484702348709106\n",
      "[459/1000]\n",
      "- [TRAIN] LOSS : 0.053085114092876515 [SCORE] : 0.5755908330281575\n",
      "[459/1000]\n",
      "- [VAL] LOSS : 0.06731615215539932 [SCORE] : 0.9484702348709106\n",
      "[460/1000]\n",
      "- [TRAIN] LOSS : 0.0530706242347757 [SCORE] : 0.5755908330281575\n",
      "[460/1000]\n",
      "- [VAL] LOSS : 0.06728687137365341 [SCORE] : 0.9484702348709106\n",
      "[461/1000]\n",
      "- [TRAIN] LOSS : 0.05305625442415476 [SCORE] : 0.5755908330281575\n",
      "[461/1000]\n",
      "- [VAL] LOSS : 0.06725763529539108 [SCORE] : 0.9484702348709106\n",
      "[462/1000]\n",
      "- [TRAIN] LOSS : 0.053042008572568494 [SCORE] : 0.5755908330281575\n",
      "[462/1000]\n",
      "- [VAL] LOSS : 0.06722861528396606 [SCORE] : 0.9484702348709106\n",
      "[463/1000]\n",
      "- [TRAIN] LOSS : 0.05302787671486537 [SCORE] : 0.5755908330281575\n",
      "[463/1000]\n",
      "- [VAL] LOSS : 0.06719974428415298 [SCORE] : 0.9484702348709106\n",
      "[464/1000]\n",
      "- [TRAIN] LOSS : 0.053013855467240016 [SCORE] : 0.5755908330281575\n",
      "[464/1000]\n",
      "- [VAL] LOSS : 0.067171111702919 [SCORE] : 0.9484702348709106\n",
      "[465/1000]\n",
      "- [TRAIN] LOSS : 0.053000042339166005 [SCORE] : 0.5755908330281575\n",
      "[465/1000]\n",
      "- [VAL] LOSS : 0.06714259088039398 [SCORE] : 0.9484702348709106\n",
      "[466/1000]\n",
      "- [TRAIN] LOSS : 0.05298628102367123 [SCORE] : 0.5755908330281575\n",
      "[466/1000]\n",
      "- [VAL] LOSS : 0.06711418926715851 [SCORE] : 0.9484702348709106\n",
      "[467/1000]\n",
      "- [TRAIN] LOSS : 0.05297264317050576 [SCORE] : 0.5755908330281575\n",
      "[467/1000]\n",
      "- [VAL] LOSS : 0.06708589941263199 [SCORE] : 0.9484702348709106\n",
      "[468/1000]\n",
      "- [TRAIN] LOSS : 0.0529591533044974 [SCORE] : 0.5755908330281575\n",
      "[468/1000]\n",
      "- [VAL] LOSS : 0.06705774366855621 [SCORE] : 0.9484702348709106\n",
      "[469/1000]\n",
      "- [TRAIN] LOSS : 0.05294575709849596 [SCORE] : 0.5755908330281575\n",
      "[469/1000]\n",
      "- [VAL] LOSS : 0.06702982634305954 [SCORE] : 0.9484702348709106\n",
      "[470/1000]\n",
      "- [TRAIN] LOSS : 0.05293249574800332 [SCORE] : 0.5755908330281575\n",
      "[470/1000]\n",
      "- [VAL] LOSS : 0.06700200587511063 [SCORE] : 0.9484702348709106\n",
      "[471/1000]\n",
      "- [TRAIN] LOSS : 0.05291931064178546 [SCORE] : 0.5755908330281575\n",
      "[471/1000]\n",
      "- [VAL] LOSS : 0.06697430461645126 [SCORE] : 0.9484702348709106\n",
      "[472/1000]\n",
      "- [TRAIN] LOSS : 0.05290622686346372 [SCORE] : 0.5755908330281575\n",
      "[472/1000]\n",
      "- [VAL] LOSS : 0.06694678962230682 [SCORE] : 0.9484702348709106\n",
      "[473/1000]\n",
      "- [TRAIN] LOSS : 0.05289329858496785 [SCORE] : 0.5755908330281575\n",
      "[473/1000]\n",
      "- [VAL] LOSS : 0.06691936403512955 [SCORE] : 0.9484702348709106\n",
      "[474/1000]\n",
      "- [TRAIN] LOSS : 0.05288041513413191 [SCORE] : 0.5755908330281575\n",
      "[474/1000]\n",
      "- [VAL] LOSS : 0.06689207255840302 [SCORE] : 0.9484702348709106\n",
      "[475/1000]\n",
      "- [TRAIN] LOSS : 0.052867730831106505 [SCORE] : 0.5755908330281575\n",
      "[475/1000]\n",
      "- [VAL] LOSS : 0.06686492264270782 [SCORE] : 0.9484702348709106\n",
      "[476/1000]\n",
      "- [TRAIN] LOSS : 0.052855096043397984 [SCORE] : 0.5755908330281575\n",
      "[476/1000]\n",
      "- [VAL] LOSS : 0.06683789193630219 [SCORE] : 0.9484702348709106\n",
      "[477/1000]\n",
      "- [TRAIN] LOSS : 0.05284255550553401 [SCORE] : 0.5755908330281575\n",
      "[477/1000]\n",
      "- [VAL] LOSS : 0.06681105494499207 [SCORE] : 0.9484702348709106\n",
      "[478/1000]\n",
      "- [TRAIN] LOSS : 0.05283012902364135 [SCORE] : 0.5755908330281575\n",
      "[478/1000]\n",
      "- [VAL] LOSS : 0.06678422540426254 [SCORE] : 0.9484702348709106\n",
      "[479/1000]\n",
      "- [TRAIN] LOSS : 0.05281780548393726 [SCORE] : 0.5755908330281575\n",
      "[479/1000]\n",
      "- [VAL] LOSS : 0.06675759702920914 [SCORE] : 0.9484702348709106\n",
      "[480/1000]\n",
      "- [TRAIN] LOSS : 0.05280571992819508 [SCORE] : 0.5755908330281575\n",
      "[480/1000]\n",
      "- [VAL] LOSS : 0.06673096120357513 [SCORE] : 0.9484702348709106\n",
      "[481/1000]\n",
      "- [TRAIN] LOSS : 0.052793619440247616 [SCORE] : 0.5755908330281575\n",
      "[481/1000]\n",
      "- [VAL] LOSS : 0.06670457124710083 [SCORE] : 0.9484702348709106\n",
      "[482/1000]\n",
      "- [TRAIN] LOSS : 0.05278161646177371 [SCORE] : 0.5755908330281575\n",
      "[482/1000]\n",
      "- [VAL] LOSS : 0.06667841970920563 [SCORE] : 0.9484702348709106\n",
      "[483/1000]\n",
      "- [TRAIN] LOSS : 0.052769687368224065 [SCORE] : 0.5755908330281575\n",
      "[483/1000]\n",
      "- [VAL] LOSS : 0.06665244698524475 [SCORE] : 0.9484702348709106\n",
      "[484/1000]\n",
      "- [TRAIN] LOSS : 0.05275790387143691 [SCORE] : 0.5755908330281575\n",
      "[484/1000]\n",
      "- [VAL] LOSS : 0.06662662327289581 [SCORE] : 0.9484702348709106\n",
      "[485/1000]\n",
      "- [TRAIN] LOSS : 0.05274618547409773 [SCORE] : 0.5755908330281575\n",
      "[485/1000]\n",
      "- [VAL] LOSS : 0.06660065799951553 [SCORE] : 0.9484702348709106\n",
      "[486/1000]\n",
      "- [TRAIN] LOSS : 0.052734611462801693 [SCORE] : 0.5755908330281575\n",
      "[486/1000]\n",
      "- [VAL] LOSS : 0.06657489389181137 [SCORE] : 0.9484702348709106\n",
      "[487/1000]\n",
      "- [TRAIN] LOSS : 0.052723175442467136 [SCORE] : 0.5755908330281575\n",
      "[487/1000]\n",
      "- [VAL] LOSS : 0.0665491595864296 [SCORE] : 0.9484702348709106\n",
      "[488/1000]\n",
      "- [TRAIN] LOSS : 0.05271174187461535 [SCORE] : 0.5755908330281575\n",
      "[488/1000]\n",
      "- [VAL] LOSS : 0.06652354449033737 [SCORE] : 0.9484702348709106\n",
      "[489/1000]\n",
      "- [TRAIN] LOSS : 0.05270048873499036 [SCORE] : 0.5755908330281575\n",
      "[489/1000]\n",
      "- [VAL] LOSS : 0.06649800390005112 [SCORE] : 0.9484702348709106\n",
      "[490/1000]\n",
      "- [TRAIN] LOSS : 0.05268931680669387 [SCORE] : 0.5755908330281575\n",
      "[490/1000]\n",
      "- [VAL] LOSS : 0.06647274643182755 [SCORE] : 0.9484702348709106\n",
      "[491/1000]\n",
      "- [TRAIN] LOSS : 0.05267817741259932 [SCORE] : 0.5755908330281575\n",
      "[491/1000]\n",
      "- [VAL] LOSS : 0.06644746661186218 [SCORE] : 0.9484702348709106\n",
      "[492/1000]\n",
      "- [TRAIN] LOSS : 0.05266719410816829 [SCORE] : 0.5755908330281575\n",
      "[492/1000]\n",
      "- [VAL] LOSS : 0.06642235815525055 [SCORE] : 0.9484702348709106\n",
      "[493/1000]\n",
      "- [TRAIN] LOSS : 0.05265632600833972 [SCORE] : 0.5755908330281575\n",
      "[493/1000]\n",
      "- [VAL] LOSS : 0.06639739125967026 [SCORE] : 0.9484702348709106\n",
      "[494/1000]\n",
      "- [TRAIN] LOSS : 0.05264542972048124 [SCORE] : 0.5755908330281575\n",
      "[494/1000]\n",
      "- [VAL] LOSS : 0.06637248396873474 [SCORE] : 0.9484702348709106\n",
      "[495/1000]\n",
      "- [TRAIN] LOSS : 0.05263467092687885 [SCORE] : 0.5755908330281575\n",
      "[495/1000]\n",
      "- [VAL] LOSS : 0.0663476288318634 [SCORE] : 0.9484702348709106\n",
      "[496/1000]\n",
      "- [TRAIN] LOSS : 0.05262403348460794 [SCORE] : 0.5755908330281575\n",
      "[496/1000]\n",
      "- [VAL] LOSS : 0.06632299721240997 [SCORE] : 0.9484702348709106\n",
      "[497/1000]\n",
      "- [TRAIN] LOSS : 0.05261352950086196 [SCORE] : 0.5755908330281575\n",
      "[497/1000]\n",
      "- [VAL] LOSS : 0.0662984549999237 [SCORE] : 0.9484702348709106\n",
      "[498/1000]\n",
      "- [TRAIN] LOSS : 0.05260300679753224 [SCORE] : 0.5755908330281575\n",
      "[498/1000]\n",
      "- [VAL] LOSS : 0.06627409160137177 [SCORE] : 0.9484702348709106\n",
      "[499/1000]\n",
      "- [TRAIN] LOSS : 0.05259260386228561 [SCORE] : 0.5755908330281575\n",
      "[499/1000]\n",
      "- [VAL] LOSS : 0.06624961644411087 [SCORE] : 0.9484702348709106\n",
      "[500/1000]\n",
      "- [TRAIN] LOSS : 0.052582254322866596 [SCORE] : 0.5755908330281575\n",
      "[500/1000]\n",
      "- [VAL] LOSS : 0.06622534990310669 [SCORE] : 0.9484702348709106\n",
      "[501/1000]\n",
      "- [TRAIN] LOSS : 0.052572042246659595 [SCORE] : 0.5755908330281575\n",
      "[501/1000]\n",
      "- [VAL] LOSS : 0.06620140373706818 [SCORE] : 0.9484702348709106\n",
      "[502/1000]\n",
      "- [TRAIN] LOSS : 0.052561918397744496 [SCORE] : 0.5755908330281575\n",
      "[502/1000]\n",
      "- [VAL] LOSS : 0.06617747247219086 [SCORE] : 0.9484702348709106\n",
      "[503/1000]\n",
      "- [TRAIN] LOSS : 0.052551804265628256 [SCORE] : 0.5755908330281575\n",
      "[503/1000]\n",
      "- [VAL] LOSS : 0.0661536157131195 [SCORE] : 0.9484702348709106\n",
      "[504/1000]\n",
      "- [TRAIN] LOSS : 0.052541831694543364 [SCORE] : 0.5755908330281575\n",
      "[504/1000]\n",
      "- [VAL] LOSS : 0.06612975150346756 [SCORE] : 0.9484702348709106\n",
      "[505/1000]\n",
      "- [TRAIN] LOSS : 0.05253185375283162 [SCORE] : 0.5755908330281575\n",
      "[505/1000]\n",
      "- [VAL] LOSS : 0.06610609591007233 [SCORE] : 0.9484702348709106\n",
      "[506/1000]\n",
      "- [TRAIN] LOSS : 0.05252199346820514 [SCORE] : 0.5755908330281575\n",
      "[506/1000]\n",
      "- [VAL] LOSS : 0.06608252227306366 [SCORE] : 0.9484702348709106\n",
      "[507/1000]\n",
      "- [TRAIN] LOSS : 0.052512257980803646 [SCORE] : 0.5755908330281575\n",
      "[507/1000]\n",
      "- [VAL] LOSS : 0.06605900079011917 [SCORE] : 0.9484702348709106\n",
      "[508/1000]\n",
      "- [TRAIN] LOSS : 0.05250275004655123 [SCORE] : 0.5755908330281575\n",
      "[508/1000]\n",
      "- [VAL] LOSS : 0.06603539735078812 [SCORE] : 0.9484702348709106\n",
      "[509/1000]\n",
      "- [TRAIN] LOSS : 0.05249318713322282 [SCORE] : 0.5755908330281575\n",
      "[509/1000]\n",
      "- [VAL] LOSS : 0.06601209193468094 [SCORE] : 0.9484702348709106\n",
      "[510/1000]\n",
      "- [TRAIN] LOSS : 0.05248360742504398 [SCORE] : 0.5755908330281575\n",
      "[510/1000]\n",
      "- [VAL] LOSS : 0.06598886102437973 [SCORE] : 0.9484702348709106\n",
      "[511/1000]\n",
      "- [TRAIN] LOSS : 0.05247413218021393 [SCORE] : 0.5755908330281575\n",
      "[511/1000]\n",
      "- [VAL] LOSS : 0.06596595793962479 [SCORE] : 0.9484702348709106\n",
      "[512/1000]\n",
      "- [TRAIN] LOSS : 0.05246474090963602 [SCORE] : 0.5755908330281575\n",
      "[512/1000]\n",
      "- [VAL] LOSS : 0.06594306230545044 [SCORE] : 0.9484702348709106\n",
      "[513/1000]\n",
      "- [TRAIN] LOSS : 0.05245544050509731 [SCORE] : 0.5755908330281575\n",
      "[513/1000]\n",
      "- [VAL] LOSS : 0.0659203976392746 [SCORE] : 0.9484702348709106\n",
      "[514/1000]\n",
      "- [TRAIN] LOSS : 0.05244622357810537 [SCORE] : 0.5755908330281575\n",
      "[514/1000]\n",
      "- [VAL] LOSS : 0.06589773297309875 [SCORE] : 0.9484702348709106\n",
      "[515/1000]\n",
      "- [TRAIN] LOSS : 0.052437069329122704 [SCORE] : 0.5755908330281575\n",
      "[515/1000]\n",
      "- [VAL] LOSS : 0.0658750832080841 [SCORE] : 0.9484702348709106\n",
      "[516/1000]\n",
      "- [TRAIN] LOSS : 0.052427955561627944 [SCORE] : 0.5755908330281575\n",
      "[516/1000]\n",
      "- [VAL] LOSS : 0.0658525675535202 [SCORE] : 0.9484702348709106\n",
      "[517/1000]\n",
      "- [TRAIN] LOSS : 0.05241893846541643 [SCORE] : 0.5755908330281575\n",
      "[517/1000]\n",
      "- [VAL] LOSS : 0.06583013385534286 [SCORE] : 0.9484702348709106\n",
      "[518/1000]\n",
      "- [TRAIN] LOSS : 0.05240999395027757 [SCORE] : 0.5755908330281575\n",
      "[518/1000]\n",
      "- [VAL] LOSS : 0.06580784916877747 [SCORE] : 0.9484702348709106\n",
      "[519/1000]\n",
      "- [TRAIN] LOSS : 0.052401114255189896 [SCORE] : 0.5755908330281575\n",
      "[519/1000]\n",
      "- [VAL] LOSS : 0.06578560173511505 [SCORE] : 0.9484702348709106\n",
      "[520/1000]\n",
      "- [TRAIN] LOSS : 0.052392291091382506 [SCORE] : 0.5755908330281575\n",
      "[520/1000]\n",
      "- [VAL] LOSS : 0.06576340645551682 [SCORE] : 0.9484702348709106\n",
      "[521/1000]\n",
      "- [TRAIN] LOSS : 0.052383529984702665 [SCORE] : 0.5755908330281575\n",
      "[521/1000]\n",
      "- [VAL] LOSS : 0.06574130803346634 [SCORE] : 0.9484702348709106\n",
      "[522/1000]\n",
      "- [TRAIN] LOSS : 0.052374786635239916 [SCORE] : 0.5607760190963745\n",
      "[522/1000]\n",
      "- [VAL] LOSS : 0.06571933627128601 [SCORE] : 0.9484702348709106\n",
      "[523/1000]\n",
      "- [TRAIN] LOSS : 0.052366154330472155 [SCORE] : 0.5607760190963745\n",
      "[523/1000]\n",
      "- [VAL] LOSS : 0.06569741666316986 [SCORE] : 0.9484702348709106\n",
      "[524/1000]\n",
      "- [TRAIN] LOSS : 0.05235760218153397 [SCORE] : 0.5607760190963745\n",
      "[524/1000]\n",
      "- [VAL] LOSS : 0.06567555665969849 [SCORE] : 0.9484702348709106\n",
      "[525/1000]\n",
      "- [TRAIN] LOSS : 0.05234907784809669 [SCORE] : 0.5607760190963745\n",
      "[525/1000]\n",
      "- [VAL] LOSS : 0.06565383821725845 [SCORE] : 0.9484702348709106\n",
      "[526/1000]\n",
      "- [TRAIN] LOSS : 0.052340631590535244 [SCORE] : 0.5607760190963745\n",
      "[526/1000]\n",
      "- [VAL] LOSS : 0.06563220173120499 [SCORE] : 0.9484702348709106\n",
      "[527/1000]\n",
      "- [TRAIN] LOSS : 0.052332272132237755 [SCORE] : 0.5607760190963745\n",
      "[527/1000]\n",
      "- [VAL] LOSS : 0.06561063230037689 [SCORE] : 0.9484702348709106\n",
      "[528/1000]\n",
      "- [TRAIN] LOSS : 0.052323930462201435 [SCORE] : 0.5607760190963745\n",
      "[528/1000]\n",
      "- [VAL] LOSS : 0.06558918207883835 [SCORE] : 0.9484702348709106\n",
      "[529/1000]\n",
      "- [TRAIN] LOSS : 0.052315674535930154 [SCORE] : 0.5607760190963745\n",
      "[529/1000]\n",
      "- [VAL] LOSS : 0.0655677318572998 [SCORE] : 0.9484702348709106\n",
      "[530/1000]\n",
      "- [TRAIN] LOSS : 0.05230746415133278 [SCORE] : 0.5607760190963745\n",
      "[530/1000]\n",
      "- [VAL] LOSS : 0.06554649770259857 [SCORE] : 0.9484702348709106\n",
      "[531/1000]\n",
      "- [TRAIN] LOSS : 0.052299289777874944 [SCORE] : 0.5607760190963745\n",
      "[531/1000]\n",
      "- [VAL] LOSS : 0.06552524864673615 [SCORE] : 0.9484702348709106\n",
      "[532/1000]\n",
      "- [TRAIN] LOSS : 0.05229119369760156 [SCORE] : 0.5607760190963745\n",
      "[532/1000]\n",
      "- [VAL] LOSS : 0.06550414860248566 [SCORE] : 0.9484702348709106\n",
      "[533/1000]\n",
      "- [TRAIN] LOSS : 0.05228319832434257 [SCORE] : 0.5607760190963745\n",
      "[533/1000]\n",
      "- [VAL] LOSS : 0.06548306345939636 [SCORE] : 0.9484702348709106\n",
      "[534/1000]\n",
      "- [TRAIN] LOSS : 0.05227524510895212 [SCORE] : 0.5607760190963745\n",
      "[534/1000]\n",
      "- [VAL] LOSS : 0.06546205282211304 [SCORE] : 0.9484702348709106\n",
      "[535/1000]\n",
      "- [TRAIN] LOSS : 0.05226727562646071 [SCORE] : 0.5607760190963745\n",
      "[535/1000]\n",
      "- [VAL] LOSS : 0.06544117629528046 [SCORE] : 0.9484702348709106\n",
      "[536/1000]\n",
      "- [TRAIN] LOSS : 0.05225942190736532 [SCORE] : 0.5607760190963745\n",
      "[536/1000]\n",
      "- [VAL] LOSS : 0.06542030721902847 [SCORE] : 0.9484702348709106\n",
      "[537/1000]\n",
      "- [TRAIN] LOSS : 0.05225159466887514 [SCORE] : 0.5607760190963745\n",
      "[537/1000]\n",
      "- [VAL] LOSS : 0.06539960950613022 [SCORE] : 0.9484702348709106\n",
      "[538/1000]\n",
      "- [TRAIN] LOSS : 0.052243859476099413 [SCORE] : 0.5607760190963745\n",
      "[538/1000]\n",
      "- [VAL] LOSS : 0.065378837287426 [SCORE] : 0.9484702348709106\n",
      "[539/1000]\n",
      "- [TRAIN] LOSS : 0.05223615871121486 [SCORE] : 0.5607760190963745\n",
      "[539/1000]\n",
      "- [VAL] LOSS : 0.06535818427801132 [SCORE] : 0.9484702348709106\n",
      "[540/1000]\n",
      "- [TRAIN] LOSS : 0.05222856464485327 [SCORE] : 0.5607760190963745\n",
      "[540/1000]\n",
      "- [VAL] LOSS : 0.065337635576725 [SCORE] : 0.9484702348709106\n",
      "[541/1000]\n",
      "- [TRAIN] LOSS : 0.05222097123041749 [SCORE] : 0.5607760190963745\n",
      "[541/1000]\n",
      "- [VAL] LOSS : 0.06531713157892227 [SCORE] : 0.9484702348709106\n",
      "[542/1000]\n",
      "- [TRAIN] LOSS : 0.052213445057471594 [SCORE] : 0.5607760190963745\n",
      "[542/1000]\n",
      "- [VAL] LOSS : 0.06529669463634491 [SCORE] : 0.9484702348709106\n",
      "[543/1000]\n",
      "- [TRAIN] LOSS : 0.052205921088655786 [SCORE] : 0.5607760190963745\n",
      "[543/1000]\n",
      "- [VAL] LOSS : 0.06527646631002426 [SCORE] : 0.9484702348709106\n",
      "[544/1000]\n",
      "- [TRAIN] LOSS : 0.0521985266978542 [SCORE] : 0.5607760190963745\n",
      "[544/1000]\n",
      "- [VAL] LOSS : 0.06525629013776779 [SCORE] : 0.9484702348709106\n",
      "[545/1000]\n",
      "- [TRAIN] LOSS : 0.05219115500027935 [SCORE] : 0.5607760190963745\n",
      "[545/1000]\n",
      "- [VAL] LOSS : 0.06523609161376953 [SCORE] : 0.9484702348709106\n",
      "[546/1000]\n",
      "- [TRAIN] LOSS : 0.05218384380762776 [SCORE] : 0.5607760190963745\n",
      "[546/1000]\n",
      "- [VAL] LOSS : 0.06521610170602798 [SCORE] : 0.9484702348709106\n",
      "[547/1000]\n",
      "- [TRAIN] LOSS : 0.05217652547483643 [SCORE] : 0.5607760190963745\n",
      "[547/1000]\n",
      "- [VAL] LOSS : 0.06519607454538345 [SCORE] : 0.9484702348709106\n",
      "[548/1000]\n",
      "- [TRAIN] LOSS : 0.052169261531283456 [SCORE] : 0.5607760190963745\n",
      "[548/1000]\n",
      "- [VAL] LOSS : 0.06517613679170609 [SCORE] : 0.9484702348709106\n",
      "[549/1000]\n",
      "- [TRAIN] LOSS : 0.05216210437938571 [SCORE] : 0.5607760190963745\n",
      "[549/1000]\n",
      "- [VAL] LOSS : 0.06515627354383469 [SCORE] : 0.9484702348709106\n",
      "[550/1000]\n",
      "- [TRAIN] LOSS : 0.05215497047950824 [SCORE] : 0.5607760190963745\n",
      "[550/1000]\n",
      "- [VAL] LOSS : 0.06513648480176926 [SCORE] : 0.9484702348709106\n",
      "[551/1000]\n",
      "- [TRAIN] LOSS : 0.052147904752443235 [SCORE] : 0.5607760190963745\n",
      "[551/1000]\n",
      "- [VAL] LOSS : 0.06511670351028442 [SCORE] : 0.9484702348709106\n",
      "[552/1000]\n",
      "- [TRAIN] LOSS : 0.05214087326700489 [SCORE] : 0.5607760190963745\n",
      "[552/1000]\n",
      "- [VAL] LOSS : 0.06509704142808914 [SCORE] : 0.9484702348709106\n",
      "[553/1000]\n",
      "- [TRAIN] LOSS : 0.05213389160732428 [SCORE] : 0.5607760190963745\n",
      "[553/1000]\n",
      "- [VAL] LOSS : 0.06507743895053864 [SCORE] : 0.9484702348709106\n",
      "[554/1000]\n",
      "- [TRAIN] LOSS : 0.052126937670012315 [SCORE] : 0.5607760190963745\n",
      "[554/1000]\n",
      "- [VAL] LOSS : 0.0650579035282135 [SCORE] : 0.9484702348709106\n",
      "[555/1000]\n",
      "- [TRAIN] LOSS : 0.05212002911915382 [SCORE] : 0.5607760190963745\n",
      "[555/1000]\n",
      "- [VAL] LOSS : 0.0650385245680809 [SCORE] : 0.9484702348709106\n",
      "[556/1000]\n",
      "- [TRAIN] LOSS : 0.05211321872969468 [SCORE] : 0.5607760190963745\n",
      "[556/1000]\n",
      "- [VAL] LOSS : 0.06501905620098114 [SCORE] : 0.9484702348709106\n",
      "[557/1000]\n",
      "- [TRAIN] LOSS : 0.052106400082508726 [SCORE] : 0.5607760190963745\n",
      "[557/1000]\n",
      "- [VAL] LOSS : 0.06499970704317093 [SCORE] : 0.9484702348709106\n",
      "[558/1000]\n",
      "- [TRAIN] LOSS : 0.05209967078020175 [SCORE] : 0.5607760190963745\n",
      "[558/1000]\n",
      "- [VAL] LOSS : 0.06498037278652191 [SCORE] : 0.9484702348709106\n",
      "[559/1000]\n",
      "- [TRAIN] LOSS : 0.052092930395156146 [SCORE] : 0.5607760190963745\n",
      "[559/1000]\n",
      "- [VAL] LOSS : 0.06496123969554901 [SCORE] : 0.9484702348709106\n",
      "[560/1000]\n",
      "- [TRAIN] LOSS : 0.0520862901583314 [SCORE] : 0.5607760190963745\n",
      "[560/1000]\n",
      "- [VAL] LOSS : 0.06494202464818954 [SCORE] : 0.9484702348709106\n",
      "[561/1000]\n",
      "- [TRAIN] LOSS : 0.05207960875704885 [SCORE] : 0.5607760190963745\n",
      "[561/1000]\n",
      "- [VAL] LOSS : 0.06492292135953903 [SCORE] : 0.9484702348709106\n",
      "[562/1000]\n",
      "- [TRAIN] LOSS : 0.052073041908442974 [SCORE] : 0.5607760190963745\n",
      "[562/1000]\n",
      "- [VAL] LOSS : 0.06490395963191986 [SCORE] : 0.9484702348709106\n",
      "[563/1000]\n",
      "- [TRAIN] LOSS : 0.05206658470754822 [SCORE] : 0.5607760190963745\n",
      "[563/1000]\n",
      "- [VAL] LOSS : 0.0648849681019783 [SCORE] : 0.9484702348709106\n",
      "[564/1000]\n",
      "- [TRAIN] LOSS : 0.05206009792163968 [SCORE] : 0.5607760190963745\n",
      "[564/1000]\n",
      "- [VAL] LOSS : 0.06486590951681137 [SCORE] : 0.9484702348709106\n",
      "[565/1000]\n",
      "- [TRAIN] LOSS : 0.05205362287039558 [SCORE] : 0.5607760190963745\n",
      "[565/1000]\n",
      "- [VAL] LOSS : 0.06484707444906235 [SCORE] : 0.9484702348709106\n",
      "[566/1000]\n",
      "- [TRAIN] LOSS : 0.05204719323664904 [SCORE] : 0.5607760190963745\n",
      "[566/1000]\n",
      "- [VAL] LOSS : 0.06482832133769989 [SCORE] : 0.9484702348709106\n",
      "[567/1000]\n",
      "- [TRAIN] LOSS : 0.052040793870886164 [SCORE] : 0.5607760190963745\n",
      "[567/1000]\n",
      "- [VAL] LOSS : 0.06480974704027176 [SCORE] : 0.9484702348709106\n",
      "[568/1000]\n",
      "- [TRAIN] LOSS : 0.052034497385223706 [SCORE] : 0.5607760190963745\n",
      "[568/1000]\n",
      "- [VAL] LOSS : 0.06479109823703766 [SCORE] : 0.9484702348709106\n",
      "[569/1000]\n",
      "- [TRAIN] LOSS : 0.052028208722670874 [SCORE] : 0.5607760190963745\n",
      "[569/1000]\n",
      "- [VAL] LOSS : 0.06477249413728714 [SCORE] : 0.9484702348709106\n",
      "[570/1000]\n",
      "- [TRAIN] LOSS : 0.05202199000244339 [SCORE] : 0.5607760190963745\n",
      "[570/1000]\n",
      "- [VAL] LOSS : 0.0647539347410202 [SCORE] : 0.9484702348709106\n",
      "[571/1000]\n",
      "- [TRAIN] LOSS : 0.05201576488713423 [SCORE] : 0.5607760190963745\n",
      "[571/1000]\n",
      "- [VAL] LOSS : 0.06473547220230103 [SCORE] : 0.9484702348709106\n",
      "[572/1000]\n",
      "- [TRAIN] LOSS : 0.052009644669791064 [SCORE] : 0.5607760190963745\n",
      "[572/1000]\n",
      "- [VAL] LOSS : 0.06471704691648483 [SCORE] : 0.9484702348709106\n",
      "[573/1000]\n",
      "- [TRAIN] LOSS : 0.05200350843369961 [SCORE] : 0.5607760190963745\n",
      "[573/1000]\n",
      "- [VAL] LOSS : 0.0646987110376358 [SCORE] : 0.9484702348709106\n",
      "[574/1000]\n",
      "- [TRAIN] LOSS : 0.05199742093682289 [SCORE] : 0.5607760190963745\n",
      "[574/1000]\n",
      "- [VAL] LOSS : 0.06468036025762558 [SCORE] : 0.9484702348709106\n",
      "[575/1000]\n",
      "- [TRAIN] LOSS : 0.05199136147275567 [SCORE] : 0.5607760190963745\n",
      "[575/1000]\n",
      "- [VAL] LOSS : 0.06466216593980789 [SCORE] : 0.9484702348709106\n",
      "[576/1000]\n",
      "- [TRAIN] LOSS : 0.051985380612313746 [SCORE] : 0.5607760190963745\n",
      "[576/1000]\n",
      "- [VAL] LOSS : 0.0646439790725708 [SCORE] : 0.9484702348709106\n",
      "[577/1000]\n",
      "- [TRAIN] LOSS : 0.05197939872741699 [SCORE] : 0.5607760190963745\n",
      "[577/1000]\n",
      "- [VAL] LOSS : 0.0646258145570755 [SCORE] : 0.9484702348709106\n",
      "[578/1000]\n",
      "- [TRAIN] LOSS : 0.051973485574126246 [SCORE] : 0.5607760190963745\n",
      "[578/1000]\n",
      "- [VAL] LOSS : 0.06460773199796677 [SCORE] : 0.9484702348709106\n",
      "[579/1000]\n",
      "- [TRAIN] LOSS : 0.05196759542450309 [SCORE] : 0.5607760190963745\n",
      "[579/1000]\n",
      "- [VAL] LOSS : 0.06458974629640579 [SCORE] : 0.9484702348709106\n",
      "[580/1000]\n",
      "- [TRAIN] LOSS : 0.05196171688536803 [SCORE] : 0.5607760190963745\n",
      "[580/1000]\n",
      "- [VAL] LOSS : 0.0645717903971672 [SCORE] : 0.9484702348709106\n",
      "[581/1000]\n",
      "- [TRAIN] LOSS : 0.05195590117946267 [SCORE] : 0.5607760190963745\n",
      "[581/1000]\n",
      "- [VAL] LOSS : 0.06455383449792862 [SCORE] : 0.9484702348709106\n",
      "[582/1000]\n",
      "- [TRAIN] LOSS : 0.05195011785253882 [SCORE] : 0.5607760190963745\n",
      "[582/1000]\n",
      "- [VAL] LOSS : 0.06453602761030197 [SCORE] : 0.9484702348709106\n",
      "[583/1000]\n",
      "- [TRAIN] LOSS : 0.05194437137494485 [SCORE] : 0.5607760190963745\n",
      "[583/1000]\n",
      "- [VAL] LOSS : 0.06451816856861115 [SCORE] : 0.9484702348709106\n",
      "[584/1000]\n",
      "- [TRAIN] LOSS : 0.051938708312809466 [SCORE] : 0.5607760190963745\n",
      "[584/1000]\n",
      "- [VAL] LOSS : 0.06450037658214569 [SCORE] : 0.9484702348709106\n",
      "[585/1000]\n",
      "- [TRAIN] LOSS : 0.05193298179656267 [SCORE] : 0.5607760190963745\n",
      "[585/1000]\n",
      "- [VAL] LOSS : 0.06448273360729218 [SCORE] : 0.9484702348709106\n",
      "[586/1000]\n",
      "- [TRAIN] LOSS : 0.05192733487735192 [SCORE] : 0.5607760190963745\n",
      "[586/1000]\n",
      "- [VAL] LOSS : 0.06446503847837448 [SCORE] : 0.9484702348709106\n",
      "[587/1000]\n",
      "- [TRAIN] LOSS : 0.051921731978654864 [SCORE] : 0.5607760190963745\n",
      "[587/1000]\n",
      "- [VAL] LOSS : 0.06444747745990753 [SCORE] : 0.9484702348709106\n",
      "[588/1000]\n",
      "- [TRAIN] LOSS : 0.05191613941763838 [SCORE] : 0.5607760190963745\n",
      "[588/1000]\n",
      "- [VAL] LOSS : 0.06442997604608536 [SCORE] : 0.9484702348709106\n",
      "[589/1000]\n",
      "- [TRAIN] LOSS : 0.051910656255980334 [SCORE] : 0.5607760190963745\n",
      "[589/1000]\n",
      "- [VAL] LOSS : 0.06441240757703781 [SCORE] : 0.9484702348709106\n",
      "[590/1000]\n",
      "- [TRAIN] LOSS : 0.05190514295051495 [SCORE] : 0.5607760190963745\n",
      "[590/1000]\n",
      "- [VAL] LOSS : 0.06439492106437683 [SCORE] : 0.9484702348709106\n",
      "[591/1000]\n",
      "- [TRAIN] LOSS : 0.05189964190746347 [SCORE] : 0.5607760190963745\n",
      "[591/1000]\n",
      "- [VAL] LOSS : 0.06437753885984421 [SCORE] : 0.9484702348709106\n",
      "[592/1000]\n",
      "- [TRAIN] LOSS : 0.05189420804381371 [SCORE] : 0.5607760190963745\n",
      "[592/1000]\n",
      "- [VAL] LOSS : 0.0643601045012474 [SCORE] : 0.9484702348709106\n",
      "[593/1000]\n",
      "- [TRAIN] LOSS : 0.05188879401733478 [SCORE] : 0.5607760190963745\n",
      "[593/1000]\n",
      "- [VAL] LOSS : 0.06434284150600433 [SCORE] : 0.9484702348709106\n",
      "[594/1000]\n",
      "- [TRAIN] LOSS : 0.05188342481851578 [SCORE] : 0.5607760190963745\n",
      "[594/1000]\n",
      "- [VAL] LOSS : 0.06432558596134186 [SCORE] : 0.9484702348709106\n",
      "[595/1000]\n",
      "- [TRAIN] LOSS : 0.05187810442099969 [SCORE] : 0.5607760190963745\n",
      "[595/1000]\n",
      "- [VAL] LOSS : 0.06430835276842117 [SCORE] : 0.9484702348709106\n",
      "[596/1000]\n",
      "- [TRAIN] LOSS : 0.0518727980243663 [SCORE] : 0.5607760190963745\n",
      "[596/1000]\n",
      "- [VAL] LOSS : 0.06429129838943481 [SCORE] : 0.9484702348709106\n",
      "[597/1000]\n",
      "- [TRAIN] LOSS : 0.051867528911679985 [SCORE] : 0.5607760190963745\n",
      "[597/1000]\n",
      "- [VAL] LOSS : 0.06427416950464249 [SCORE] : 0.9484702348709106\n",
      "[598/1000]\n",
      "- [TRAIN] LOSS : 0.051862254211058216 [SCORE] : 0.5607760190963745\n",
      "[598/1000]\n",
      "- [VAL] LOSS : 0.06425704807043076 [SCORE] : 0.9484702348709106\n",
      "[599/1000]\n",
      "- [TRAIN] LOSS : 0.051857030329604946 [SCORE] : 0.5607760190963745\n",
      "[599/1000]\n",
      "- [VAL] LOSS : 0.06424008309841156 [SCORE] : 0.9484702348709106\n",
      "[600/1000]\n",
      "- [TRAIN] LOSS : 0.05185188983256618 [SCORE] : 0.5607760190963745\n",
      "[600/1000]\n",
      "- [VAL] LOSS : 0.0642230361700058 [SCORE] : 0.9484702348709106\n",
      "[601/1000]\n",
      "- [TRAIN] LOSS : 0.05184669699519873 [SCORE] : 0.5607760190963745\n",
      "[601/1000]\n",
      "- [VAL] LOSS : 0.06420613080263138 [SCORE] : 0.9484702348709106\n",
      "[602/1000]\n",
      "- [TRAIN] LOSS : 0.051841597507397336 [SCORE] : 0.5607760190963745\n",
      "[602/1000]\n",
      "- [VAL] LOSS : 0.06418930739164352 [SCORE] : 0.9484702348709106\n",
      "[603/1000]\n",
      "- [TRAIN] LOSS : 0.05183652297904094 [SCORE] : 0.5607760190963745\n",
      "[603/1000]\n",
      "- [VAL] LOSS : 0.06417246907949448 [SCORE] : 0.9484702348709106\n",
      "[604/1000]\n",
      "- [TRAIN] LOSS : 0.051831457670778035 [SCORE] : 0.5607760190963745\n",
      "[604/1000]\n",
      "- [VAL] LOSS : 0.06415564566850662 [SCORE] : 0.9484702348709106\n",
      "[605/1000]\n",
      "- [TRAIN] LOSS : 0.05182639329383771 [SCORE] : 0.5607760190963745\n",
      "[605/1000]\n",
      "- [VAL] LOSS : 0.06413890421390533 [SCORE] : 0.9484702348709106\n",
      "[606/1000]\n",
      "- [TRAIN] LOSS : 0.05182140115648508 [SCORE] : 0.5607760190963745\n",
      "[606/1000]\n",
      "- [VAL] LOSS : 0.06412219256162643 [SCORE] : 0.9484702348709106\n",
      "[607/1000]\n",
      "- [TRAIN] LOSS : 0.0518164386972785 [SCORE] : 0.5607760190963745\n",
      "[607/1000]\n",
      "- [VAL] LOSS : 0.06410560756921768 [SCORE] : 0.9484702348709106\n",
      "[608/1000]\n",
      "- [TRAIN] LOSS : 0.05181151255965233 [SCORE] : 0.5607760190963745\n",
      "[608/1000]\n",
      "- [VAL] LOSS : 0.0640890970826149 [SCORE] : 0.9484702348709106\n",
      "[609/1000]\n",
      "- [TRAIN] LOSS : 0.05180658840884765 [SCORE] : 0.5607760190963745\n",
      "[609/1000]\n",
      "- [VAL] LOSS : 0.06407252699136734 [SCORE] : 0.9484702348709106\n",
      "[610/1000]\n",
      "- [TRAIN] LOSS : 0.05180170061066747 [SCORE] : 0.5607760190963745\n",
      "[610/1000]\n",
      "- [VAL] LOSS : 0.06405599415302277 [SCORE] : 0.9484702348709106\n",
      "[611/1000]\n",
      "- [TRAIN] LOSS : 0.051796855746457975 [SCORE] : 0.5607760190963745\n",
      "[611/1000]\n",
      "- [VAL] LOSS : 0.06403952836990356 [SCORE] : 0.9484702348709106\n",
      "[612/1000]\n",
      "- [TRAIN] LOSS : 0.051791978875796 [SCORE] : 0.5607760190963745\n",
      "[612/1000]\n",
      "- [VAL] LOSS : 0.06402308493852615 [SCORE] : 0.9484702348709106\n",
      "[613/1000]\n",
      "- [TRAIN] LOSS : 0.05178718008100987 [SCORE] : 0.5607760190963745\n",
      "[613/1000]\n",
      "- [VAL] LOSS : 0.06400676816701889 [SCORE] : 0.9484702348709106\n",
      "[614/1000]\n",
      "- [TRAIN] LOSS : 0.05178242021550735 [SCORE] : 0.5607760190963745\n",
      "[614/1000]\n",
      "- [VAL] LOSS : 0.06399048864841461 [SCORE] : 0.9484702348709106\n",
      "[615/1000]\n",
      "- [TRAIN] LOSS : 0.05177765510355433 [SCORE] : 0.5607760190963745\n",
      "[615/1000]\n",
      "- [VAL] LOSS : 0.06397420167922974 [SCORE] : 0.9484702348709106\n",
      "[616/1000]\n",
      "- [TRAIN] LOSS : 0.05177294127643108 [SCORE] : 0.5607760190963745\n",
      "[616/1000]\n",
      "- [VAL] LOSS : 0.06395791471004486 [SCORE] : 0.9484702348709106\n",
      "[617/1000]\n",
      "- [TRAIN] LOSS : 0.05176826358462373 [SCORE] : 0.5607760190963745\n",
      "[617/1000]\n",
      "- [VAL] LOSS : 0.06394168734550476 [SCORE] : 0.9484702348709106\n",
      "[618/1000]\n",
      "- [TRAIN] LOSS : 0.05176357481007775 [SCORE] : 0.5607760190963745\n",
      "[618/1000]\n",
      "- [VAL] LOSS : 0.06392551958560944 [SCORE] : 0.9484702348709106\n",
      "[619/1000]\n",
      "- [TRAIN] LOSS : 0.05175896150370439 [SCORE] : 0.5607760190963745\n",
      "[619/1000]\n",
      "- [VAL] LOSS : 0.06390947848558426 [SCORE] : 0.9484702348709106\n",
      "[620/1000]\n",
      "- [TRAIN] LOSS : 0.05175435959051053 [SCORE] : 0.5607760190963745\n",
      "[620/1000]\n",
      "- [VAL] LOSS : 0.0638933777809143 [SCORE] : 0.9484702348709106\n",
      "[621/1000]\n",
      "- [TRAIN] LOSS : 0.05174972318733732 [SCORE] : 0.5607760190963745\n",
      "[621/1000]\n",
      "- [VAL] LOSS : 0.06387735158205032 [SCORE] : 0.9484702348709106\n",
      "[622/1000]\n",
      "- [TRAIN] LOSS : 0.051745176470528044 [SCORE] : 0.5607760190963745\n",
      "[622/1000]\n",
      "- [VAL] LOSS : 0.06386139988899231 [SCORE] : 0.9484702348709106\n",
      "[623/1000]\n",
      "- [TRAIN] LOSS : 0.0517406535645326 [SCORE] : 0.5607760190963745\n",
      "[623/1000]\n",
      "- [VAL] LOSS : 0.0638454407453537 [SCORE] : 0.9484702348709106\n",
      "[624/1000]\n",
      "- [TRAIN] LOSS : 0.051736121128002806 [SCORE] : 0.5607760190963745\n",
      "[624/1000]\n",
      "- [VAL] LOSS : 0.06382957845926285 [SCORE] : 0.9484702348709106\n",
      "[625/1000]\n",
      "- [TRAIN] LOSS : 0.05173161436493198 [SCORE] : 0.5607760190963745\n",
      "[625/1000]\n",
      "- [VAL] LOSS : 0.06381367892026901 [SCORE] : 0.9484702348709106\n",
      "[626/1000]\n",
      "- [TRAIN] LOSS : 0.051727185926089686 [SCORE] : 0.5607760190963745\n",
      "[626/1000]\n",
      "- [VAL] LOSS : 0.06379786878824234 [SCORE] : 0.9484702348709106\n",
      "[627/1000]\n",
      "- [TRAIN] LOSS : 0.05172273858139912 [SCORE] : 0.5607760190963745\n",
      "[627/1000]\n",
      "- [VAL] LOSS : 0.06378205865621567 [SCORE] : 0.9484702348709106\n",
      "[628/1000]\n",
      "- [TRAIN] LOSS : 0.05171831700329979 [SCORE] : 0.5607760190963745\n",
      "[628/1000]\n",
      "- [VAL] LOSS : 0.06376636028289795 [SCORE] : 0.9484702348709106\n",
      "[629/1000]\n",
      "- [TRAIN] LOSS : 0.05171391156812509 [SCORE] : 0.5607760190963745\n",
      "[629/1000]\n",
      "- [VAL] LOSS : 0.06375052034854889 [SCORE] : 0.9484702348709106\n",
      "[630/1000]\n",
      "- [TRAIN] LOSS : 0.051709519761304064 [SCORE] : 0.5607760190963745\n",
      "[630/1000]\n",
      "- [VAL] LOSS : 0.06373492628335953 [SCORE] : 0.9484702348709106\n",
      "[631/1000]\n",
      "- [TRAIN] LOSS : 0.05170521878947814 [SCORE] : 0.5607760190963745\n",
      "[631/1000]\n",
      "- [VAL] LOSS : 0.06371928751468658 [SCORE] : 0.9484702348709106\n",
      "[632/1000]\n",
      "- [TRAIN] LOSS : 0.05170091325417161 [SCORE] : 0.5607760190963745\n",
      "[632/1000]\n",
      "- [VAL] LOSS : 0.06370367854833603 [SCORE] : 0.9484702348709106\n",
      "[633/1000]\n",
      "- [TRAIN] LOSS : 0.05169657589867711 [SCORE] : 0.5607760190963745\n",
      "[633/1000]\n",
      "- [VAL] LOSS : 0.06368814408779144 [SCORE] : 0.9484702348709106\n",
      "[634/1000]\n",
      "- [TRAIN] LOSS : 0.051692309758315486 [SCORE] : 0.5607760190963745\n",
      "[634/1000]\n",
      "- [VAL] LOSS : 0.06367258727550507 [SCORE] : 0.9484702348709106\n",
      "[635/1000]\n",
      "- [TRAIN] LOSS : 0.05168808872501055 [SCORE] : 0.5607760190963745\n",
      "[635/1000]\n",
      "- [VAL] LOSS : 0.06365714222192764 [SCORE] : 0.9484702348709106\n",
      "[636/1000]\n",
      "- [TRAIN] LOSS : 0.051683867319176596 [SCORE] : 0.5607760190963745\n",
      "[636/1000]\n",
      "- [VAL] LOSS : 0.0636417344212532 [SCORE] : 0.9484702348709106\n",
      "[637/1000]\n",
      "- [TRAIN] LOSS : 0.05167964417487383 [SCORE] : 0.5607760190963745\n",
      "[637/1000]\n",
      "- [VAL] LOSS : 0.06362632662057877 [SCORE] : 0.9484702348709106\n",
      "[638/1000]\n",
      "- [TRAIN] LOSS : 0.051675459990898766 [SCORE] : 0.5607760190963745\n",
      "[638/1000]\n",
      "- [VAL] LOSS : 0.06361088901758194 [SCORE] : 0.9484702348709106\n",
      "[639/1000]\n",
      "- [TRAIN] LOSS : 0.05167132808516423 [SCORE] : 0.5607760190963745\n",
      "[639/1000]\n",
      "- [VAL] LOSS : 0.06359563767910004 [SCORE] : 0.9484702348709106\n",
      "[640/1000]\n",
      "- [TRAIN] LOSS : 0.051667169885089 [SCORE] : 0.5607760190963745\n",
      "[640/1000]\n",
      "- [VAL] LOSS : 0.06358028948307037 [SCORE] : 0.9484702348709106\n",
      "[641/1000]\n",
      "- [TRAIN] LOSS : 0.051663064987709124 [SCORE] : 0.5607760190963745\n",
      "[641/1000]\n",
      "- [VAL] LOSS : 0.06356503069400787 [SCORE] : 0.9484702348709106\n",
      "[642/1000]\n",
      "- [TRAIN] LOSS : 0.051658981417616205 [SCORE] : 0.5607760190963745\n",
      "[642/1000]\n",
      "- [VAL] LOSS : 0.06354983896017075 [SCORE] : 0.9484702348709106\n",
      "[643/1000]\n",
      "- [TRAIN] LOSS : 0.051654925507803755 [SCORE] : 0.5607760190963745\n",
      "[643/1000]\n",
      "- [VAL] LOSS : 0.06353465467691422 [SCORE] : 0.9484702348709106\n",
      "[644/1000]\n",
      "- [TRAIN] LOSS : 0.05165085227539142 [SCORE] : 0.5607760190963745\n",
      "[644/1000]\n",
      "- [VAL] LOSS : 0.06351947039365768 [SCORE] : 0.9484702348709106\n",
      "[645/1000]\n",
      "- [TRAIN] LOSS : 0.051646851313610874 [SCORE] : 0.5607760190963745\n",
      "[645/1000]\n",
      "- [VAL] LOSS : 0.06350427120923996 [SCORE] : 0.9484702348709106\n",
      "[646/1000]\n",
      "- [TRAIN] LOSS : 0.05164280785247684 [SCORE] : 0.5607760190963745\n",
      "[646/1000]\n",
      "- [VAL] LOSS : 0.06348928064107895 [SCORE] : 0.9484702348709106\n",
      "[647/1000]\n",
      "- [TRAIN] LOSS : 0.05163893109808366 [SCORE] : 0.5607760190963745\n",
      "[647/1000]\n",
      "- [VAL] LOSS : 0.06347410380840302 [SCORE] : 0.9484702348709106\n",
      "[648/1000]\n",
      "- [TRAIN] LOSS : 0.05163496883275608 [SCORE] : 0.5607760190963745\n",
      "[648/1000]\n",
      "- [VAL] LOSS : 0.06345894187688828 [SCORE] : 0.9484702348709106\n",
      "[649/1000]\n",
      "- [TRAIN] LOSS : 0.05163087924011052 [SCORE] : 0.5607760190963745\n",
      "[649/1000]\n",
      "- [VAL] LOSS : 0.0634441152215004 [SCORE] : 0.9484702348709106\n",
      "[650/1000]\n",
      "- [TRAIN] LOSS : 0.0516269495865951 [SCORE] : 0.5607760190963745\n",
      "[650/1000]\n",
      "- [VAL] LOSS : 0.06342930346727371 [SCORE] : 0.9484702348709106\n",
      "[651/1000]\n",
      "- [TRAIN] LOSS : 0.05162311292563875 [SCORE] : 0.5607760190963745\n",
      "[651/1000]\n",
      "- [VAL] LOSS : 0.06341435760259628 [SCORE] : 0.9484702348709106\n",
      "[652/1000]\n",
      "- [TRAIN] LOSS : 0.05161936852770547 [SCORE] : 0.5607760190963745\n",
      "[652/1000]\n",
      "- [VAL] LOSS : 0.0633992999792099 [SCORE] : 0.9484702348709106\n",
      "[653/1000]\n",
      "- [TRAIN] LOSS : 0.05161531864044567 [SCORE] : 0.5607760190963745\n",
      "[653/1000]\n",
      "- [VAL] LOSS : 0.0633845329284668 [SCORE] : 0.9484702348709106\n",
      "[654/1000]\n",
      "- [TRAIN] LOSS : 0.05161155580232541 [SCORE] : 0.5607760190963745\n",
      "[654/1000]\n",
      "- [VAL] LOSS : 0.06336983293294907 [SCORE] : 0.9484702348709106\n",
      "[655/1000]\n",
      "- [TRAIN] LOSS : 0.051607815207292636 [SCORE] : 0.5607760190963745\n",
      "[655/1000]\n",
      "- [VAL] LOSS : 0.06335490196943283 [SCORE] : 0.9484702348709106\n",
      "[656/1000]\n",
      "- [TRAIN] LOSS : 0.051603901525959374 [SCORE] : 0.5607760190963745\n",
      "[656/1000]\n",
      "- [VAL] LOSS : 0.06334027647972107 [SCORE] : 0.9484702348709106\n",
      "[657/1000]\n",
      "- [TRAIN] LOSS : 0.051600175149117904 [SCORE] : 0.5607760190963745\n",
      "[657/1000]\n",
      "- [VAL] LOSS : 0.06332556903362274 [SCORE] : 0.9484702348709106\n",
      "[658/1000]\n",
      "- [TRAIN] LOSS : 0.051596465660259126 [SCORE] : 0.5607760190963745\n",
      "[658/1000]\n",
      "- [VAL] LOSS : 0.06331074237823486 [SCORE] : 0.9484702348709106\n",
      "[659/1000]\n",
      "- [TRAIN] LOSS : 0.05159259385739764 [SCORE] : 0.5607760190963745\n",
      "[659/1000]\n",
      "- [VAL] LOSS : 0.06329618394374847 [SCORE] : 0.9484702348709106\n",
      "[660/1000]\n",
      "- [TRAIN] LOSS : 0.05158893909926216 [SCORE] : 0.5607760190963745\n",
      "[660/1000]\n",
      "- [VAL] LOSS : 0.06328161060810089 [SCORE] : 0.9484702348709106\n",
      "[661/1000]\n",
      "- [TRAIN] LOSS : 0.05158533308034142 [SCORE] : 0.5607760190963745\n",
      "[661/1000]\n",
      "- [VAL] LOSS : 0.06326694786548615 [SCORE] : 0.9484702348709106\n",
      "[662/1000]\n",
      "- [TRAIN] LOSS : 0.051581489155068994 [SCORE] : 0.5607760190963745\n",
      "[662/1000]\n",
      "- [VAL] LOSS : 0.06325246393680573 [SCORE] : 0.9484702348709106\n",
      "[663/1000]\n",
      "- [TRAIN] LOSS : 0.05157789777343472 [SCORE] : 0.5607760190963745\n",
      "[663/1000]\n",
      "- [VAL] LOSS : 0.06323794275522232 [SCORE] : 0.9484702348709106\n",
      "[664/1000]\n",
      "- [TRAIN] LOSS : 0.051574317769457895 [SCORE] : 0.5607760190963745\n",
      "[664/1000]\n",
      "- [VAL] LOSS : 0.06322333961725235 [SCORE] : 0.9484702348709106\n",
      "[665/1000]\n",
      "- [TRAIN] LOSS : 0.051570640162875256 [SCORE] : 0.5607760190963745\n",
      "[665/1000]\n",
      "- [VAL] LOSS : 0.06320886313915253 [SCORE] : 0.9484702348709106\n",
      "[666/1000]\n",
      "- [TRAIN] LOSS : 0.05156704445059101 [SCORE] : 0.5607760190963745\n",
      "[666/1000]\n",
      "- [VAL] LOSS : 0.0631944090127945 [SCORE] : 0.9484702348709106\n",
      "[667/1000]\n",
      "- [TRAIN] LOSS : 0.051563323754817245 [SCORE] : 0.5607760190963745\n",
      "[667/1000]\n",
      "- [VAL] LOSS : 0.06318020075559616 [SCORE] : 0.9484702348709106\n",
      "[668/1000]\n",
      "- [TRAIN] LOSS : 0.051559831496948996 [SCORE] : 0.5607760190963745\n",
      "[668/1000]\n",
      "- [VAL] LOSS : 0.06316594779491425 [SCORE] : 0.9484702348709106\n",
      "[669/1000]\n",
      "- [TRAIN] LOSS : 0.05155636458657682 [SCORE] : 0.5607760190963745\n",
      "[669/1000]\n",
      "- [VAL] LOSS : 0.06315145641565323 [SCORE] : 0.9484702348709106\n",
      "[670/1000]\n",
      "- [TRAIN] LOSS : 0.0515527425489078 [SCORE] : 0.5607760190963745\n",
      "[670/1000]\n",
      "- [VAL] LOSS : 0.06313715875148773 [SCORE] : 0.9484702348709106\n",
      "[671/1000]\n",
      "- [TRAIN] LOSS : 0.05154930045828223 [SCORE] : 0.5607760190963745\n",
      "[671/1000]\n",
      "- [VAL] LOSS : 0.06312272697687149 [SCORE] : 0.9484702348709106\n",
      "[672/1000]\n",
      "- [TRAIN] LOSS : 0.051545645979543524 [SCORE] : 0.5607760190963745\n",
      "[672/1000]\n",
      "- [VAL] LOSS : 0.06310869753360748 [SCORE] : 0.9484702348709106\n",
      "[673/1000]\n",
      "- [TRAIN] LOSS : 0.0515422104857862 [SCORE] : 0.5607760190963745\n",
      "[673/1000]\n",
      "- [VAL] LOSS : 0.06309448182582855 [SCORE] : 0.9484702348709106\n",
      "[674/1000]\n",
      "- [TRAIN] LOSS : 0.05153887604052822 [SCORE] : 0.5607760190963745\n",
      "[674/1000]\n",
      "- [VAL] LOSS : 0.06308011710643768 [SCORE] : 0.9484702348709106\n",
      "[675/1000]\n",
      "- [TRAIN] LOSS : 0.05153536186553538 [SCORE] : 0.5607760190963745\n",
      "[675/1000]\n",
      "- [VAL] LOSS : 0.06306587159633636 [SCORE] : 0.9484702348709106\n",
      "[676/1000]\n",
      "- [TRAIN] LOSS : 0.05153182155142228 [SCORE] : 0.5607760190963745\n",
      "[676/1000]\n",
      "- [VAL] LOSS : 0.06305190920829773 [SCORE] : 0.9484702348709106\n",
      "[677/1000]\n",
      "- [TRAIN] LOSS : 0.05152845128128926 [SCORE] : 0.5607760190963745\n",
      "[677/1000]\n",
      "- [VAL] LOSS : 0.06303796917200089 [SCORE] : 0.9484702348709106\n",
      "[678/1000]\n",
      "- [TRAIN] LOSS : 0.051525111527492604 [SCORE] : 0.5607760190963745\n",
      "[678/1000]\n",
      "- [VAL] LOSS : 0.06302375346422195 [SCORE] : 0.9484702348709106\n",
      "[679/1000]\n",
      "- [TRAIN] LOSS : 0.051521725673228505 [SCORE] : 0.5607760190963745\n",
      "[679/1000]\n",
      "- [VAL] LOSS : 0.06300970166921616 [SCORE] : 0.9484702348709106\n",
      "[680/1000]\n",
      "- [TRAIN] LOSS : 0.05151838104551037 [SCORE] : 0.5607760190963745\n",
      "[680/1000]\n",
      "- [VAL] LOSS : 0.0629955530166626 [SCORE] : 0.9484702348709106\n",
      "[681/1000]\n",
      "- [TRAIN] LOSS : 0.051514962268993256 [SCORE] : 0.5607760190963745\n",
      "[681/1000]\n",
      "- [VAL] LOSS : 0.06298167258501053 [SCORE] : 0.9484702348709106\n",
      "[682/1000]\n",
      "- [TRAIN] LOSS : 0.05151163746292393 [SCORE] : 0.5607760190963745\n",
      "[682/1000]\n",
      "- [VAL] LOSS : 0.06296767294406891 [SCORE] : 0.9484702348709106\n",
      "[683/1000]\n",
      "- [TRAIN] LOSS : 0.05150827987429996 [SCORE] : 0.5607760190963745\n",
      "[683/1000]\n",
      "- [VAL] LOSS : 0.06295378506183624 [SCORE] : 0.9484702348709106\n",
      "[684/1000]\n",
      "- [TRAIN] LOSS : 0.051505016690740986 [SCORE] : 0.5607760190963745\n",
      "[684/1000]\n",
      "- [VAL] LOSS : 0.06293982267379761 [SCORE] : 0.9484702348709106\n",
      "[685/1000]\n",
      "- [TRAIN] LOSS : 0.051501694104323786 [SCORE] : 0.5607760190963745\n",
      "[685/1000]\n",
      "- [VAL] LOSS : 0.0629260241985321 [SCORE] : 0.9484702348709106\n",
      "[686/1000]\n",
      "- [TRAIN] LOSS : 0.05149850078547994 [SCORE] : 0.5607760190963745\n",
      "[686/1000]\n",
      "- [VAL] LOSS : 0.06291203200817108 [SCORE] : 0.9484702348709106\n",
      "[687/1000]\n",
      "- [TRAIN] LOSS : 0.051495184345791735 [SCORE] : 0.5607760190963745\n",
      "[687/1000]\n",
      "- [VAL] LOSS : 0.0628981813788414 [SCORE] : 0.9484702348709106\n",
      "[688/1000]\n",
      "- [TRAIN] LOSS : 0.05149181039693455 [SCORE] : 0.5607760190963745\n",
      "[688/1000]\n",
      "- [VAL] LOSS : 0.06288459151983261 [SCORE] : 0.9484702348709106\n",
      "[689/1000]\n",
      "- [TRAIN] LOSS : 0.0514886573733141 [SCORE] : 0.5607760190963745\n",
      "[689/1000]\n",
      "- [VAL] LOSS : 0.06287092715501785 [SCORE] : 0.9484702348709106\n",
      "[690/1000]\n",
      "- [TRAIN] LOSS : 0.05148561857640743 [SCORE] : 0.5607760190963745\n",
      "[690/1000]\n",
      "- [VAL] LOSS : 0.0628570094704628 [SCORE] : 0.9484702348709106\n",
      "[691/1000]\n",
      "- [TRAIN] LOSS : 0.051482356572523716 [SCORE] : 0.5607760190963745\n",
      "[691/1000]\n",
      "- [VAL] LOSS : 0.06284315884113312 [SCORE] : 0.9484702348709106\n",
      "[692/1000]\n",
      "- [TRAIN] LOSS : 0.051479065759728354 [SCORE] : 0.5607760190963745\n",
      "[692/1000]\n",
      "- [VAL] LOSS : 0.06282950192689896 [SCORE] : 0.9484702348709106\n",
      "[693/1000]\n",
      "- [TRAIN] LOSS : 0.05147591979863743 [SCORE] : 0.5607760190963745\n",
      "[693/1000]\n",
      "- [VAL] LOSS : 0.06281588971614838 [SCORE] : 0.9484702348709106\n",
      "[694/1000]\n",
      "- [TRAIN] LOSS : 0.051472791191190484 [SCORE] : 0.5607760190963745\n",
      "[694/1000]\n",
      "- [VAL] LOSS : 0.06280236691236496 [SCORE] : 0.9484702348709106\n",
      "[695/1000]\n",
      "- [TRAIN] LOSS : 0.05146972641038398 [SCORE] : 0.5607760190963745\n",
      "[695/1000]\n",
      "- [VAL] LOSS : 0.06278859078884125 [SCORE] : 0.9484702348709106\n",
      "[696/1000]\n",
      "- [TRAIN] LOSS : 0.05146655899782975 [SCORE] : 0.5607760190963745\n",
      "[696/1000]\n",
      "- [VAL] LOSS : 0.06277494877576828 [SCORE] : 0.9484702348709106\n",
      "[697/1000]\n",
      "- [TRAIN] LOSS : 0.05146331368014216 [SCORE] : 0.5607760190963745\n",
      "[697/1000]\n",
      "- [VAL] LOSS : 0.06276156008243561 [SCORE] : 0.9484702348709106\n",
      "[698/1000]\n",
      "- [TRAIN] LOSS : 0.051460292531798284 [SCORE] : 0.5607760190963745\n",
      "[698/1000]\n",
      "- [VAL] LOSS : 0.06274814903736115 [SCORE] : 0.9484702348709106\n",
      "[699/1000]\n",
      "- [TRAIN] LOSS : 0.051457377274831136 [SCORE] : 0.5607760190963745\n",
      "[699/1000]\n",
      "- [VAL] LOSS : 0.06273450702428818 [SCORE] : 0.9484702348709106\n",
      "[700/1000]\n",
      "- [TRAIN] LOSS : 0.051454288170983395 [SCORE] : 0.5607760190963745\n",
      "[700/1000]\n",
      "- [VAL] LOSS : 0.06272082775831223 [SCORE] : 0.9484702348709106\n",
      "[701/1000]\n",
      "- [TRAIN] LOSS : 0.05145112588070333 [SCORE] : 0.5607760190963745\n",
      "[701/1000]\n",
      "- [VAL] LOSS : 0.06270735710859299 [SCORE] : 0.9484702348709106\n",
      "[702/1000]\n",
      "- [TRAIN] LOSS : 0.05144805024998884 [SCORE] : 0.5607760190963745\n",
      "[702/1000]\n",
      "- [VAL] LOSS : 0.06269408762454987 [SCORE] : 0.9484702348709106\n",
      "[703/1000]\n",
      "- [TRAIN] LOSS : 0.05144505783294638 [SCORE] : 0.5607760190963745\n",
      "[703/1000]\n",
      "- [VAL] LOSS : 0.06268071383237839 [SCORE] : 0.9484702348709106\n",
      "[704/1000]\n",
      "- [TRAIN] LOSS : 0.0514420193930467 [SCORE] : 0.5607760190963745\n",
      "[704/1000]\n",
      "- [VAL] LOSS : 0.06266741454601288 [SCORE] : 0.9484702348709106\n",
      "[705/1000]\n",
      "- [TRAIN] LOSS : 0.05143911950290203 [SCORE] : 0.5607760190963745\n",
      "[705/1000]\n",
      "- [VAL] LOSS : 0.06265395134687424 [SCORE] : 0.9484702348709106\n",
      "[706/1000]\n",
      "- [TRAIN] LOSS : 0.05143608775300284 [SCORE] : 0.5607760190963745\n",
      "[706/1000]\n",
      "- [VAL] LOSS : 0.06264054775238037 [SCORE] : 0.9484702348709106\n",
      "[707/1000]\n",
      "- [TRAIN] LOSS : 0.05143304346129298 [SCORE] : 0.5607760190963745\n",
      "[707/1000]\n",
      "- [VAL] LOSS : 0.06262732297182083 [SCORE] : 0.9484702348709106\n",
      "[708/1000]\n",
      "- [TRAIN] LOSS : 0.05143018801075717 [SCORE] : 0.5607760190963745\n",
      "[708/1000]\n",
      "- [VAL] LOSS : 0.06261391937732697 [SCORE] : 0.9484702348709106\n",
      "[709/1000]\n",
      "- [TRAIN] LOSS : 0.05142721766605973 [SCORE] : 0.5607760190963745\n",
      "[709/1000]\n",
      "- [VAL] LOSS : 0.06260063499212265 [SCORE] : 0.9484702348709106\n",
      "[710/1000]\n",
      "- [TRAIN] LOSS : 0.05142423400344948 [SCORE] : 0.5607760190963745\n",
      "[710/1000]\n",
      "- [VAL] LOSS : 0.06258751451969147 [SCORE] : 0.9484702348709106\n",
      "[711/1000]\n",
      "- [TRAIN] LOSS : 0.05142139764502644 [SCORE] : 0.5607760190963745\n",
      "[711/1000]\n",
      "- [VAL] LOSS : 0.06257425248622894 [SCORE] : 0.9484702348709106\n",
      "[712/1000]\n",
      "- [TRAIN] LOSS : 0.05141842400965591 [SCORE] : 0.5607760190963745\n",
      "[712/1000]\n",
      "- [VAL] LOSS : 0.06256107240915298 [SCORE] : 0.9484702348709106\n",
      "[713/1000]\n",
      "- [TRAIN] LOSS : 0.05141566670499742 [SCORE] : 0.5607760190963745\n",
      "[713/1000]\n",
      "- [VAL] LOSS : 0.06254766881465912 [SCORE] : 0.9484702348709106\n",
      "[714/1000]\n",
      "- [TRAIN] LOSS : 0.05141286179423332 [SCORE] : 0.5607760190963745\n",
      "[714/1000]\n",
      "- [VAL] LOSS : 0.06253410130739212 [SCORE] : 0.9484702348709106\n",
      "[715/1000]\n",
      "- [TRAIN] LOSS : 0.05140987286965052 [SCORE] : 0.5607760190963745\n",
      "[715/1000]\n",
      "- [VAL] LOSS : 0.06252085417509079 [SCORE] : 0.9484702348709106\n",
      "[716/1000]\n",
      "- [TRAIN] LOSS : 0.051406906762470804 [SCORE] : 0.5607760190963745\n",
      "[716/1000]\n",
      "- [VAL] LOSS : 0.06250780075788498 [SCORE] : 0.9484702348709106\n",
      "[717/1000]\n",
      "- [TRAIN] LOSS : 0.05140417216656109 [SCORE] : 0.5607760190963745\n",
      "[717/1000]\n",
      "- [VAL] LOSS : 0.06249472126364708 [SCORE] : 0.9484702348709106\n",
      "[718/1000]\n",
      "- [TRAIN] LOSS : 0.05140130864456296 [SCORE] : 0.5607760190963745\n",
      "[718/1000]\n",
      "- [VAL] LOSS : 0.06248172000050545 [SCORE] : 0.9484702348709106\n",
      "[719/1000]\n",
      "- [TRAIN] LOSS : 0.05139844850637019 [SCORE] : 0.5607760190963745\n",
      "[719/1000]\n",
      "- [VAL] LOSS : 0.06246877461671829 [SCORE] : 0.9484702348709106\n",
      "[720/1000]\n",
      "- [TRAIN] LOSS : 0.05139572381352385 [SCORE] : 0.5607760190963745\n",
      "[720/1000]\n",
      "- [VAL] LOSS : 0.062455710023641586 [SCORE] : 0.9484702348709106\n",
      "[721/1000]\n",
      "- [TRAIN] LOSS : 0.05139293007863065 [SCORE] : 0.5607760190963745\n",
      "[721/1000]\n",
      "- [VAL] LOSS : 0.062442559748888016 [SCORE] : 0.9484702348709106\n",
      "[722/1000]\n",
      "- [TRAIN] LOSS : 0.05139011419378221 [SCORE] : 0.5607760190963745\n",
      "[722/1000]\n",
      "- [VAL] LOSS : 0.062429703772068024 [SCORE] : 0.9484702348709106\n",
      "[723/1000]\n",
      "- [TRAIN] LOSS : 0.051387278952946265 [SCORE] : 0.5607760190963745\n",
      "[723/1000]\n",
      "- [VAL] LOSS : 0.06241695210337639 [SCORE] : 0.9484702348709106\n",
      "[724/1000]\n",
      "- [TRAIN] LOSS : 0.05138462376780808 [SCORE] : 0.5607760190963745\n",
      "[724/1000]\n",
      "- [VAL] LOSS : 0.06240394338965416 [SCORE] : 0.9484702348709106\n",
      "[725/1000]\n",
      "- [TRAIN] LOSS : 0.05138184276099007 [SCORE] : 0.5607760190963745\n",
      "[725/1000]\n",
      "- [VAL] LOSS : 0.06239105388522148 [SCORE] : 0.9484702348709106\n",
      "[726/1000]\n",
      "- [TRAIN] LOSS : 0.05137910017122825 [SCORE] : 0.5607760190963745\n",
      "[726/1000]\n",
      "- [VAL] LOSS : 0.06237817928195 [SCORE] : 0.9484702348709106\n",
      "[727/1000]\n",
      "- [TRAIN] LOSS : 0.05137646324001253 [SCORE] : 0.5607760190963745\n",
      "[727/1000]\n",
      "- [VAL] LOSS : 0.06236523762345314 [SCORE] : 0.9484702348709106\n",
      "[728/1000]\n",
      "- [TRAIN] LOSS : 0.05137373902834952 [SCORE] : 0.5607760190963745\n",
      "[728/1000]\n",
      "- [VAL] LOSS : 0.062352366745471954 [SCORE] : 0.9484702348709106\n",
      "[729/1000]\n",
      "- [TRAIN] LOSS : 0.05137101341970265 [SCORE] : 0.5607760190963745\n",
      "[729/1000]\n",
      "- [VAL] LOSS : 0.06233959645032883 [SCORE] : 0.9484702348709106\n",
      "[730/1000]\n",
      "- [TRAIN] LOSS : 0.05136822435694436 [SCORE] : 0.5607760190963745\n",
      "[730/1000]\n",
      "- [VAL] LOSS : 0.062326978892087936 [SCORE] : 0.9484702348709106\n",
      "[731/1000]\n",
      "- [TRAIN] LOSS : 0.05136569120610754 [SCORE] : 0.5607760190963745\n",
      "[731/1000]\n",
      "- [VAL] LOSS : 0.06231410428881645 [SCORE] : 0.9484702348709106\n",
      "[732/1000]\n",
      "- [TRAIN] LOSS : 0.05136300828307867 [SCORE] : 0.5607760190963745\n",
      "[732/1000]\n",
      "- [VAL] LOSS : 0.062301281839609146 [SCORE] : 0.9484702348709106\n",
      "[733/1000]\n",
      "- [TRAIN] LOSS : 0.05136032933369279 [SCORE] : 0.5607760190963745\n",
      "[733/1000]\n",
      "- [VAL] LOSS : 0.062288545072078705 [SCORE] : 0.9484702348709106\n",
      "[734/1000]\n",
      "- [TRAIN] LOSS : 0.05135761761727432 [SCORE] : 0.5607760190963745\n",
      "[734/1000]\n",
      "- [VAL] LOSS : 0.06227598711848259 [SCORE] : 0.9484702348709106\n",
      "[735/1000]\n",
      "- [TRAIN] LOSS : 0.05135507535499831 [SCORE] : 0.5607760190963745\n",
      "[735/1000]\n",
      "- [VAL] LOSS : 0.06226326897740364 [SCORE] : 0.9484702348709106\n",
      "[736/1000]\n",
      "- [TRAIN] LOSS : 0.051352481140444675 [SCORE] : 0.5607760190963745\n",
      "[736/1000]\n",
      "- [VAL] LOSS : 0.062250543385744095 [SCORE] : 0.9484702348709106\n",
      "[737/1000]\n",
      "- [TRAIN] LOSS : 0.05134979390228788 [SCORE] : 0.5607760190963745\n",
      "[737/1000]\n",
      "- [VAL] LOSS : 0.06223783642053604 [SCORE] : 0.9484702348709106\n",
      "[738/1000]\n",
      "- [TRAIN] LOSS : 0.05134713367248575 [SCORE] : 0.5607760190963745\n",
      "[738/1000]\n",
      "- [VAL] LOSS : 0.062225375324487686 [SCORE] : 0.9484702348709106\n",
      "[739/1000]\n",
      "- [TRAIN] LOSS : 0.05134471184574067 [SCORE] : 0.5607760190963745\n",
      "[739/1000]\n",
      "- [VAL] LOSS : 0.062212709337472916 [SCORE] : 0.9484702348709106\n",
      "[740/1000]\n",
      "- [TRAIN] LOSS : 0.051342113455757496 [SCORE] : 0.5607760190963745\n",
      "[740/1000]\n",
      "- [VAL] LOSS : 0.06220003217458725 [SCORE] : 0.9484702348709106\n",
      "[741/1000]\n",
      "- [TRAIN] LOSS : 0.05133948811950783 [SCORE] : 0.5607760190963745\n",
      "[741/1000]\n",
      "- [VAL] LOSS : 0.06218751147389412 [SCORE] : 0.9484702348709106\n",
      "[742/1000]\n",
      "- [TRAIN] LOSS : 0.05133690650885304 [SCORE] : 0.5607760190963745\n",
      "[742/1000]\n",
      "- [VAL] LOSS : 0.062174998223781586 [SCORE] : 0.9484702348709106\n",
      "[743/1000]\n",
      "- [TRAIN] LOSS : 0.05133432416866223 [SCORE] : 0.5607760190963745\n",
      "[743/1000]\n",
      "- [VAL] LOSS : 0.062162697315216064 [SCORE] : 0.9484702348709106\n",
      "[744/1000]\n",
      "- [TRAIN] LOSS : 0.05133190530662735 [SCORE] : 0.5607760190963745\n",
      "[744/1000]\n",
      "- [VAL] LOSS : 0.06215012073516846 [SCORE] : 0.9484702348709106\n",
      "[745/1000]\n",
      "- [TRAIN] LOSS : 0.05132939317263663 [SCORE] : 0.5607760190963745\n",
      "[745/1000]\n",
      "- [VAL] LOSS : 0.062137532979249954 [SCORE] : 0.9484702348709106\n",
      "[746/1000]\n",
      "- [TRAIN] LOSS : 0.05132686722402771 [SCORE] : 0.5607760190963745\n",
      "[746/1000]\n",
      "- [VAL] LOSS : 0.062124963849782944 [SCORE] : 0.9484702348709106\n",
      "[747/1000]\n",
      "- [TRAIN] LOSS : 0.05132431535360714 [SCORE] : 0.5607760190963745\n",
      "[747/1000]\n",
      "- [VAL] LOSS : 0.06211259216070175 [SCORE] : 0.9484702348709106\n",
      "[748/1000]\n",
      "- [TRAIN] LOSS : 0.051321716994668046 [SCORE] : 0.5607760190963745\n",
      "[748/1000]\n",
      "- [VAL] LOSS : 0.06210040673613548 [SCORE] : 0.9484702348709106\n",
      "[749/1000]\n",
      "- [TRAIN] LOSS : 0.051319369953125714 [SCORE] : 0.5607760190963745\n",
      "[749/1000]\n",
      "- [VAL] LOSS : 0.062087882310152054 [SCORE] : 0.9484702348709106\n",
      "[750/1000]\n",
      "- [TRAIN] LOSS : 0.051316904928535224 [SCORE] : 0.5607760190963745\n",
      "[750/1000]\n",
      "- [VAL] LOSS : 0.062075335532426834 [SCORE] : 0.9484702348709106\n",
      "[751/1000]\n",
      "- [TRAIN] LOSS : 0.0513144431480517 [SCORE] : 0.5607760190963745\n",
      "[751/1000]\n",
      "- [VAL] LOSS : 0.06206294521689415 [SCORE] : 0.9484702348709106\n",
      "[752/1000]\n",
      "- [TRAIN] LOSS : 0.0513119130084912 [SCORE] : 0.5607760190963745\n",
      "[752/1000]\n",
      "- [VAL] LOSS : 0.06205066666007042 [SCORE] : 0.9484702348709106\n",
      "[753/1000]\n",
      "- [TRAIN] LOSS : 0.05130940976863106 [SCORE] : 0.5607760190963745\n",
      "[753/1000]\n",
      "- [VAL] LOSS : 0.06203857436776161 [SCORE] : 0.9484702348709106\n",
      "[754/1000]\n",
      "- [TRAIN] LOSS : 0.051307115827997525 [SCORE] : 0.5607760190963745\n",
      "[754/1000]\n",
      "- [VAL] LOSS : 0.062026143074035645 [SCORE] : 0.9484702348709106\n",
      "[755/1000]\n",
      "- [TRAIN] LOSS : 0.05130468091617028 [SCORE] : 0.5607760190963745\n",
      "[755/1000]\n",
      "- [VAL] LOSS : 0.06201377138495445 [SCORE] : 0.9484702348709106\n",
      "[756/1000]\n",
      "- [TRAIN] LOSS : 0.05130220741654436 [SCORE] : 0.5607760190963745\n",
      "[756/1000]\n",
      "- [VAL] LOSS : 0.06200152263045311 [SCORE] : 0.9484702348709106\n",
      "[757/1000]\n",
      "- [TRAIN] LOSS : 0.051299846110244594 [SCORE] : 0.5607760190963745\n",
      "[757/1000]\n",
      "- [VAL] LOSS : 0.06198922172188759 [SCORE] : 0.9484702348709106\n",
      "[758/1000]\n",
      "- [TRAIN] LOSS : 0.051297341007739305 [SCORE] : 0.5607760190963745\n",
      "[758/1000]\n",
      "- [VAL] LOSS : 0.06197703629732132 [SCORE] : 0.9484702348709106\n",
      "[759/1000]\n",
      "- [TRAIN] LOSS : 0.051294927733639875 [SCORE] : 0.5607760190963745\n",
      "[759/1000]\n",
      "- [VAL] LOSS : 0.061964984983205795 [SCORE] : 0.9484702348709106\n",
      "[760/1000]\n",
      "- [TRAIN] LOSS : 0.05129265294720729 [SCORE] : 0.5607760190963745\n",
      "[760/1000]\n",
      "- [VAL] LOSS : 0.06195276975631714 [SCORE] : 0.9484702348709106\n",
      "[761/1000]\n",
      "- [TRAIN] LOSS : 0.05129031197478374 [SCORE] : 0.5607760190963745\n",
      "[761/1000]\n",
      "- [VAL] LOSS : 0.061940524727106094 [SCORE] : 0.9484702348709106\n",
      "[762/1000]\n",
      "- [TRAIN] LOSS : 0.05128795304335654 [SCORE] : 0.5607760190963745\n",
      "[762/1000]\n",
      "- [VAL] LOSS : 0.06192826107144356 [SCORE] : 0.9484702348709106\n",
      "[763/1000]\n",
      "- [TRAIN] LOSS : 0.05128558037492136 [SCORE] : 0.5607760190963745\n",
      "[763/1000]\n",
      "- [VAL] LOSS : 0.061916105449199677 [SCORE] : 0.9484702348709106\n",
      "[764/1000]\n",
      "- [TRAIN] LOSS : 0.05128315642165641 [SCORE] : 0.5607760190963745\n",
      "[764/1000]\n",
      "- [VAL] LOSS : 0.06190407648682594 [SCORE] : 0.9484702348709106\n",
      "[765/1000]\n",
      "- [TRAIN] LOSS : 0.0512807909399271 [SCORE] : 0.5607760190963745\n",
      "[765/1000]\n",
      "- [VAL] LOSS : 0.061891984194517136 [SCORE] : 0.9484702348709106\n",
      "[766/1000]\n",
      "- [TRAIN] LOSS : 0.051278458923722306 [SCORE] : 0.5607760190963745\n",
      "[766/1000]\n",
      "- [VAL] LOSS : 0.06187999248504639 [SCORE] : 0.9484702348709106\n",
      "[767/1000]\n",
      "- [TRAIN] LOSS : 0.051276256795972586 [SCORE] : 0.5607760190963745\n",
      "[767/1000]\n",
      "- [VAL] LOSS : 0.06186791881918907 [SCORE] : 0.9484702348709106\n",
      "[768/1000]\n",
      "- [TRAIN] LOSS : 0.05127394671241443 [SCORE] : 0.5607760190963745\n",
      "[768/1000]\n",
      "- [VAL] LOSS : 0.061855874955654144 [SCORE] : 0.9484702348709106\n",
      "[769/1000]\n",
      "- [TRAIN] LOSS : 0.05127157863850395 [SCORE] : 0.5607760190963745\n",
      "[769/1000]\n",
      "- [VAL] LOSS : 0.061843715608119965 [SCORE] : 0.9484702348709106\n",
      "[770/1000]\n",
      "- [TRAIN] LOSS : 0.051269284014900524 [SCORE] : 0.5607760190963745\n",
      "[770/1000]\n",
      "- [VAL] LOSS : 0.06183170899748802 [SCORE] : 0.9484702348709106\n",
      "[771/1000]\n",
      "- [TRAIN] LOSS : 0.051266989577561616 [SCORE] : 0.5607760190963745\n",
      "[771/1000]\n",
      "- [VAL] LOSS : 0.06181969866156578 [SCORE] : 0.9484702348709106\n",
      "[772/1000]\n",
      "- [TRAIN] LOSS : 0.051264747697860005 [SCORE] : 0.5607760190963745\n",
      "[772/1000]\n",
      "- [VAL] LOSS : 0.06180771067738533 [SCORE] : 0.9484702348709106\n",
      "[773/1000]\n",
      "- [TRAIN] LOSS : 0.051262426842004064 [SCORE] : 0.5607760190963745\n",
      "[773/1000]\n",
      "- [VAL] LOSS : 0.06179582700133324 [SCORE] : 0.9484702348709106\n",
      "[774/1000]\n",
      "- [TRAIN] LOSS : 0.05126017328972618 [SCORE] : 0.5607760190963745\n",
      "[774/1000]\n",
      "- [VAL] LOSS : 0.0617840513586998 [SCORE] : 0.9484702348709106\n",
      "[775/1000]\n",
      "- [TRAIN] LOSS : 0.05125803054931263 [SCORE] : 0.5607760190963745\n",
      "[775/1000]\n",
      "- [VAL] LOSS : 0.06177205592393875 [SCORE] : 0.9484702348709106\n",
      "[776/1000]\n",
      "- [TRAIN] LOSS : 0.0512557888093094 [SCORE] : 0.5607760190963745\n",
      "[776/1000]\n",
      "- [VAL] LOSS : 0.06176009401679039 [SCORE] : 0.9484702348709106\n",
      "[777/1000]\n",
      "- [TRAIN] LOSS : 0.05125351906754076 [SCORE] : 0.5607760190963745\n",
      "[777/1000]\n",
      "- [VAL] LOSS : 0.061748113483190536 [SCORE] : 0.9484702348709106\n",
      "[778/1000]\n",
      "- [TRAIN] LOSS : 0.05125126149505377 [SCORE] : 0.5607760190963745\n",
      "[778/1000]\n",
      "- [VAL] LOSS : 0.06173621863126755 [SCORE] : 0.9484702348709106\n",
      "[779/1000]\n",
      "- [TRAIN] LOSS : 0.051249018978948395 [SCORE] : 0.5607760190963745\n",
      "[779/1000]\n",
      "- [VAL] LOSS : 0.061724454164505005 [SCORE] : 0.9484702348709106\n",
      "[780/1000]\n",
      "- [TRAIN] LOSS : 0.051246837324773274 [SCORE] : 0.5607760190963745\n",
      "[780/1000]\n",
      "- [VAL] LOSS : 0.061712611466646194 [SCORE] : 0.9484702348709106\n",
      "[781/1000]\n",
      "- [TRAIN] LOSS : 0.05124464151449502 [SCORE] : 0.5607760190963745\n",
      "[781/1000]\n",
      "- [VAL] LOSS : 0.061700738966464996 [SCORE] : 0.9484702348709106\n",
      "[782/1000]\n",
      "- [TRAIN] LOSS : 0.05124241303031643 [SCORE] : 0.5607760190963745\n",
      "[782/1000]\n",
      "- [VAL] LOSS : 0.061688993126153946 [SCORE] : 0.9484702348709106\n",
      "[783/1000]\n",
      "- [TRAIN] LOSS : 0.05124018027757605 [SCORE] : 0.5607760190963745\n",
      "[783/1000]\n",
      "- [VAL] LOSS : 0.06167725473642349 [SCORE] : 0.9484702348709106\n",
      "[784/1000]\n",
      "- [TRAIN] LOSS : 0.05123798733887573 [SCORE] : 0.5607760190963745\n",
      "[784/1000]\n",
      "- [VAL] LOSS : 0.061665527522563934 [SCORE] : 0.9484702348709106\n",
      "[785/1000]\n",
      "- [TRAIN] LOSS : 0.051235920004546645 [SCORE] : 0.5607760190963745\n",
      "[785/1000]\n",
      "- [VAL] LOSS : 0.06165375933051109 [SCORE] : 0.9484702348709106\n",
      "[786/1000]\n",
      "- [TRAIN] LOSS : 0.05123379086144268 [SCORE] : 0.5607760190963745\n",
      "[786/1000]\n",
      "- [VAL] LOSS : 0.0616418793797493 [SCORE] : 0.9484702348709106\n",
      "[787/1000]\n",
      "- [TRAIN] LOSS : 0.05123156919144094 [SCORE] : 0.5607760190963745\n",
      "[787/1000]\n",
      "- [VAL] LOSS : 0.06163020804524422 [SCORE] : 0.9484702348709106\n",
      "[788/1000]\n",
      "- [TRAIN] LOSS : 0.051229413598775864 [SCORE] : 0.5607760190963745\n",
      "[788/1000]\n",
      "- [VAL] LOSS : 0.06161853298544884 [SCORE] : 0.9484702348709106\n",
      "[789/1000]\n",
      "- [TRAIN] LOSS : 0.05122726506864031 [SCORE] : 0.5607760190963745\n",
      "[789/1000]\n",
      "- [VAL] LOSS : 0.0616067610681057 [SCORE] : 0.9484702348709106\n",
      "[790/1000]\n",
      "- [TRAIN] LOSS : 0.05122513046177725 [SCORE] : 0.5607760190963745\n",
      "[790/1000]\n",
      "- [VAL] LOSS : 0.061595115810632706 [SCORE] : 0.9484702348709106\n",
      "[791/1000]\n",
      "- [TRAIN] LOSS : 0.051222984384124474 [SCORE] : 0.5607760190963745\n",
      "[791/1000]\n",
      "- [VAL] LOSS : 0.06158352643251419 [SCORE] : 0.9484702348709106\n",
      "[792/1000]\n",
      "- [TRAIN] LOSS : 0.05122087694083651 [SCORE] : 0.5607760190963745\n",
      "[792/1000]\n",
      "- [VAL] LOSS : 0.06157190725207329 [SCORE] : 0.9484702348709106\n",
      "[793/1000]\n",
      "- [TRAIN] LOSS : 0.05121881437177459 [SCORE] : 0.5607760190963745\n",
      "[793/1000]\n",
      "- [VAL] LOSS : 0.06156023591756821 [SCORE] : 0.9484702348709106\n",
      "[794/1000]\n",
      "- [TRAIN] LOSS : 0.05121665325326224 [SCORE] : 0.5607760190963745\n",
      "[794/1000]\n",
      "- [VAL] LOSS : 0.061548616737127304 [SCORE] : 0.9484702348709106\n",
      "[795/1000]\n",
      "- [TRAIN] LOSS : 0.051214576388398804 [SCORE] : 0.5607760190963745\n",
      "[795/1000]\n",
      "- [VAL] LOSS : 0.061536986380815506 [SCORE] : 0.9484702348709106\n",
      "[796/1000]\n",
      "- [TRAIN] LOSS : 0.051212464629982905 [SCORE] : 0.5607760190963745\n",
      "[796/1000]\n",
      "- [VAL] LOSS : 0.06152547523379326 [SCORE] : 0.9484702348709106\n",
      "[797/1000]\n",
      "- [TRAIN] LOSS : 0.051210413019483286 [SCORE] : 0.5607760190963745\n",
      "[797/1000]\n",
      "- [VAL] LOSS : 0.061513904482126236 [SCORE] : 0.9484702348709106\n",
      "[798/1000]\n",
      "- [TRAIN] LOSS : 0.05120833879336715 [SCORE] : 0.5607760190963745\n",
      "[798/1000]\n",
      "- [VAL] LOSS : 0.061502326279878616 [SCORE] : 0.9484702348709106\n",
      "[799/1000]\n",
      "- [TRAIN] LOSS : 0.05120627535507083 [SCORE] : 0.5607760190963745\n",
      "[799/1000]\n",
      "- [VAL] LOSS : 0.06149079650640488 [SCORE] : 0.9484702348709106\n",
      "[800/1000]\n",
      "- [TRAIN] LOSS : 0.05120418546721339 [SCORE] : 0.5607760190963745\n",
      "[800/1000]\n",
      "- [VAL] LOSS : 0.061479292809963226 [SCORE] : 0.9484702348709106\n",
      "[801/1000]\n",
      "- [TRAIN] LOSS : 0.051202143325159946 [SCORE] : 0.5607760190963745\n",
      "[801/1000]\n",
      "- [VAL] LOSS : 0.061467885971069336 [SCORE] : 0.9484702348709106\n",
      "[802/1000]\n",
      "- [TRAIN] LOSS : 0.05120011605322361 [SCORE] : 0.5607760190963745\n",
      "[802/1000]\n",
      "- [VAL] LOSS : 0.06145633012056351 [SCORE] : 0.9484702348709106\n",
      "[803/1000]\n",
      "- [TRAIN] LOSS : 0.05119809041110178 [SCORE] : 0.5607760190963745\n",
      "[803/1000]\n",
      "- [VAL] LOSS : 0.06144481897354126 [SCORE] : 0.9484702348709106\n",
      "[804/1000]\n",
      "- [TRAIN] LOSS : 0.05119605500561496 [SCORE] : 0.5607760190963745\n",
      "[804/1000]\n",
      "- [VAL] LOSS : 0.06143331900238991 [SCORE] : 0.9484702348709106\n",
      "[805/1000]\n",
      "- [TRAIN] LOSS : 0.0511940175977846 [SCORE] : 0.5607760190963745\n",
      "[805/1000]\n",
      "- [VAL] LOSS : 0.061421770602464676 [SCORE] : 0.9484702348709106\n",
      "[806/1000]\n",
      "- [TRAIN] LOSS : 0.05119195512185494 [SCORE] : 0.5607760190963745\n",
      "[806/1000]\n",
      "- [VAL] LOSS : 0.061410415917634964 [SCORE] : 0.9484702348709106\n",
      "[807/1000]\n",
      "- [TRAIN] LOSS : 0.051189958273122706 [SCORE] : 0.5607760190963745\n",
      "[807/1000]\n",
      "- [VAL] LOSS : 0.06139909103512764 [SCORE] : 0.9484702348709106\n",
      "[808/1000]\n",
      "- [TRAIN] LOSS : 0.05118794795125723 [SCORE] : 0.5607760190963745\n",
      "[808/1000]\n",
      "- [VAL] LOSS : 0.06138760596513748 [SCORE] : 0.9484702348709106\n",
      "[809/1000]\n",
      "- [TRAIN] LOSS : 0.05118597646554311 [SCORE] : 0.5607760190963745\n",
      "[809/1000]\n",
      "- [VAL] LOSS : 0.06137629598379135 [SCORE] : 0.9484702348709106\n",
      "[810/1000]\n",
      "- [TRAIN] LOSS : 0.051183978421613575 [SCORE] : 0.5607760190963745\n",
      "[810/1000]\n",
      "- [VAL] LOSS : 0.06136489659547806 [SCORE] : 0.9484702348709106\n",
      "[811/1000]\n",
      "- [TRAIN] LOSS : 0.051181971871604524 [SCORE] : 0.5607760190963745\n",
      "[811/1000]\n",
      "- [VAL] LOSS : 0.061353545635938644 [SCORE] : 0.9484702348709106\n",
      "[812/1000]\n",
      "- [TRAIN] LOSS : 0.05118000671888391 [SCORE] : 0.5607760190963745\n",
      "[812/1000]\n",
      "- [VAL] LOSS : 0.06134221702814102 [SCORE] : 0.9484702348709106\n",
      "[813/1000]\n",
      "- [TRAIN] LOSS : 0.05117800738662481 [SCORE] : 0.5607760190963745\n",
      "[813/1000]\n",
      "- [VAL] LOSS : 0.061330873519182205 [SCORE] : 0.9484702348709106\n",
      "[814/1000]\n",
      "- [TRAIN] LOSS : 0.05117603636657198 [SCORE] : 0.5607760190963745\n",
      "[814/1000]\n",
      "- [VAL] LOSS : 0.061319656670093536 [SCORE] : 0.9484702348709106\n",
      "[815/1000]\n",
      "- [TRAIN] LOSS : 0.051174077189837894 [SCORE] : 0.5607760190963745\n",
      "[815/1000]\n",
      "- [VAL] LOSS : 0.06130828335881233 [SCORE] : 0.9484702348709106\n",
      "[816/1000]\n",
      "- [TRAIN] LOSS : 0.05117215621285141 [SCORE] : 0.5607760190963745\n",
      "[816/1000]\n",
      "- [VAL] LOSS : 0.061296962201595306 [SCORE] : 0.9484702348709106\n",
      "[817/1000]\n",
      "- [TRAIN] LOSS : 0.051170199473078055 [SCORE] : 0.5607760190963745\n",
      "[817/1000]\n",
      "- [VAL] LOSS : 0.06128575652837753 [SCORE] : 0.9484702348709106\n",
      "[818/1000]\n",
      "- [TRAIN] LOSS : 0.051168234723930554 [SCORE] : 0.5607760190963745\n",
      "[818/1000]\n",
      "- [VAL] LOSS : 0.06127455085515976 [SCORE] : 0.9484702348709106\n",
      "[819/1000]\n",
      "- [TRAIN] LOSS : 0.05116628101095557 [SCORE] : 0.5607760190963745\n",
      "[819/1000]\n",
      "- [VAL] LOSS : 0.06126319617033005 [SCORE] : 0.9484702348709106\n",
      "[820/1000]\n",
      "- [TRAIN] LOSS : 0.051164387601117294 [SCORE] : 0.5607760190963745\n",
      "[820/1000]\n",
      "- [VAL] LOSS : 0.061251964420080185 [SCORE] : 0.9484702348709106\n",
      "[821/1000]\n",
      "- [TRAIN] LOSS : 0.051162434198583163 [SCORE] : 0.5607760190963745\n",
      "[821/1000]\n",
      "- [VAL] LOSS : 0.0612407922744751 [SCORE] : 0.9484702348709106\n",
      "[822/1000]\n",
      "- [TRAIN] LOSS : 0.051160523279880486 [SCORE] : 0.5607760190963745\n",
      "[822/1000]\n",
      "- [VAL] LOSS : 0.06122955307364464 [SCORE] : 0.9484702348709106\n",
      "[823/1000]\n",
      "- [TRAIN] LOSS : 0.05115858068068822 [SCORE] : 0.5607760190963745\n",
      "[823/1000]\n",
      "- [VAL] LOSS : 0.06121845170855522 [SCORE] : 0.9484702348709106\n",
      "[824/1000]\n",
      "- [TRAIN] LOSS : 0.05115667982026935 [SCORE] : 0.5607760190963745\n",
      "[824/1000]\n",
      "- [VAL] LOSS : 0.06120738387107849 [SCORE] : 0.9484702348709106\n",
      "[825/1000]\n",
      "- [TRAIN] LOSS : 0.051154789964978895 [SCORE] : 0.5607760190963745\n",
      "[825/1000]\n",
      "- [VAL] LOSS : 0.06119616702198982 [SCORE] : 0.9484702348709106\n",
      "[826/1000]\n",
      "- [TRAIN] LOSS : 0.05115293113825222 [SCORE] : 0.5607760190963745\n",
      "[826/1000]\n",
      "- [VAL] LOSS : 0.06118498370051384 [SCORE] : 0.9484702348709106\n",
      "[827/1000]\n",
      "- [TRAIN] LOSS : 0.051151012210175394 [SCORE] : 0.5607760190963745\n",
      "[827/1000]\n",
      "- [VAL] LOSS : 0.061173826456069946 [SCORE] : 0.9484702348709106\n",
      "[828/1000]\n",
      "- [TRAIN] LOSS : 0.051149089153235155 [SCORE] : 0.5607760190963745\n",
      "[828/1000]\n",
      "- [VAL] LOSS : 0.06116277351975441 [SCORE] : 0.9484702348709106\n",
      "[829/1000]\n",
      "- [TRAIN] LOSS : 0.051147247493887944 [SCORE] : 0.5607760190963745\n",
      "[829/1000]\n",
      "- [VAL] LOSS : 0.061151765286922455 [SCORE] : 0.9484702348709106\n",
      "[830/1000]\n",
      "- [TRAIN] LOSS : 0.051145396459226805 [SCORE] : 0.5607760190963745\n",
      "[830/1000]\n",
      "- [VAL] LOSS : 0.061140600591897964 [SCORE] : 0.9484702348709106\n",
      "[831/1000]\n",
      "- [TRAIN] LOSS : 0.05114351608790457 [SCORE] : 0.5607760190963745\n",
      "[831/1000]\n",
      "- [VAL] LOSS : 0.061129551380872726 [SCORE] : 0.9484702348709106\n",
      "[832/1000]\n",
      "- [TRAIN] LOSS : 0.05114166525502999 [SCORE] : 0.5607760190963745\n",
      "[832/1000]\n",
      "- [VAL] LOSS : 0.06111842021346092 [SCORE] : 0.9484702348709106\n",
      "[833/1000]\n",
      "- [TRAIN] LOSS : 0.05113980903600653 [SCORE] : 0.5607760190963745\n",
      "[833/1000]\n",
      "- [VAL] LOSS : 0.06110728904604912 [SCORE] : 0.9484702348709106\n",
      "[834/1000]\n",
      "- [TRAIN] LOSS : 0.0511379814085861 [SCORE] : 0.5607760190963745\n",
      "[834/1000]\n",
      "- [VAL] LOSS : 0.06109635904431343 [SCORE] : 0.9484702348709106\n",
      "[835/1000]\n",
      "- [TRAIN] LOSS : 0.051136155442024274 [SCORE] : 0.5607760190963745\n",
      "[835/1000]\n",
      "- [VAL] LOSS : 0.06108531355857849 [SCORE] : 0.9484702348709106\n",
      "[836/1000]\n",
      "- [TRAIN] LOSS : 0.05113431919987003 [SCORE] : 0.5607760190963745\n",
      "[836/1000]\n",
      "- [VAL] LOSS : 0.061074305325746536 [SCORE] : 0.9484702348709106\n",
      "[837/1000]\n",
      "- [TRAIN] LOSS : 0.05113246928279599 [SCORE] : 0.5607760190963745\n",
      "[837/1000]\n",
      "- [VAL] LOSS : 0.06106333062052727 [SCORE] : 0.9484702348709106\n",
      "[838/1000]\n",
      "- [TRAIN] LOSS : 0.05113065992482006 [SCORE] : 0.5607760190963745\n",
      "[838/1000]\n",
      "- [VAL] LOSS : 0.06105227395892143 [SCORE] : 0.9484702348709106\n",
      "[839/1000]\n",
      "- [TRAIN] LOSS : 0.05112879956141114 [SCORE] : 0.5607760190963745\n",
      "[839/1000]\n",
      "- [VAL] LOSS : 0.06104127690196037 [SCORE] : 0.9484702348709106\n",
      "[840/1000]\n",
      "- [TRAIN] LOSS : 0.051126972399652006 [SCORE] : 0.5607760190963745\n",
      "[840/1000]\n",
      "- [VAL] LOSS : 0.061030399054288864 [SCORE] : 0.9484702348709106\n",
      "[841/1000]\n",
      "- [TRAIN] LOSS : 0.05112518587460121 [SCORE] : 0.5607760190963745\n",
      "[841/1000]\n",
      "- [VAL] LOSS : 0.06101939082145691 [SCORE] : 0.9484702348709106\n",
      "[842/1000]\n",
      "- [TRAIN] LOSS : 0.05112338277200858 [SCORE] : 0.5607760190963745\n",
      "[842/1000]\n",
      "- [VAL] LOSS : 0.06100854277610779 [SCORE] : 0.9484702348709106\n",
      "[843/1000]\n",
      "- [TRAIN] LOSS : 0.05112161144303779 [SCORE] : 0.5607760190963745\n",
      "[843/1000]\n",
      "- [VAL] LOSS : 0.0609976090490818 [SCORE] : 0.9484702348709106\n",
      "[844/1000]\n",
      "- [TRAIN] LOSS : 0.05111980528260271 [SCORE] : 0.5607760190963745\n",
      "[844/1000]\n",
      "- [VAL] LOSS : 0.0609867163002491 [SCORE] : 0.9484702348709106\n",
      "[845/1000]\n",
      "- [TRAIN] LOSS : 0.05111800543963909 [SCORE] : 0.5607760190963745\n",
      "[845/1000]\n",
      "- [VAL] LOSS : 0.060975756496191025 [SCORE] : 0.9484702348709106\n",
      "[846/1000]\n",
      "- [TRAIN] LOSS : 0.051116207319622235 [SCORE] : 0.5607760190963745\n",
      "[846/1000]\n",
      "- [VAL] LOSS : 0.06096488609910011 [SCORE] : 0.9484702348709106\n",
      "[847/1000]\n",
      "- [TRAIN] LOSS : 0.0511144213223209 [SCORE] : 0.5607760190963745\n",
      "[847/1000]\n",
      "- [VAL] LOSS : 0.0609540119767189 [SCORE] : 0.9484702348709106\n",
      "[848/1000]\n",
      "- [TRAIN] LOSS : 0.051112681378920874 [SCORE] : 0.5607760190963745\n",
      "[848/1000]\n",
      "- [VAL] LOSS : 0.06094316393136978 [SCORE] : 0.9484702348709106\n",
      "[849/1000]\n",
      "- [TRAIN] LOSS : 0.05111093334853649 [SCORE] : 0.5607760190963745\n",
      "[849/1000]\n",
      "- [VAL] LOSS : 0.06093233451247215 [SCORE] : 0.9484702348709106\n",
      "[850/1000]\n",
      "- [TRAIN] LOSS : 0.051109135166431466 [SCORE] : 0.5607760190963745\n",
      "[850/1000]\n",
      "- [VAL] LOSS : 0.06092153862118721 [SCORE] : 0.9484702348709106\n",
      "[851/1000]\n",
      "- [TRAIN] LOSS : 0.05110735703880588 [SCORE] : 0.5607760190963745\n",
      "[851/1000]\n",
      "- [VAL] LOSS : 0.06091075390577316 [SCORE] : 0.9484702348709106\n",
      "[852/1000]\n",
      "- [TRAIN] LOSS : 0.05110565436383088 [SCORE] : 0.5607760190963745\n",
      "[852/1000]\n",
      "- [VAL] LOSS : 0.060899920761585236 [SCORE] : 0.9484702348709106\n",
      "[853/1000]\n",
      "- [TRAIN] LOSS : 0.05110393029948076 [SCORE] : 0.5607760190963745\n",
      "[853/1000]\n",
      "- [VAL] LOSS : 0.06088901311159134 [SCORE] : 0.9484702348709106\n",
      "[854/1000]\n",
      "- [TRAIN] LOSS : 0.05110213576505582 [SCORE] : 0.5607760190963745\n",
      "[854/1000]\n",
      "- [VAL] LOSS : 0.06087831035256386 [SCORE] : 0.9484702348709106\n",
      "[855/1000]\n",
      "- [TRAIN] LOSS : 0.05110040563158691 [SCORE] : 0.5607760190963745\n",
      "[855/1000]\n",
      "- [VAL] LOSS : 0.060867536813020706 [SCORE] : 0.9484702348709106\n",
      "[856/1000]\n",
      "- [TRAIN] LOSS : 0.051098695335288845 [SCORE] : 0.5607760190963745\n",
      "[856/1000]\n",
      "- [VAL] LOSS : 0.06085679307579994 [SCORE] : 0.9484702348709106\n",
      "[857/1000]\n",
      "- [TRAIN] LOSS : 0.05109695432086785 [SCORE] : 0.5607760190963745\n",
      "[857/1000]\n",
      "- [VAL] LOSS : 0.060845937579870224 [SCORE] : 0.9484702348709106\n",
      "[858/1000]\n",
      "- [TRAIN] LOSS : 0.051095241028815505 [SCORE] : 0.5607760190963745\n",
      "[858/1000]\n",
      "- [VAL] LOSS : 0.060835305601358414 [SCORE] : 0.9484702348709106\n",
      "[859/1000]\n",
      "- [TRAIN] LOSS : 0.05109356182316939 [SCORE] : 0.5607760190963745\n",
      "[859/1000]\n",
      "- [VAL] LOSS : 0.06082458421587944 [SCORE] : 0.9484702348709106\n",
      "[860/1000]\n",
      "- [TRAIN] LOSS : 0.051091807723666234 [SCORE] : 0.5607760190963745\n",
      "[860/1000]\n",
      "- [VAL] LOSS : 0.06081381440162659 [SCORE] : 0.9484702348709106\n",
      "[861/1000]\n",
      "- [TRAIN] LOSS : 0.05109010431915521 [SCORE] : 0.5607760190963745\n",
      "[861/1000]\n",
      "- [VAL] LOSS : 0.060803208500146866 [SCORE] : 0.9484702348709106\n",
      "[862/1000]\n",
      "- [TRAIN] LOSS : 0.05108841098845005 [SCORE] : 0.5607760190963745\n",
      "[862/1000]\n",
      "- [VAL] LOSS : 0.06079242751002312 [SCORE] : 0.9484702348709106\n",
      "[863/1000]\n",
      "- [TRAIN] LOSS : 0.05108670727349818 [SCORE] : 0.5607760190963745\n",
      "[863/1000]\n",
      "- [VAL] LOSS : 0.0607818104326725 [SCORE] : 0.9484702348709106\n",
      "[864/1000]\n",
      "- [TRAIN] LOSS : 0.051085036046182114 [SCORE] : 0.5607760190963745\n",
      "[864/1000]\n",
      "- [VAL] LOSS : 0.06077108904719353 [SCORE] : 0.9484702348709106\n",
      "[865/1000]\n",
      "- [TRAIN] LOSS : 0.05108337124499182 [SCORE] : 0.5607760190963745\n",
      "[865/1000]\n",
      "- [VAL] LOSS : 0.06076046824455261 [SCORE] : 0.9484702348709106\n",
      "[866/1000]\n",
      "- [TRAIN] LOSS : 0.05108161236469944 [SCORE] : 0.5607760190963745\n",
      "[866/1000]\n",
      "- [VAL] LOSS : 0.06074977293610573 [SCORE] : 0.9484702348709106\n",
      "[867/1000]\n",
      "- [TRAIN] LOSS : 0.05107992758663992 [SCORE] : 0.5607760190963745\n",
      "[867/1000]\n",
      "- [VAL] LOSS : 0.0607391856610775 [SCORE] : 0.9484702348709106\n",
      "[868/1000]\n",
      "- [TRAIN] LOSS : 0.05107831737647454 [SCORE] : 0.5607760190963745\n",
      "[868/1000]\n",
      "- [VAL] LOSS : 0.060728590935468674 [SCORE] : 0.9484702348709106\n",
      "[869/1000]\n",
      "- [TRAIN] LOSS : 0.051076605031266806 [SCORE] : 0.5607760190963745\n",
      "[869/1000]\n",
      "- [VAL] LOSS : 0.060717932879924774 [SCORE] : 0.9484702348709106\n",
      "[870/1000]\n",
      "- [TRAIN] LOSS : 0.05107499153042833 [SCORE] : 0.5607760190963745\n",
      "[870/1000]\n",
      "- [VAL] LOSS : 0.06070730462670326 [SCORE] : 0.9484702348709106\n",
      "[871/1000]\n",
      "- [TRAIN] LOSS : 0.051073264346147576 [SCORE] : 0.5607760190963745\n",
      "[871/1000]\n",
      "- [VAL] LOSS : 0.060696691274642944 [SCORE] : 0.9484702348709106\n",
      "[872/1000]\n",
      "- [TRAIN] LOSS : 0.05107157579623163 [SCORE] : 0.5607760190963745\n",
      "[872/1000]\n",
      "- [VAL] LOSS : 0.06068625673651695 [SCORE] : 0.9484702348709106\n",
      "[873/1000]\n",
      "- [TRAIN] LOSS : 0.05106999206667145 [SCORE] : 0.5607760190963745\n",
      "[873/1000]\n",
      "- [VAL] LOSS : 0.06067565083503723 [SCORE] : 0.9484702348709106\n",
      "[874/1000]\n",
      "- [TRAIN] LOSS : 0.05106831189865867 [SCORE] : 0.5607760190963745\n",
      "[874/1000]\n",
      "- [VAL] LOSS : 0.060665104538202286 [SCORE] : 0.9484702348709106\n",
      "[875/1000]\n",
      "- [TRAIN] LOSS : 0.05106672987652321 [SCORE] : 0.5607760190963745\n",
      "[875/1000]\n",
      "- [VAL] LOSS : 0.06065456196665764 [SCORE] : 0.9484702348709106\n",
      "[876/1000]\n",
      "- [TRAIN] LOSS : 0.05106509635224939 [SCORE] : 0.5607760190963745\n",
      "[876/1000]\n",
      "- [VAL] LOSS : 0.06064405292272568 [SCORE] : 0.9484702348709106\n",
      "[877/1000]\n",
      "- [TRAIN] LOSS : 0.051063445241500936 [SCORE] : 0.5607760190963745\n",
      "[877/1000]\n",
      "- [VAL] LOSS : 0.06063350662589073 [SCORE] : 0.9484702348709106\n",
      "[878/1000]\n",
      "- [TRAIN] LOSS : 0.05106178031613429 [SCORE] : 0.5607760190963745\n",
      "[878/1000]\n",
      "- [VAL] LOSS : 0.060623012483119965 [SCORE] : 0.9484702348709106\n",
      "[879/1000]\n",
      "- [TRAIN] LOSS : 0.051060140458866955 [SCORE] : 0.5607760190963745\n",
      "[879/1000]\n",
      "- [VAL] LOSS : 0.060612477362155914 [SCORE] : 0.9484702348709106\n",
      "[880/1000]\n",
      "- [TRAIN] LOSS : 0.05105859725736082 [SCORE] : 0.5607760190963745\n",
      "[880/1000]\n",
      "- [VAL] LOSS : 0.060602039098739624 [SCORE] : 0.9484702348709106\n",
      "[881/1000]\n",
      "- [TRAIN] LOSS : 0.05105694172283014 [SCORE] : 0.5607760190963745\n",
      "[881/1000]\n",
      "- [VAL] LOSS : 0.06059163436293602 [SCORE] : 0.9484702348709106\n",
      "[882/1000]\n",
      "- [TRAIN] LOSS : 0.05105533475677172 [SCORE] : 0.5607760190963745\n",
      "[882/1000]\n",
      "- [VAL] LOSS : 0.06058119982481003 [SCORE] : 0.9484702348709106\n",
      "[883/1000]\n",
      "- [TRAIN] LOSS : 0.05105369100347161 [SCORE] : 0.5607760190963745\n",
      "[883/1000]\n",
      "- [VAL] LOSS : 0.06057072430849075 [SCORE] : 0.9484702348709106\n",
      "[884/1000]\n",
      "- [TRAIN] LOSS : 0.05105211629221837 [SCORE] : 0.5607760190963745\n",
      "[884/1000]\n",
      "- [VAL] LOSS : 0.06056028604507446 [SCORE] : 0.9484702348709106\n",
      "[885/1000]\n",
      "- [TRAIN] LOSS : 0.05105049492170413 [SCORE] : 0.5607760190963745\n",
      "[885/1000]\n",
      "- [VAL] LOSS : 0.06054985523223877 [SCORE] : 0.9484702348709106\n",
      "[886/1000]\n",
      "- [TRAIN] LOSS : 0.051048922212794424 [SCORE] : 0.5607760190963745\n",
      "[886/1000]\n",
      "- [VAL] LOSS : 0.06053953245282173 [SCORE] : 0.9484702348709106\n",
      "[887/1000]\n",
      "- [TRAIN] LOSS : 0.05104735835144917 [SCORE] : 0.5607760190963745\n",
      "[887/1000]\n",
      "- [VAL] LOSS : 0.06052907556295395 [SCORE] : 0.9484702348709106\n",
      "[888/1000]\n",
      "- [TRAIN] LOSS : 0.05104574132710695 [SCORE] : 0.5607760190963745\n",
      "[888/1000]\n",
      "- [VAL] LOSS : 0.06051865592598915 [SCORE] : 0.9484702348709106\n",
      "[889/1000]\n",
      "- [TRAIN] LOSS : 0.051044141470144196 [SCORE] : 0.5607760190963745\n",
      "[889/1000]\n",
      "- [VAL] LOSS : 0.06050828844308853 [SCORE] : 0.9484702348709106\n",
      "[890/1000]\n",
      "- [TRAIN] LOSS : 0.0510425945588698 [SCORE] : 0.5607760190963745\n",
      "[890/1000]\n",
      "- [VAL] LOSS : 0.060497961938381195 [SCORE] : 0.9484702348709106\n",
      "[891/1000]\n",
      "- [TRAIN] LOSS : 0.05104100010357797 [SCORE] : 0.5607760190963745\n",
      "[891/1000]\n",
      "- [VAL] LOSS : 0.060487620532512665 [SCORE] : 0.9484702348709106\n",
      "[892/1000]\n",
      "- [TRAIN] LOSS : 0.05103942696005106 [SCORE] : 0.5607760190963745\n",
      "[892/1000]\n",
      "- [VAL] LOSS : 0.06047717481851578 [SCORE] : 0.9484702348709106\n",
      "[893/1000]\n",
      "- [TRAIN] LOSS : 0.05103784988944729 [SCORE] : 0.5607760190963745\n",
      "[893/1000]\n",
      "- [VAL] LOSS : 0.060466933995485306 [SCORE] : 0.9484702348709106\n",
      "[894/1000]\n",
      "- [TRAIN] LOSS : 0.051036278794830046 [SCORE] : 0.5607760190963745\n",
      "[894/1000]\n",
      "- [VAL] LOSS : 0.06045662611722946 [SCORE] : 0.9484702348709106\n",
      "[895/1000]\n",
      "- [TRAIN] LOSS : 0.051034757712235056 [SCORE] : 0.5607760190963745\n",
      "[895/1000]\n",
      "- [VAL] LOSS : 0.06044634059071541 [SCORE] : 0.9484702348709106\n",
      "[896/1000]\n",
      "- [TRAIN] LOSS : 0.05103317772348722 [SCORE] : 0.5607760190963745\n",
      "[896/1000]\n",
      "- [VAL] LOSS : 0.06043597310781479 [SCORE] : 0.9484702348709106\n",
      "[897/1000]\n",
      "- [TRAIN] LOSS : 0.05103163829383751 [SCORE] : 0.5607760190963745\n",
      "[897/1000]\n",
      "- [VAL] LOSS : 0.06042568013072014 [SCORE] : 0.9484702348709106\n",
      "[898/1000]\n",
      "- [TRAIN] LOSS : 0.0510300702881068 [SCORE] : 0.5607760190963745\n",
      "[898/1000]\n",
      "- [VAL] LOSS : 0.06041532754898071 [SCORE] : 0.9484702348709106\n",
      "[899/1000]\n",
      "- [TRAIN] LOSS : 0.051028567769875126 [SCORE] : 0.5607760190963745\n",
      "[899/1000]\n",
      "- [VAL] LOSS : 0.06040507182478905 [SCORE] : 0.9484702348709106\n",
      "[900/1000]\n",
      "- [TRAIN] LOSS : 0.05102699447112779 [SCORE] : 0.5607760190963745\n",
      "[900/1000]\n",
      "- [VAL] LOSS : 0.06039487570524216 [SCORE] : 0.9484702348709106\n",
      "[901/1000]\n",
      "- [TRAIN] LOSS : 0.051025459092731276 [SCORE] : 0.5607760190963745\n",
      "[901/1000]\n",
      "- [VAL] LOSS : 0.060384634882211685 [SCORE] : 0.9484702348709106\n",
      "[902/1000]\n",
      "- [TRAIN] LOSS : 0.051023902588834366 [SCORE] : 0.5607760190963745\n",
      "[902/1000]\n",
      "- [VAL] LOSS : 0.060374390333890915 [SCORE] : 0.9484702348709106\n",
      "[903/1000]\n",
      "- [TRAIN] LOSS : 0.05102238322918614 [SCORE] : 0.5607760190963745\n",
      "[903/1000]\n",
      "- [VAL] LOSS : 0.06036413088440895 [SCORE] : 0.9484702348709106\n",
      "[904/1000]\n",
      "- [TRAIN] LOSS : 0.051020884327590464 [SCORE] : 0.5607760190963745\n",
      "[904/1000]\n",
      "- [VAL] LOSS : 0.06035391241312027 [SCORE] : 0.9484702348709106\n",
      "[905/1000]\n",
      "- [TRAIN] LOSS : 0.051019352146734795 [SCORE] : 0.5607760190963745\n",
      "[905/1000]\n",
      "- [VAL] LOSS : 0.0603436641395092 [SCORE] : 0.9484702348709106\n",
      "[906/1000]\n",
      "- [TRAIN] LOSS : 0.05101782322550814 [SCORE] : 0.5607760190963745\n",
      "[906/1000]\n",
      "- [VAL] LOSS : 0.0603334978222847 [SCORE] : 0.9484702348709106\n",
      "[907/1000]\n",
      "- [TRAIN] LOSS : 0.05101629242611428 [SCORE] : 0.5607760190963745\n",
      "[907/1000]\n",
      "- [VAL] LOSS : 0.060323316603899 [SCORE] : 0.9484702348709106\n",
      "[908/1000]\n",
      "- [TRAIN] LOSS : 0.051014787517488 [SCORE] : 0.5607760190963745\n",
      "[908/1000]\n",
      "- [VAL] LOSS : 0.06031310558319092 [SCORE] : 0.9484702348709106\n",
      "[909/1000]\n",
      "- [TRAIN] LOSS : 0.05101330143709978 [SCORE] : 0.5607760190963745\n",
      "[909/1000]\n",
      "- [VAL] LOSS : 0.060302939265966415 [SCORE] : 0.9484702348709106\n",
      "[910/1000]\n",
      "- [TRAIN] LOSS : 0.051011793812115985 [SCORE] : 0.5607760190963745\n",
      "[910/1000]\n",
      "- [VAL] LOSS : 0.060292791575193405 [SCORE] : 0.9484702348709106\n",
      "[911/1000]\n",
      "- [TRAIN] LOSS : 0.05101028865513702 [SCORE] : 0.5607760190963745\n",
      "[911/1000]\n",
      "- [VAL] LOSS : 0.0602826364338398 [SCORE] : 0.9484702348709106\n",
      "[912/1000]\n",
      "- [TRAIN] LOSS : 0.051008768503864606 [SCORE] : 0.5607760190963745\n",
      "[912/1000]\n",
      "- [VAL] LOSS : 0.060272566974163055 [SCORE] : 0.9484702348709106\n",
      "[913/1000]\n",
      "- [TRAIN] LOSS : 0.05100728908243279 [SCORE] : 0.5607760190963745\n",
      "[913/1000]\n",
      "- [VAL] LOSS : 0.06026235595345497 [SCORE] : 0.9484702348709106\n",
      "[914/1000]\n",
      "- [TRAIN] LOSS : 0.051005817980815965 [SCORE] : 0.5607760190963745\n",
      "[914/1000]\n",
      "- [VAL] LOSS : 0.060252267867326736 [SCORE] : 0.9484702348709106\n",
      "[915/1000]\n",
      "- [TRAIN] LOSS : 0.05100431144237518 [SCORE] : 0.5607760190963745\n",
      "[915/1000]\n",
      "- [VAL] LOSS : 0.060242120176553726 [SCORE] : 0.9484702348709106\n",
      "[916/1000]\n",
      "- [TRAIN] LOSS : 0.051002836289505166 [SCORE] : 0.5607760190963745\n",
      "[916/1000]\n",
      "- [VAL] LOSS : 0.06023211032152176 [SCORE] : 0.9484702348709106\n",
      "[917/1000]\n",
      "- [TRAIN] LOSS : 0.05100137311965227 [SCORE] : 0.5607760190963745\n",
      "[917/1000]\n",
      "- [VAL] LOSS : 0.060221899300813675 [SCORE] : 0.9484702348709106\n",
      "[918/1000]\n",
      "- [TRAIN] LOSS : 0.050999862343693775 [SCORE] : 0.5607760190963745\n",
      "[918/1000]\n",
      "- [VAL] LOSS : 0.06021185964345932 [SCORE] : 0.9484702348709106\n",
      "[919/1000]\n",
      "- [TRAIN] LOSS : 0.05099834338761866 [SCORE] : 0.5607760190963745\n",
      "[919/1000]\n",
      "- [VAL] LOSS : 0.060201648622751236 [SCORE] : 0.9484702348709106\n",
      "[920/1000]\n",
      "- [TRAIN] LOSS : 0.050996881102522214 [SCORE] : 0.5607760190963745\n",
      "[920/1000]\n",
      "- [VAL] LOSS : 0.06019173189997673 [SCORE] : 0.9484702348709106\n",
      "[921/1000]\n",
      "- [TRAIN] LOSS : 0.05099539533257484 [SCORE] : 0.5607760190963745\n",
      "[921/1000]\n",
      "- [VAL] LOSS : 0.06018172577023506 [SCORE] : 0.9484702348709106\n",
      "[922/1000]\n",
      "- [TRAIN] LOSS : 0.05099397675755123 [SCORE] : 0.5607760190963745\n",
      "[922/1000]\n",
      "- [VAL] LOSS : 0.06017161160707474 [SCORE] : 0.9484702348709106\n",
      "[923/1000]\n",
      "- [TRAIN] LOSS : 0.05099254543893039 [SCORE] : 0.5607760190963745\n",
      "[923/1000]\n",
      "- [VAL] LOSS : 0.06016159802675247 [SCORE] : 0.9484702348709106\n",
      "[924/1000]\n",
      "- [TRAIN] LOSS : 0.05099109243601561 [SCORE] : 0.5607760190963745\n",
      "[924/1000]\n",
      "- [VAL] LOSS : 0.060151487588882446 [SCORE] : 0.9484702348709106\n",
      "[925/1000]\n",
      "- [TRAIN] LOSS : 0.05098966470298668 [SCORE] : 0.5607760190963745\n",
      "[925/1000]\n",
      "- [VAL] LOSS : 0.06014147028326988 [SCORE] : 0.9484702348709106\n",
      "[926/1000]\n",
      "- [TRAIN] LOSS : 0.05098819488969942 [SCORE] : 0.5607760190963745\n",
      "[926/1000]\n",
      "- [VAL] LOSS : 0.060131512582302094 [SCORE] : 0.9484702348709106\n",
      "[927/1000]\n",
      "- [TRAIN] LOSS : 0.050986744137480854 [SCORE] : 0.5607760190963745\n",
      "[927/1000]\n",
      "- [VAL] LOSS : 0.06012148782610893 [SCORE] : 0.9484702348709106\n",
      "[928/1000]\n",
      "- [TRAIN] LOSS : 0.050985273780922095 [SCORE] : 0.5607760190963745\n",
      "[928/1000]\n",
      "- [VAL] LOSS : 0.06011160463094711 [SCORE] : 0.9484702348709106\n",
      "[929/1000]\n",
      "- [TRAIN] LOSS : 0.05098382728174329 [SCORE] : 0.5607760190963745\n",
      "[929/1000]\n",
      "- [VAL] LOSS : 0.06010156124830246 [SCORE] : 0.9484702348709106\n",
      "[930/1000]\n",
      "- [TRAIN] LOSS : 0.05098237826799353 [SCORE] : 0.5607760190963745\n",
      "[930/1000]\n",
      "- [VAL] LOSS : 0.0600915290415287 [SCORE] : 0.9484702348709106\n",
      "[931/1000]\n",
      "- [TRAIN] LOSS : 0.05098097433025638 [SCORE] : 0.5607760190963745\n",
      "[931/1000]\n",
      "- [VAL] LOSS : 0.06008153781294823 [SCORE] : 0.9484702348709106\n",
      "[932/1000]\n",
      "- [TRAIN] LOSS : 0.05097952438518405 [SCORE] : 0.5607760190963745\n",
      "[932/1000]\n",
      "- [VAL] LOSS : 0.060071635991334915 [SCORE] : 0.9484702348709106\n",
      "[933/1000]\n",
      "- [TRAIN] LOSS : 0.05097805235224465 [SCORE] : 0.5607760190963745\n",
      "[933/1000]\n",
      "- [VAL] LOSS : 0.06006171554327011 [SCORE] : 0.9484702348709106\n",
      "[934/1000]\n",
      "- [TRAIN] LOSS : 0.050976663315668705 [SCORE] : 0.5607760190963745\n",
      "[934/1000]\n",
      "- [VAL] LOSS : 0.06005183234810829 [SCORE] : 0.9484702348709106\n",
      "[935/1000]\n",
      "- [TRAIN] LOSS : 0.05097525240853429 [SCORE] : 0.5607760190963745\n",
      "[935/1000]\n",
      "- [VAL] LOSS : 0.06004184111952782 [SCORE] : 0.9484702348709106\n",
      "[936/1000]\n",
      "- [TRAIN] LOSS : 0.05097383825729291 [SCORE] : 0.5607760190963745\n",
      "[936/1000]\n",
      "- [VAL] LOSS : 0.060031913220882416 [SCORE] : 0.9484702348709106\n",
      "[937/1000]\n",
      "- [TRAIN] LOSS : 0.050972412806004286 [SCORE] : 0.5607760190963745\n",
      "[937/1000]\n",
      "- [VAL] LOSS : 0.06002204492688179 [SCORE] : 0.9484702348709106\n",
      "[938/1000]\n",
      "- [TRAIN] LOSS : 0.05097100144873063 [SCORE] : 0.5607760190963745\n",
      "[938/1000]\n",
      "- [VAL] LOSS : 0.06001210957765579 [SCORE] : 0.9484702348709106\n",
      "[939/1000]\n",
      "- [TRAIN] LOSS : 0.05096958922222257 [SCORE] : 0.5607760190963745\n",
      "[939/1000]\n",
      "- [VAL] LOSS : 0.060002267360687256 [SCORE] : 0.9484702348709106\n",
      "[940/1000]\n",
      "- [TRAIN] LOSS : 0.050968148807684584 [SCORE] : 0.5607760190963745\n",
      "[940/1000]\n",
      "- [VAL] LOSS : 0.05999239906668663 [SCORE] : 0.9484702348709106\n",
      "[941/1000]\n",
      "- [TRAIN] LOSS : 0.05096673898709317 [SCORE] : 0.5607760190963745\n",
      "[941/1000]\n",
      "- [VAL] LOSS : 0.059982556849718094 [SCORE] : 0.9484702348709106\n",
      "[942/1000]\n",
      "- [TRAIN] LOSS : 0.05096533588754634 [SCORE] : 0.5607760190963745\n",
      "[942/1000]\n",
      "- [VAL] LOSS : 0.05997264385223389 [SCORE] : 0.9484702348709106\n",
      "[943/1000]\n",
      "- [TRAIN] LOSS : 0.05096396088289718 [SCORE] : 0.5607760190963745\n",
      "[943/1000]\n",
      "- [VAL] LOSS : 0.05996276065707207 [SCORE] : 0.9484702348709106\n",
      "[944/1000]\n",
      "- [TRAIN] LOSS : 0.05096254581585526 [SCORE] : 0.5607760190963745\n",
      "[944/1000]\n",
      "- [VAL] LOSS : 0.05995297431945801 [SCORE] : 0.9484702348709106\n",
      "[945/1000]\n",
      "- [TRAIN] LOSS : 0.05096117050076524 [SCORE] : 0.5607760190963745\n",
      "[945/1000]\n",
      "- [VAL] LOSS : 0.059943102300167084 [SCORE] : 0.9484702348709106\n",
      "[946/1000]\n",
      "- [TRAIN] LOSS : 0.05095979020309945 [SCORE] : 0.5607760190963745\n",
      "[946/1000]\n",
      "- [VAL] LOSS : 0.059933245182037354 [SCORE] : 0.9484702348709106\n",
      "[947/1000]\n",
      "- [TRAIN] LOSS : 0.05095839037870367 [SCORE] : 0.5607760190963745\n",
      "[947/1000]\n",
      "- [VAL] LOSS : 0.05992349982261658 [SCORE] : 0.9484702348709106\n",
      "[948/1000]\n",
      "- [TRAIN] LOSS : 0.050956980294237536 [SCORE] : 0.5607760190963745\n",
      "[948/1000]\n",
      "- [VAL] LOSS : 0.059913624078035355 [SCORE] : 0.9484702348709106\n",
      "[949/1000]\n",
      "- [TRAIN] LOSS : 0.05095558276710411 [SCORE] : 0.5607760190963745\n",
      "[949/1000]\n",
      "- [VAL] LOSS : 0.059903956949710846 [SCORE] : 0.9484702348709106\n",
      "[950/1000]\n",
      "- [TRAIN] LOSS : 0.050954252419372396 [SCORE] : 0.5607760190963745\n",
      "[950/1000]\n",
      "- [VAL] LOSS : 0.0598941445350647 [SCORE] : 0.9484702348709106\n",
      "[951/1000]\n",
      "- [TRAIN] LOSS : 0.050952841077620784 [SCORE] : 0.5607760190963745\n",
      "[951/1000]\n",
      "- [VAL] LOSS : 0.05988433212041855 [SCORE] : 0.9484702348709106\n",
      "[952/1000]\n",
      "- [TRAIN] LOSS : 0.050951490085572 [SCORE] : 0.5607760190963745\n",
      "[952/1000]\n",
      "- [VAL] LOSS : 0.05987456440925598 [SCORE] : 0.9484702348709106\n",
      "[953/1000]\n",
      "- [TRAIN] LOSS : 0.05095011234904329 [SCORE] : 0.5607760190963745\n",
      "[953/1000]\n",
      "- [VAL] LOSS : 0.059864822775125504 [SCORE] : 0.9484702348709106\n",
      "[954/1000]\n",
      "- [TRAIN] LOSS : 0.050948757647226256 [SCORE] : 0.5607760190963745\n",
      "[954/1000]\n",
      "- [VAL] LOSS : 0.05985506996512413 [SCORE] : 0.9484702348709106\n",
      "[955/1000]\n",
      "- [TRAIN] LOSS : 0.05094739239042004 [SCORE] : 0.5607760190963745\n",
      "[955/1000]\n",
      "- [VAL] LOSS : 0.059845250099897385 [SCORE] : 0.9484702348709106\n",
      "[956/1000]\n",
      "- [TRAIN] LOSS : 0.05094600853820642 [SCORE] : 0.5607760190963745\n",
      "[956/1000]\n",
      "- [VAL] LOSS : 0.0598355270922184 [SCORE] : 0.9484702348709106\n",
      "[957/1000]\n",
      "- [TRAIN] LOSS : 0.050944644926736754 [SCORE] : 0.5607760190963745\n",
      "[957/1000]\n",
      "- [VAL] LOSS : 0.05982581153512001 [SCORE] : 0.9484702348709106\n",
      "[958/1000]\n",
      "- [TRAIN] LOSS : 0.05094328996104499 [SCORE] : 0.5607760190963745\n",
      "[958/1000]\n",
      "- [VAL] LOSS : 0.05981609225273132 [SCORE] : 0.9484702348709106\n",
      "[959/1000]\n",
      "- [TRAIN] LOSS : 0.05094197715322177 [SCORE] : 0.5607760190963745\n",
      "[959/1000]\n",
      "- [VAL] LOSS : 0.05980636924505234 [SCORE] : 0.9484702348709106\n",
      "[960/1000]\n",
      "- [TRAIN] LOSS : 0.05094063856328527 [SCORE] : 0.5607760190963745\n",
      "[960/1000]\n",
      "- [VAL] LOSS : 0.05979669466614723 [SCORE] : 0.9484702348709106\n",
      "[961/1000]\n",
      "- [TRAIN] LOSS : 0.05093923967021207 [SCORE] : 0.5607760190963745\n",
      "[961/1000]\n",
      "- [VAL] LOSS : 0.05978702753782272 [SCORE] : 0.9484702348709106\n",
      "[962/1000]\n",
      "- [TRAIN] LOSS : 0.05093795275315642 [SCORE] : 0.5607760190963745\n",
      "[962/1000]\n",
      "- [VAL] LOSS : 0.059777338057756424 [SCORE] : 0.9484702348709106\n",
      "[963/1000]\n",
      "- [TRAIN] LOSS : 0.050936545990407464 [SCORE] : 0.5607760190963745\n",
      "[963/1000]\n",
      "- [VAL] LOSS : 0.05976758524775505 [SCORE] : 0.9484702348709106\n",
      "[964/1000]\n",
      "- [TRAIN] LOSS : 0.0509352532060196 [SCORE] : 0.5607760190963745\n",
      "[964/1000]\n",
      "- [VAL] LOSS : 0.05975788086652756 [SCORE] : 0.9484702348709106\n",
      "[965/1000]\n",
      "- [TRAIN] LOSS : 0.050933870999142525 [SCORE] : 0.5607760190963745\n",
      "[965/1000]\n",
      "- [VAL] LOSS : 0.05974821373820305 [SCORE] : 0.9484702348709106\n",
      "[966/1000]\n",
      "- [TRAIN] LOSS : 0.05093251617314915 [SCORE] : 0.5607760190963745\n",
      "[966/1000]\n",
      "- [VAL] LOSS : 0.059738628566265106 [SCORE] : 0.9484702348709106\n",
      "[967/1000]\n",
      "- [TRAIN] LOSS : 0.05093121339256565 [SCORE] : 0.5607760190963745\n",
      "[967/1000]\n",
      "- [VAL] LOSS : 0.0597289577126503 [SCORE] : 0.9484702348709106\n",
      "[968/1000]\n",
      "- [TRAIN] LOSS : 0.050929858814924955 [SCORE] : 0.5607760190963745\n",
      "[968/1000]\n",
      "- [VAL] LOSS : 0.05971929803490639 [SCORE] : 0.9484702348709106\n",
      "[969/1000]\n",
      "- [TRAIN] LOSS : 0.05092860978717605 [SCORE] : 0.5607760190963745\n",
      "[969/1000]\n",
      "- [VAL] LOSS : 0.05970966815948486 [SCORE] : 0.9484702348709106\n",
      "[970/1000]\n",
      "- [TRAIN] LOSS : 0.05092719014113148 [SCORE] : 0.5607760190963745\n",
      "[970/1000]\n",
      "- [VAL] LOSS : 0.05969998612999916 [SCORE] : 0.9484702348709106\n",
      "[971/1000]\n",
      "- [TRAIN] LOSS : 0.05092590435718496 [SCORE] : 0.5607760190963745\n",
      "[971/1000]\n",
      "- [VAL] LOSS : 0.0596904456615448 [SCORE] : 0.9484702348709106\n",
      "[972/1000]\n",
      "- [TRAIN] LOSS : 0.05092457955082257 [SCORE] : 0.5607760190963745\n",
      "[972/1000]\n",
      "- [VAL] LOSS : 0.05968082696199417 [SCORE] : 0.9484702348709106\n",
      "[973/1000]\n",
      "- [TRAIN] LOSS : 0.05092325895093382 [SCORE] : 0.5607760190963745\n",
      "[973/1000]\n",
      "- [VAL] LOSS : 0.059671152383089066 [SCORE] : 0.9484702348709106\n",
      "[974/1000]\n",
      "- [TRAIN] LOSS : 0.050921937574942906 [SCORE] : 0.5607760190963745\n",
      "[974/1000]\n",
      "- [VAL] LOSS : 0.059661608189344406 [SCORE] : 0.9484702348709106\n",
      "[975/1000]\n",
      "- [TRAIN] LOSS : 0.050920630277444916 [SCORE] : 0.5607760190963745\n",
      "[975/1000]\n",
      "- [VAL] LOSS : 0.05965208634734154 [SCORE] : 0.9484702348709106\n",
      "[976/1000]\n",
      "- [TRAIN] LOSS : 0.05091934516094625 [SCORE] : 0.5607760190963745\n",
      "[976/1000]\n",
      "- [VAL] LOSS : 0.059642452746629715 [SCORE] : 0.9484702348709106\n",
      "[977/1000]\n",
      "- [TRAIN] LOSS : 0.050918019159386554 [SCORE] : 0.5607760190963745\n",
      "[977/1000]\n",
      "- [VAL] LOSS : 0.05963287875056267 [SCORE] : 0.9484702348709106\n",
      "[978/1000]\n",
      "- [TRAIN] LOSS : 0.0509167002979666 [SCORE] : 0.5607760190963745\n",
      "[978/1000]\n",
      "- [VAL] LOSS : 0.05962332338094711 [SCORE] : 0.9484702348709106\n",
      "[979/1000]\n",
      "- [TRAIN] LOSS : 0.050915392286454635 [SCORE] : 0.5607760190963745\n",
      "[979/1000]\n",
      "- [VAL] LOSS : 0.05961376056075096 [SCORE] : 0.9484702348709106\n",
      "[980/1000]\n",
      "- [TRAIN] LOSS : 0.050914128124713895 [SCORE] : 0.5607760190963745\n",
      "[980/1000]\n",
      "- [VAL] LOSS : 0.059604160487651825 [SCORE] : 0.9484702348709106\n",
      "[981/1000]\n",
      "- [TRAIN] LOSS : 0.050912787578999995 [SCORE] : 0.5607760190963745\n",
      "[981/1000]\n",
      "- [VAL] LOSS : 0.05959470197558403 [SCORE] : 0.9484702348709106\n",
      "[982/1000]\n",
      "- [TRAIN] LOSS : 0.050911496548602976 [SCORE] : 0.5607760190963745\n",
      "[982/1000]\n",
      "- [VAL] LOSS : 0.05958516523241997 [SCORE] : 0.9484702348709106\n",
      "[983/1000]\n",
      "- [TRAIN] LOSS : 0.05091023507217566 [SCORE] : 0.5607760190963745\n",
      "[983/1000]\n",
      "- [VAL] LOSS : 0.05957566574215889 [SCORE] : 0.9484702348709106\n",
      "[984/1000]\n",
      "- [TRAIN] LOSS : 0.050908937118947506 [SCORE] : 0.5607760190963745\n",
      "[984/1000]\n",
      "- [VAL] LOSS : 0.05956626683473587 [SCORE] : 0.9484702348709106\n",
      "[985/1000]\n",
      "- [TRAIN] LOSS : 0.05090764664734403 [SCORE] : 0.5607760190963745\n",
      "[985/1000]\n",
      "- [VAL] LOSS : 0.059556715190410614 [SCORE] : 0.9484702348709106\n",
      "[986/1000]\n",
      "- [TRAIN] LOSS : 0.05090634183337291 [SCORE] : 0.5607760190963745\n",
      "[986/1000]\n",
      "- [VAL] LOSS : 0.05954725295305252 [SCORE] : 0.9484702348709106\n",
      "[987/1000]\n",
      "- [TRAIN] LOSS : 0.050905057011793056 [SCORE] : 0.5607760190963745\n",
      "[987/1000]\n",
      "- [VAL] LOSS : 0.059537675231695175 [SCORE] : 0.9484702348709106\n",
      "[988/1000]\n",
      "- [TRAIN] LOSS : 0.050903782698636255 [SCORE] : 0.5607760190963745\n",
      "[988/1000]\n",
      "- [VAL] LOSS : 0.05952814221382141 [SCORE] : 0.9484702348709106\n",
      "[989/1000]\n",
      "- [TRAIN] LOSS : 0.05090250779564182 [SCORE] : 0.5607760190963745\n",
      "[989/1000]\n",
      "- [VAL] LOSS : 0.05951866880059242 [SCORE] : 0.9484702348709106\n",
      "[990/1000]\n",
      "- [TRAIN] LOSS : 0.05090117508855959 [SCORE] : 0.5607760190963745\n",
      "[990/1000]\n",
      "- [VAL] LOSS : 0.059509243816137314 [SCORE] : 0.9484702348709106\n",
      "[991/1000]\n",
      "- [TRAIN] LOSS : 0.05089990617707372 [SCORE] : 0.5607760190963745\n",
      "[991/1000]\n",
      "- [VAL] LOSS : 0.059499796479940414 [SCORE] : 0.9484702348709106\n",
      "[992/1000]\n",
      "- [TRAIN] LOSS : 0.050898650226493675 [SCORE] : 0.5607760190963745\n",
      "[992/1000]\n",
      "- [VAL] LOSS : 0.05949028208851814 [SCORE] : 0.9484702348709106\n",
      "[993/1000]\n",
      "- [TRAIN] LOSS : 0.050897350224355854 [SCORE] : 0.5607760190963745\n",
      "[993/1000]\n",
      "- [VAL] LOSS : 0.05948096513748169 [SCORE] : 0.9484702348709106\n",
      "[994/1000]\n",
      "- [TRAIN] LOSS : 0.05089614054498573 [SCORE] : 0.5607760190963745\n",
      "[994/1000]\n",
      "- [VAL] LOSS : 0.05947144702076912 [SCORE] : 0.9484702348709106\n",
      "[995/1000]\n",
      "- [TRAIN] LOSS : 0.05089487936347723 [SCORE] : 0.5607760190963745\n",
      "[995/1000]\n",
      "- [VAL] LOSS : 0.05946209281682968 [SCORE] : 0.9484702348709106\n",
      "[996/1000]\n",
      "- [TRAIN] LOSS : 0.050893585508068405 [SCORE] : 0.5607760190963745\n",
      "[996/1000]\n",
      "- [VAL] LOSS : 0.05945265293121338 [SCORE] : 0.9484702348709106\n",
      "[997/1000]\n",
      "- [TRAIN] LOSS : 0.05089234630577266 [SCORE] : 0.5607760190963745\n",
      "[997/1000]\n",
      "- [VAL] LOSS : 0.05944318696856499 [SCORE] : 0.9484702348709106\n",
      "[998/1000]\n",
      "- [TRAIN] LOSS : 0.050891088942686714 [SCORE] : 0.5607760190963745\n",
      "[998/1000]\n",
      "- [VAL] LOSS : 0.059433769434690475 [SCORE] : 0.9484702348709106\n",
      "[999/1000]\n",
      "- [TRAIN] LOSS : 0.05088981802885731 [SCORE] : 0.5607760190963745\n",
      "[999/1000]\n",
      "- [VAL] LOSS : 0.05942443013191223 [SCORE] : 0.9484702348709106\n",
      "[1000/1000]\n",
      "- [TRAIN] LOSS : 0.05088856869066755 [SCORE] : 0.5607760190963745\n",
      "[1000/1000]\n",
      "- [VAL] LOSS : 0.059414979070425034 [SCORE] : 0.9484702348709106\n"
     ]
    }
   ],
   "source": [
    "# 학습의 효과 확인 손실값과 성능평가값 저장 필요\n",
    "LOSS_HISTORY, SCORE_HISTROY=[[],[]], [[],[]]\n",
    "\n",
    "f1_score_metric = MulticlassF1Score(num_classes=3)\n",
    "# 중단 기준\n",
    "BREAK_CNT =0\n",
    "for epoch in range(1,EPOCH+1):\n",
    "    # 학습 모드로 모델 설정\n",
    "    model.train()\n",
    "\n",
    "    # 배치크기 만큼 데이터 로딩해서 학습 진행\n",
    "    loss_total, score_total=0, 0\n",
    "    for featureTS, targetTS in trainDL:\n",
    "\n",
    "        # 학습 진행\n",
    "        pre_y=model(featureTS)\n",
    "\n",
    "        # 손실 계산 : nn.CrossEntropyLoss 요구사항 : 정답/타겟은 0D 또는 1D, 타입은 long\n",
    "        loss=crossLoss(pre_y, targetTS.reshape(-1).long())\n",
    "        loss_total += loss.item()\n",
    "\n",
    "        # 성능평가 계산\n",
    "        score=f1_score_metric(pre_y, targetTS.reshape(-1))\n",
    "        score_total += score.item()\n",
    "\n",
    "        # 최적화 진행\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 에포크 당 검증기능\n",
    "    # 모델 검증 모드 설정\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 검증 데이터셋\n",
    "        val_featureTS=torch.FloatTensor(valDS.featureDF.values)\n",
    "        val_targetTS=torch.FloatTensor(valDS.targetDF.values)\n",
    "\n",
    "        # 추론/평가\n",
    "        pre_val=model(val_featureTS)\n",
    "\n",
    "        # 손실\n",
    "        loss_val=crossLoss(pre_val, val_targetTS.reshape(-1).long())\n",
    "\n",
    "        # 성능평가\n",
    "        score_val=MulticlassF1Score(num_classes=3)(pre_val, val_targetTS.reshape(-1))\n",
    "    \n",
    "    # 에포크 당 손실값과 성능평가값 저장\n",
    "    LOSS_HISTORY[0].append(loss_total/BATCH_CNT)\n",
    "    SCORE_HISTROY[0].append(score_total/BATCH_CNT)\n",
    "\n",
    "    LOSS_HISTORY[1].append(loss_val)\n",
    "    SCORE_HISTROY[1].append(score_val)\n",
    "\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- [TRAIN] LOSS : {LOSS_HISTORY[0][-1]} [SCORE] : {SCORE_HISTROY[0][-1]}')\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- [VAL] LOSS : {LOSS_HISTORY[1][-1]} [SCORE] : {SCORE_HISTROY[1][-1]}')\n",
    "\n",
    "\n",
    "    # 학습이 잘안되면 중단\n",
    "    # LOSS를 기준으로 중단\n",
    "\n",
    "    if len(LOSS_HISTORY[1]) >=2:\n",
    "        if LOSS_HISTORY[1][-1] >= LOSS_HISTORY[1][-2]:\n",
    "            BREAK_CNT +=1\n",
    "\n",
    "    if len(LOSS_HISTORY[1]) ==1:\n",
    "        # 첫번째라서 무조건 저장\n",
    "        torch.save(model.state_dict(), SAVE_FILE)\n",
    "        # 모델 저장\n",
    "        torch.save(model, SAVE_MODEL)\n",
    "    else:\n",
    "        if LOSS_HISTORY[1][-1] < min(LOSS_HISTORY[1]):      # 현재 에포크를 포함하여 전체 검증 손실에서 최소값 찾음음\n",
    "            torch.save(model.state_dict(), SAVE_FILE)\n",
    "            torch.save(model, SAVE_MODEL)\n",
    "            \n",
    "    # 학습 중단 여부 설정\n",
    "    if BREAK_CNT >10 :\n",
    "        print(\"성능 및 손실 개선이 없어서 학습\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 검증 모드 설정\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 검증 데이터셋\n",
    "    test_featureTS=torch.FloatTensor(testDS.featureDF.values)\n",
    "    test_targetTS=torch.FloatTensor(testDS.targetDF.values)\n",
    "\n",
    "    # 추론/평가\n",
    "    pre_val=model(test_featureTS)\n",
    "    #print(pre_val)\n",
    "\n",
    "    # 손실\n",
    "    loss_test=crossLoss(pre_val, test_targetTS.reshape(-1).long())\n",
    "\n",
    "    # 성능평가\n",
    "    score_test=MulticlassF1Score(num_classes=3)(pre_val, test_targetTS.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function_list import predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [float(x) for x in input(\"SL, SW, PL , PW: \").split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5, 1.2, 1.4, 1.8]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_val, idx =predict_data(model, data, multi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Setosa', 'Versicolor', 'Virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virginica\n"
     ]
    }
   ],
   "source": [
    "predict = class_names[idx]\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function_list import plot_loss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_score(LOSS_HISTORY, SCORE_HISTROY, 'score',100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
