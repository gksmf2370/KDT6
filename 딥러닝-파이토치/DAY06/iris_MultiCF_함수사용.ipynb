{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN 기반 다중분류 모델 구현\n",
    "- 데이터셋 : iris.csv\n",
    "- Feature : 4개 Sepal_Length, Sepal_Width, Petal_Length, Petal_Width\n",
    "- Target : 1개 Variety\n",
    "- 학습-방법 : 지도학습 > 분류 > 다중분류 (클래스 3개)\n",
    "- 알고리즘 : 인공신경망(ANN) => MLP, DNN : 은닉층이 많은 구성\n",
    "- 프레임워크 : Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 및 데이터 준비 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 관련 모듈 로딩\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torchinfo import summary\n",
    "\n",
    "# Data 관련 모듈 로딩\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch V. 2.4.1\n",
      "Pandas V. 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# 활용 패키지 버전 체크 => 사용자 정의 함수로 구현하기\n",
    "print(f'Pytorch V. {torch.__version__}')\n",
    "print(f'Pandas V. {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width variety\n",
       "0           5.1          3.5           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로딩\n",
    "DATA_FILE='../data/iris.csv'\n",
    "\n",
    "# CSV => DataFrame\n",
    "irisDF=pd.read_csv(DATA_FILE)\n",
    "\n",
    "# 데이터 확인\n",
    "irisDF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Virginica'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타겟 변경 => 정수화, 클래스 3개\n",
    "irisDF['variety'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels => {'Setosa': 0, 'Versicolor': 1, 'Virginica': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0           5.1          3.5           1.4          0.2        0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=dict(zip(irisDF['variety'].unique().tolist(),range(3)))\n",
    "print(f'labels => {labels}')\n",
    "\n",
    "irisDF['variety']=irisDF['variety'].replace(labels)\n",
    "irisDF.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 모델 클래스 설계 및 정의<hr>\n",
    "- 클래스목적 : iris 데이터를 학습 및 추론 목적 \n",
    "- 클래스이름 : IrisMCFModel\n",
    "- 부모클래스 : nn.Module\n",
    "- 매개변수 : 층별 입출력 개수 고정하기때문에 필요 없음\n",
    "- 속성필드 : \n",
    "- 기능역할 : __init__() : 모델 구조, forward() : 순방향 학습 <= 오버라이딩\n",
    "- 클래스구조\n",
    "    * 입력층 : 입력  4개(피처)  출력 10개(퍼셉트론/뉴런 10개 존재)\n",
    "    * 은닉층 : 입력 10개        출력 5개(퍼셉트론/뉴런 5개 존재)\n",
    "    * 출력층 : 입력  5개        출력 1개(퍼셉트론/뉴런 1개 존재 : 2진분류)\n",
    "\n",
    "- 활성화함수\n",
    "    * 클래스형태 => nn.MESLoss, nn.ReLU => __init__() 메서드\n",
    "    * 함수형태 => torch.nn.fuctional 아래에 => forward() 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisMCFModel(nn.Module):\n",
    "\n",
    "    # 모델 구조 구성 및 인스턴스 생성 메서드\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer=nn.Linear(4,10)\n",
    "        self.hd_layer=nn.Linear(10,5)\n",
    "        self.out_layer=nn.Linear(5,3) # 다중분류 'Setosa', 'Versicolor', 'Virginica' \n",
    "\n",
    "    # 순방향 학습 진행 메서드\n",
    "    def forward(self, x):\n",
    "        y=F.relu(self.in_layer(x))\n",
    "        y=F.relu(self.hd_layer(y))\n",
    "        return self.out_layer(y) # 5개의 숫자 값 => 다중분류 : 손실함수 CrossEntrpyLoss가 내부에서 softmax 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisMCFModel(\n",
      "  (in_layer): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (hd_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=5, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스 생성\n",
    "model=IrisMCFModel()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "IrisMCFModel                             [1000, 3]                 --\n",
       "├─Linear: 1-1                            [1000, 10]                50\n",
       "├─Linear: 1-2                            [1000, 5]                 55\n",
       "├─Linear: 1-3                            [1000, 3]                 18\n",
       "==========================================================================================\n",
       "Total params: 123\n",
       "Trainable params: 123\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.12\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.14\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 사용 메모리 정보 확인\n",
    "summary(model, input_size=(1000,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 데이터셋 클래스 설계 및 정의<hr>\n",
    "- 데이터셋 : iris.csv\n",
    "- 피쳐개수 : 4개\n",
    "- 타겟개수 : 1개\n",
    "- 클래스이름 : IrisDataset\n",
    "- 부모클래스 : utils.data.Dataset\n",
    "- 속성필드 : featureDF, targetDF, n_rows, n_features\n",
    "- 필수메서드\n",
    "    * __init__(self) : 데이터셋 저장 및 전처리, 개발자가 필요한 속성 설정\n",
    "    * __len__(self) : 데이터의 개수 반환\n",
    "    * __getitem__(self, index) : 특정 인덱스의 피쳐와 타겟 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, featureDF, targetDF):\n",
    "        self.featureDF=featureDF\n",
    "        self.targetDF=targetDF\n",
    "        self.n_rows=featureDF.shape[0]\n",
    "        self.n_features=featureDF.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 넘파이를 텐서로\n",
    "        featureTS=torch.FloatTensor(self.featureDF.iloc[index].values)\n",
    "        targetTS=torch.FloatTensor(self.targetDF.iloc[index].values)        \n",
    "        # 피쳐와 타겟 반환\n",
    "        return featureTS, targetTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3-1] 데이터셋 인스턴스 생성 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureDF => (150, 4), targetDF => (150, 1)\n"
     ]
    }
   ],
   "source": [
    "# 피쳐, 타겟 추출\n",
    "featureDF, targetDF=irisDF[irisDF.columns[:-1]], irisDF[irisDF.columns[-1:]]\n",
    "print(f'featureDF => {featureDF.shape}, targetDF => {targetDF.shape}')\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "irisDS=IrisDataset(featureDF, targetDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 학습 준비\n",
    "- 학습 횟수 : EPOCH <= 처음부터 끝까지 학습하는 단위\n",
    "- 배치 크기 : BATCH_SIZE <= 한번에 학습할 데이터셋 양\n",
    "- 위치 지정 : DEVICE <= 텐서 저장 및 실행 위치 (GPU/CPU)\n",
    "- 학습률 : LR 가중치와 절편 업데이트 시 경사하강법으로 업데이트 간격 설정 0.001~0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 진행 관련 설정\n",
    "EPOCH=1000\n",
    "BATCH_SIZE=10\n",
    "BATCH_CNT=irisDF.shape[0]/BATCH_SIZE\n",
    "DEVICE= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인스턴스/객체 : 모델, 데이터셋, 최적화 (+ 손실함수, 성능지표)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 4) (38, 4) (28, 4)\n",
      "(84, 1) (38, 1) (28, 1)\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스\n",
    "model=IrisMCFModel()\n",
    "\n",
    "# 데이터셋 인스턴스\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터 분리\n",
    "X_train, X_test, y_train, y_test=train_test_split(featureDF, targetDF, random_state=1)\n",
    "X_train, X_val, y_train, y_val=train_test_split(X_train, y_train, random_state=1)\n",
    "print(f'{X_train.shape} {X_test.shape} {X_val.shape}')\n",
    "print(f'{y_train.shape} {y_test.shape} {y_val.shape}')\n",
    "\n",
    "trainDS=IrisDataset(X_train, y_train)\n",
    "valDS=IrisDataset(X_val, y_val)\n",
    "testDS=IrisDataset(X_test, y_test)\n",
    "\n",
    "# 데이터로드 인스턴스\n",
    "trainDL=DataLoader(trainDS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화, 손실함수 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 인스턴스 => W, b 텐서 즉, model.parameters() 전달\n",
    "optimizer=optim.Adam(model.parameters(),lr=LR)\n",
    "\n",
    "# 손실함수 인스턴스 => 분류 => 다중분류 CrossEntropyLoss\n",
    "#                            예측값은 선형식 결과값으로 전달 => AF 처리 X\n",
    "crossLoss=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 파일 관련\n",
    "### models 폴더 아래 프로젝트 폴더 아래 모델 파일 저장\n",
    "import os\n",
    "\n",
    "# 저장 경로\n",
    "SAVE_PATH = '../models/iris/MCF/'\n",
    "# 저장 파일명\n",
    "SAVE_FILE=SAVE_PATH+'model_train_wbs.pth'\n",
    "\n",
    "# 모델 구조 및 파라미터 모두 저장 파일명명\n",
    "SAVE_MODEL=SAVE_PATH+'model_all.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로상 폴더 존재 여부 체크\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)   # 폴더 / 폴더/.. 하위폴더까지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000]\n",
      "- [TRAIN] LOSS : 0.6914055903752645 [SCORE] : 0.1039793555935224\n",
      "[1/1000]\n",
      "- [VAL] LOSS : 1.1237021684646606 [SCORE] : 0.18803419172763824\n",
      "[2/1000]\n",
      "- [TRAIN] LOSS : 0.6824872096379598 [SCORE] : 0.1039793555935224\n",
      "[2/1000]\n",
      "- [VAL] LOSS : 1.1140891313552856 [SCORE] : 0.18803419172763824\n",
      "[3/1000]\n",
      "- [TRAIN] LOSS : 0.6744647264480591 [SCORE] : 0.1039793555935224\n",
      "[3/1000]\n",
      "- [VAL] LOSS : 1.1032519340515137 [SCORE] : 0.18803419172763824\n",
      "[4/1000]\n",
      "- [TRAIN] LOSS : 0.6665433565775554 [SCORE] : 0.1039793555935224\n",
      "[4/1000]\n",
      "- [VAL] LOSS : 1.0936614274978638 [SCORE] : 0.18803419172763824\n",
      "[5/1000]\n",
      "- [TRAIN] LOSS : 0.6596641620000203 [SCORE] : 0.1039793555935224\n",
      "[5/1000]\n",
      "- [VAL] LOSS : 1.085762858390808 [SCORE] : 0.18803419172763824\n",
      "[6/1000]\n",
      "- [TRAIN] LOSS : 0.6537962834040324 [SCORE] : 0.1039793555935224\n",
      "[6/1000]\n",
      "- [VAL] LOSS : 1.0783411264419556 [SCORE] : 0.18803419172763824\n",
      "[7/1000]\n",
      "- [TRAIN] LOSS : 0.6478392362594605 [SCORE] : 0.1039793555935224\n",
      "[7/1000]\n",
      "- [VAL] LOSS : 1.0709097385406494 [SCORE] : 0.18803419172763824\n",
      "[8/1000]\n",
      "- [TRAIN] LOSS : 0.6423333724339803 [SCORE] : 0.1039793555935224\n",
      "[8/1000]\n",
      "- [VAL] LOSS : 1.0641558170318604 [SCORE] : 0.18803419172763824\n",
      "[9/1000]\n",
      "- [TRAIN] LOSS : 0.6374373833338419 [SCORE] : 0.1039793555935224\n",
      "[9/1000]\n",
      "- [VAL] LOSS : 1.0580750703811646 [SCORE] : 0.18803419172763824\n",
      "[10/1000]\n",
      "- [TRAIN] LOSS : 0.632799498240153 [SCORE] : 0.1039793555935224\n",
      "[10/1000]\n",
      "- [VAL] LOSS : 1.0514519214630127 [SCORE] : 0.18803419172763824\n",
      "[11/1000]\n",
      "- [TRAIN] LOSS : 0.6280202269554138 [SCORE] : 0.1039793555935224\n",
      "[11/1000]\n",
      "- [VAL] LOSS : 1.0435466766357422 [SCORE] : 0.18803419172763824\n",
      "[12/1000]\n",
      "- [TRAIN] LOSS : 0.6229413827260335 [SCORE] : 0.1039793555935224\n",
      "[12/1000]\n",
      "- [VAL] LOSS : 1.03451406955719 [SCORE] : 0.18803419172763824\n",
      "[13/1000]\n",
      "- [TRAIN] LOSS : 0.6174653093020122 [SCORE] : 0.1039793555935224\n",
      "[13/1000]\n",
      "- [VAL] LOSS : 1.024208903312683 [SCORE] : 0.18803419172763824\n",
      "[14/1000]\n",
      "- [TRAIN] LOSS : 0.6115942358970642 [SCORE] : 0.1039793555935224\n",
      "[14/1000]\n",
      "- [VAL] LOSS : 1.0124871730804443 [SCORE] : 0.18803419172763824\n",
      "[15/1000]\n",
      "- [TRAIN] LOSS : 0.6052581032117208 [SCORE] : 0.1039793555935224\n",
      "[15/1000]\n",
      "- [VAL] LOSS : 0.9993976950645447 [SCORE] : 0.18803419172763824\n",
      "[16/1000]\n",
      "- [TRAIN] LOSS : 0.598432461420695 [SCORE] : 0.1039793555935224\n",
      "[16/1000]\n",
      "- [VAL] LOSS : 0.9849671125411987 [SCORE] : 0.18803419172763824\n",
      "[17/1000]\n",
      "- [TRAIN] LOSS : 0.5911361694335937 [SCORE] : 0.1039793555935224\n",
      "[17/1000]\n",
      "- [VAL] LOSS : 0.969817042350769 [SCORE] : 0.18803419172763824\n",
      "[18/1000]\n",
      "- [TRAIN] LOSS : 0.5833705147107442 [SCORE] : 0.11384504586458206\n",
      "[18/1000]\n",
      "- [VAL] LOSS : 0.9538992643356323 [SCORE] : 0.18803419172763824\n",
      "[19/1000]\n",
      "- [TRAIN] LOSS : 0.5755986332893371 [SCORE] : 0.12371073613564174\n",
      "[19/1000]\n",
      "- [VAL] LOSS : 0.939366340637207 [SCORE] : 0.29343628883361816\n",
      "[20/1000]\n",
      "- [TRAIN] LOSS : 0.5680347681045532 [SCORE] : 0.1575327157974243\n",
      "[20/1000]\n",
      "- [VAL] LOSS : 0.9268923997879028 [SCORE] : 0.4117647111415863\n",
      "[21/1000]\n",
      "- [TRAIN] LOSS : 0.5603212157885233 [SCORE] : 0.24326636691888173\n",
      "[21/1000]\n",
      "- [VAL] LOSS : 0.9142842888832092 [SCORE] : 0.5032258033752441\n",
      "[22/1000]\n",
      "- [TRAIN] LOSS : 0.5526695450146993 [SCORE] : 0.24936545391877493\n",
      "[22/1000]\n",
      "- [VAL] LOSS : 0.9017122387886047 [SCORE] : 0.5559039115905762\n",
      "[23/1000]\n",
      "- [TRAIN] LOSS : 0.545259968439738 [SCORE] : 0.2812457988659541\n",
      "[23/1000]\n",
      "- [VAL] LOSS : 0.8898163437843323 [SCORE] : 0.5807453393936157\n",
      "[24/1000]\n",
      "- [TRAIN] LOSS : 0.5379013538360595 [SCORE] : 0.32429251074790955\n",
      "[24/1000]\n",
      "- [VAL] LOSS : 0.8773149251937866 [SCORE] : 0.604938268661499\n",
      "[25/1000]\n",
      "- [TRAIN] LOSS : 0.5309595982233684 [SCORE] : 0.32429251074790955\n",
      "[25/1000]\n",
      "- [VAL] LOSS : 0.8656885027885437 [SCORE] : 0.604938268661499\n",
      "[26/1000]\n",
      "- [TRAIN] LOSS : 0.524200189113617 [SCORE] : 0.32429251074790955\n",
      "[26/1000]\n",
      "- [VAL] LOSS : 0.8539808988571167 [SCORE] : 0.604938268661499\n",
      "[27/1000]\n",
      "- [TRAIN] LOSS : 0.5177357951800029 [SCORE] : 0.32429251074790955\n",
      "[27/1000]\n",
      "- [VAL] LOSS : 0.8422754406929016 [SCORE] : 0.604938268661499\n",
      "[28/1000]\n",
      "- [TRAIN] LOSS : 0.5115039388338725 [SCORE] : 0.3286134998003642\n",
      "[28/1000]\n",
      "- [VAL] LOSS : 0.8312073349952698 [SCORE] : 0.604938268661499\n",
      "[29/1000]\n",
      "- [TRAIN] LOSS : 0.5055705746014912 [SCORE] : 0.3286134998003642\n",
      "[29/1000]\n",
      "- [VAL] LOSS : 0.8203187584877014 [SCORE] : 0.604938268661499\n",
      "[30/1000]\n",
      "- [TRAIN] LOSS : 0.49974445899327596 [SCORE] : 0.3286134998003642\n",
      "[30/1000]\n",
      "- [VAL] LOSS : 0.8095394968986511 [SCORE] : 0.604938268661499\n",
      "[31/1000]\n",
      "- [TRAIN] LOSS : 0.4940868059794108 [SCORE] : 0.3286134998003642\n",
      "[31/1000]\n",
      "- [VAL] LOSS : 0.7990254163742065 [SCORE] : 0.604938268661499\n",
      "[32/1000]\n",
      "- [TRAIN] LOSS : 0.48847121397654214 [SCORE] : 0.3286134998003642\n",
      "[32/1000]\n",
      "- [VAL] LOSS : 0.7884764075279236 [SCORE] : 0.604938268661499\n",
      "[33/1000]\n",
      "- [TRAIN] LOSS : 0.48291831811269126 [SCORE] : 0.33266993761062624\n",
      "[33/1000]\n",
      "- [VAL] LOSS : 0.7775073647499084 [SCORE] : 0.604938268661499\n",
      "[34/1000]\n",
      "- [TRAIN] LOSS : 0.47741562525431314 [SCORE] : 0.33266993761062624\n",
      "[34/1000]\n",
      "- [VAL] LOSS : 0.7661804556846619 [SCORE] : 0.604938268661499\n",
      "[35/1000]\n",
      "- [TRAIN] LOSS : 0.4720178405443827 [SCORE] : 0.33266993761062624\n",
      "[35/1000]\n",
      "- [VAL] LOSS : 0.7549456357955933 [SCORE] : 0.604938268661499\n",
      "[36/1000]\n",
      "- [TRAIN] LOSS : 0.4668060064315796 [SCORE] : 0.33266993761062624\n",
      "[36/1000]\n",
      "- [VAL] LOSS : 0.7436484098434448 [SCORE] : 0.604938268661499\n",
      "[37/1000]\n",
      "- [TRAIN] LOSS : 0.46163610617319745 [SCORE] : 0.33266993761062624\n",
      "[37/1000]\n",
      "- [VAL] LOSS : 0.7320610284805298 [SCORE] : 0.604938268661499\n",
      "[38/1000]\n",
      "- [TRAIN] LOSS : 0.4564761519432068 [SCORE] : 0.33266993761062624\n",
      "[38/1000]\n",
      "- [VAL] LOSS : 0.720131516456604 [SCORE] : 0.604938268661499\n",
      "[39/1000]\n",
      "- [TRAIN] LOSS : 0.4512036999066671 [SCORE] : 0.33266993761062624\n",
      "[39/1000]\n",
      "- [VAL] LOSS : 0.7077264785766602 [SCORE] : 0.604938268661499\n",
      "[40/1000]\n",
      "- [TRAIN] LOSS : 0.4457505583763123 [SCORE] : 0.33266993761062624\n",
      "[40/1000]\n",
      "- [VAL] LOSS : 0.6947401165962219 [SCORE] : 0.604938268661499\n",
      "[41/1000]\n",
      "- [TRAIN] LOSS : 0.44014336665471393 [SCORE] : 0.33266993761062624\n",
      "[41/1000]\n",
      "- [VAL] LOSS : 0.6811572313308716 [SCORE] : 0.604938268661499\n",
      "[42/1000]\n",
      "- [TRAIN] LOSS : 0.434257980187734 [SCORE] : 0.33266993761062624\n",
      "[42/1000]\n",
      "- [VAL] LOSS : 0.6670019030570984 [SCORE] : 0.604938268661499\n",
      "[43/1000]\n",
      "- [TRAIN] LOSS : 0.4281221866607666 [SCORE] : 0.33266993761062624\n",
      "[43/1000]\n",
      "- [VAL] LOSS : 0.6518746614456177 [SCORE] : 0.604938268661499\n",
      "[44/1000]\n",
      "- [TRAIN] LOSS : 0.4216189980506897 [SCORE] : 0.33266993761062624\n",
      "[44/1000]\n",
      "- [VAL] LOSS : 0.6359047889709473 [SCORE] : 0.604938268661499\n",
      "[45/1000]\n",
      "- [TRAIN] LOSS : 0.4148372213045756 [SCORE] : 0.33266993761062624\n",
      "[45/1000]\n",
      "- [VAL] LOSS : 0.6193775534629822 [SCORE] : 0.604938268661499\n",
      "[46/1000]\n",
      "- [TRAIN] LOSS : 0.40777995586395266 [SCORE] : 0.33266993761062624\n",
      "[46/1000]\n",
      "- [VAL] LOSS : 0.6022729277610779 [SCORE] : 0.604938268661499\n",
      "[47/1000]\n",
      "- [TRAIN] LOSS : 0.40044367710749307 [SCORE] : 0.33266993761062624\n",
      "[47/1000]\n",
      "- [VAL] LOSS : 0.5847248435020447 [SCORE] : 0.604938268661499\n",
      "[48/1000]\n",
      "- [TRAIN] LOSS : 0.39291783968607585 [SCORE] : 0.33266993761062624\n",
      "[48/1000]\n",
      "- [VAL] LOSS : 0.5669658780097961 [SCORE] : 0.604938268661499\n",
      "[49/1000]\n",
      "- [TRAIN] LOSS : 0.38527005513509116 [SCORE] : 0.33266993761062624\n",
      "[49/1000]\n",
      "- [VAL] LOSS : 0.5492040514945984 [SCORE] : 0.604938268661499\n",
      "[50/1000]\n",
      "- [TRAIN] LOSS : 0.37757572730382283 [SCORE] : 0.33266993761062624\n",
      "[50/1000]\n",
      "- [VAL] LOSS : 0.5316815376281738 [SCORE] : 0.604938268661499\n",
      "[51/1000]\n",
      "- [TRAIN] LOSS : 0.36994587182998656 [SCORE] : 0.33266993761062624\n",
      "[51/1000]\n",
      "- [VAL] LOSS : 0.5146230459213257 [SCORE] : 0.604938268661499\n",
      "[52/1000]\n",
      "- [TRAIN] LOSS : 0.36245233615239464 [SCORE] : 0.33266993761062624\n",
      "[52/1000]\n",
      "- [VAL] LOSS : 0.4984416961669922 [SCORE] : 0.604938268661499\n",
      "[53/1000]\n",
      "- [TRAIN] LOSS : 0.3551565806070964 [SCORE] : 0.33266993761062624\n",
      "[53/1000]\n",
      "- [VAL] LOSS : 0.48317810893058777 [SCORE] : 0.604938268661499\n",
      "[54/1000]\n",
      "- [TRAIN] LOSS : 0.3481205681959788 [SCORE] : 0.33266993761062624\n",
      "[54/1000]\n",
      "- [VAL] LOSS : 0.46870264410972595 [SCORE] : 0.604938268661499\n",
      "[55/1000]\n",
      "- [TRAIN] LOSS : 0.34143089652061465 [SCORE] : 0.33266993761062624\n",
      "[55/1000]\n",
      "- [VAL] LOSS : 0.45521309971809387 [SCORE] : 0.604938268661499\n",
      "[56/1000]\n",
      "- [TRAIN] LOSS : 0.3350453058878581 [SCORE] : 0.33266993761062624\n",
      "[56/1000]\n",
      "- [VAL] LOSS : 0.4423660635948181 [SCORE] : 0.604938268661499\n",
      "[57/1000]\n",
      "- [TRAIN] LOSS : 0.32902917861938474 [SCORE] : 0.33266993761062624\n",
      "[57/1000]\n",
      "- [VAL] LOSS : 0.43035706877708435 [SCORE] : 0.604938268661499\n",
      "[58/1000]\n",
      "- [TRAIN] LOSS : 0.3233230928579966 [SCORE] : 0.33266993761062624\n",
      "[58/1000]\n",
      "- [VAL] LOSS : 0.4194532036781311 [SCORE] : 0.604938268661499\n",
      "[59/1000]\n",
      "- [TRAIN] LOSS : 0.3179068903128306 [SCORE] : 0.33266993761062624\n",
      "[59/1000]\n",
      "- [VAL] LOSS : 0.4093966484069824 [SCORE] : 0.604938268661499\n",
      "[60/1000]\n",
      "- [TRAIN] LOSS : 0.3127368708451589 [SCORE] : 0.33266993761062624\n",
      "[60/1000]\n",
      "- [VAL] LOSS : 0.40013405680656433 [SCORE] : 0.604938268661499\n",
      "[61/1000]\n",
      "- [TRAIN] LOSS : 0.3077639122804006 [SCORE] : 0.33266993761062624\n",
      "[61/1000]\n",
      "- [VAL] LOSS : 0.3916318416595459 [SCORE] : 0.7264957427978516\n",
      "[62/1000]\n",
      "- [TRAIN] LOSS : 0.30297087828318275 [SCORE] : 0.3412169456481934\n",
      "[62/1000]\n",
      "- [VAL] LOSS : 0.3836391568183899 [SCORE] : 0.7264957427978516\n",
      "[63/1000]\n",
      "- [TRAIN] LOSS : 0.2983254075050354 [SCORE] : 0.3549976110458374\n",
      "[63/1000]\n",
      "- [VAL] LOSS : 0.3759538531303406 [SCORE] : 0.7264957427978516\n",
      "[64/1000]\n",
      "- [TRAIN] LOSS : 0.2938207983970642 [SCORE] : 0.3793650905291239\n",
      "[64/1000]\n",
      "- [VAL] LOSS : 0.36858993768692017 [SCORE] : 0.7264957427978516\n",
      "[65/1000]\n",
      "- [TRAIN] LOSS : 0.2893414815266927 [SCORE] : 0.40164022048314413\n",
      "[65/1000]\n",
      "- [VAL] LOSS : 0.3606399893760681 [SCORE] : 0.8171428442001343\n",
      "[66/1000]\n",
      "- [TRAIN] LOSS : 0.2851313531398773 [SCORE] : 0.40164022048314413\n",
      "[66/1000]\n",
      "- [VAL] LOSS : 0.35376209020614624 [SCORE] : 0.8171428442001343\n",
      "[67/1000]\n",
      "- [TRAIN] LOSS : 0.2808797279993693 [SCORE] : 0.40164022048314413\n",
      "[67/1000]\n",
      "- [VAL] LOSS : 0.3466309905052185 [SCORE] : 0.8171428442001343\n",
      "[68/1000]\n",
      "- [TRAIN] LOSS : 0.27667325536410015 [SCORE] : 0.4334920724232992\n",
      "[68/1000]\n",
      "- [VAL] LOSS : 0.33947157859802246 [SCORE] : 0.8171428442001343\n",
      "[69/1000]\n",
      "- [TRAIN] LOSS : 0.2725700795650482 [SCORE] : 0.4431217034657796\n",
      "[69/1000]\n",
      "- [VAL] LOSS : 0.33278340101242065 [SCORE] : 0.8171428442001343\n",
      "[70/1000]\n",
      "- [TRAIN] LOSS : 0.2684333900610606 [SCORE] : 0.4431217034657796\n",
      "[70/1000]\n",
      "- [VAL] LOSS : 0.32628899812698364 [SCORE] : 0.888888955116272\n",
      "[71/1000]\n",
      "- [TRAIN] LOSS : 0.26421499252319336 [SCORE] : 0.44830689032872517\n",
      "[71/1000]\n",
      "- [VAL] LOSS : 0.3197278082370758 [SCORE] : 0.888888955116272\n",
      "[72/1000]\n",
      "- [TRAIN] LOSS : 0.2598855197429657 [SCORE] : 0.45854258139928183\n",
      "[72/1000]\n",
      "- [VAL] LOSS : 0.31333690881729126 [SCORE] : 0.888888955116272\n",
      "[73/1000]\n",
      "- [TRAIN] LOSS : 0.25526387492815655 [SCORE] : 0.48793651660283405\n",
      "[73/1000]\n",
      "- [VAL] LOSS : 0.30657273530960083 [SCORE] : 0.888888955116272\n",
      "[74/1000]\n",
      "- [TRAIN] LOSS : 0.2504157284895579 [SCORE] : 0.5351160009702046\n",
      "[74/1000]\n",
      "- [VAL] LOSS : 0.2992541491985321 [SCORE] : 0.888888955116272\n",
      "[75/1000]\n",
      "- [TRAIN] LOSS : 0.24534293214480082 [SCORE] : 0.5351160009702046\n",
      "[75/1000]\n",
      "- [VAL] LOSS : 0.29145342111587524 [SCORE] : 0.9484702348709106\n",
      "[76/1000]\n",
      "- [TRAIN] LOSS : 0.24036365747451782 [SCORE] : 0.5489431619644165\n",
      "[76/1000]\n",
      "- [VAL] LOSS : 0.28379836678504944 [SCORE] : 0.9484702348709106\n",
      "[77/1000]\n",
      "- [TRAIN] LOSS : 0.2353355367978414 [SCORE] : 0.5538814345995585\n",
      "[77/1000]\n",
      "- [VAL] LOSS : 0.2765091061592102 [SCORE] : 0.9484702348709106\n",
      "[78/1000]\n",
      "- [TRAIN] LOSS : 0.23018314639727275 [SCORE] : 0.5600093762079875\n",
      "[78/1000]\n",
      "- [VAL] LOSS : 0.268463671207428 [SCORE] : 0.9484702348709106\n",
      "[79/1000]\n",
      "- [TRAIN] LOSS : 0.22523006399472553 [SCORE] : 0.5600093762079875\n",
      "[79/1000]\n",
      "- [VAL] LOSS : 0.2611568570137024 [SCORE] : 0.9484702348709106\n",
      "[80/1000]\n",
      "- [TRAIN] LOSS : 0.22021123965581257 [SCORE] : 0.5600093762079875\n",
      "[80/1000]\n",
      "- [VAL] LOSS : 0.2542780637741089 [SCORE] : 0.9484702348709106\n",
      "[81/1000]\n",
      "- [TRAIN] LOSS : 0.2151170531908671 [SCORE] : 0.5600093762079875\n",
      "[81/1000]\n",
      "- [VAL] LOSS : 0.247012659907341 [SCORE] : 0.9484702348709106\n",
      "[82/1000]\n",
      "- [TRAIN] LOSS : 0.21014058391253154 [SCORE] : 0.5600093762079875\n",
      "[82/1000]\n",
      "- [VAL] LOSS : 0.24001793563365936 [SCORE] : 0.9484702348709106\n",
      "[83/1000]\n",
      "- [TRAIN] LOSS : 0.2051884671052297 [SCORE] : 0.5600093762079875\n",
      "[83/1000]\n",
      "- [VAL] LOSS : 0.2333325892686844 [SCORE] : 0.9484702348709106\n",
      "[84/1000]\n",
      "- [TRAIN] LOSS : 0.2002471407254537 [SCORE] : 0.5600093762079875\n",
      "[84/1000]\n",
      "- [VAL] LOSS : 0.22657136619091034 [SCORE] : 0.9484702348709106\n",
      "[85/1000]\n",
      "- [TRAIN] LOSS : 0.19534763197104135 [SCORE] : 0.5600093762079875\n",
      "[85/1000]\n",
      "- [VAL] LOSS : 0.2193903923034668 [SCORE] : 0.9484702348709106\n",
      "[86/1000]\n",
      "- [TRAIN] LOSS : 0.1908232678969701 [SCORE] : 0.5600093762079875\n",
      "[86/1000]\n",
      "- [VAL] LOSS : 0.213580921292305 [SCORE] : 0.9484702348709106\n",
      "[87/1000]\n",
      "- [TRAIN] LOSS : 0.18603045642375945 [SCORE] : 0.5600093762079875\n",
      "[87/1000]\n",
      "- [VAL] LOSS : 0.2069128453731537 [SCORE] : 0.9484702348709106\n",
      "[88/1000]\n",
      "- [TRAIN] LOSS : 0.18158803979555765 [SCORE] : 0.5661632219950358\n",
      "[88/1000]\n",
      "- [VAL] LOSS : 0.20112669467926025 [SCORE] : 0.9484702348709106\n",
      "[89/1000]\n",
      "- [TRAIN] LOSS : 0.17704892655213675 [SCORE] : 0.5661632219950358\n",
      "[89/1000]\n",
      "- [VAL] LOSS : 0.1949935406446457 [SCORE] : 0.9484702348709106\n",
      "[90/1000]\n",
      "- [TRAIN] LOSS : 0.1727108895778656 [SCORE] : 0.5755908330281575\n",
      "[90/1000]\n",
      "- [VAL] LOSS : 0.18914936482906342 [SCORE] : 0.9484702348709106\n",
      "[91/1000]\n",
      "- [TRAIN] LOSS : 0.16845185856024425 [SCORE] : 0.5755908330281575\n",
      "[91/1000]\n",
      "- [VAL] LOSS : 0.18345914781093597 [SCORE] : 0.9484702348709106\n",
      "[92/1000]\n",
      "- [TRAIN] LOSS : 0.16434539755185446 [SCORE] : 0.5755908330281575\n",
      "[92/1000]\n",
      "- [VAL] LOSS : 0.17816302180290222 [SCORE] : 0.9484702348709106\n",
      "[93/1000]\n",
      "- [TRAIN] LOSS : 0.16029727856318157 [SCORE] : 0.5755908330281575\n",
      "[93/1000]\n",
      "- [VAL] LOSS : 0.17256410419940948 [SCORE] : 0.9484702348709106\n",
      "[94/1000]\n",
      "- [TRAIN] LOSS : 0.1563622216383616 [SCORE] : 0.5755908330281575\n",
      "[94/1000]\n",
      "- [VAL] LOSS : 0.16724278032779694 [SCORE] : 0.9484702348709106\n",
      "[95/1000]\n",
      "- [TRAIN] LOSS : 0.15291881163914997 [SCORE] : 0.5755908330281575\n",
      "[95/1000]\n",
      "- [VAL] LOSS : 0.16325972974300385 [SCORE] : 0.9484702348709106\n",
      "[96/1000]\n",
      "- [TRAIN] LOSS : 0.14876350661118826 [SCORE] : 0.5755908330281575\n",
      "[96/1000]\n",
      "- [VAL] LOSS : 0.1574101746082306 [SCORE] : 0.9484702348709106\n",
      "[97/1000]\n",
      "- [TRAIN] LOSS : 0.14571984310944874 [SCORE] : 0.5755908330281575\n",
      "[97/1000]\n",
      "- [VAL] LOSS : 0.15390504896640778 [SCORE] : 0.9484702348709106\n",
      "[98/1000]\n",
      "- [TRAIN] LOSS : 0.1420841117699941 [SCORE] : 0.5755908330281575\n",
      "[98/1000]\n",
      "- [VAL] LOSS : 0.14913998544216156 [SCORE] : 0.9484702348709106\n",
      "[99/1000]\n",
      "- [TRAIN] LOSS : 0.13870502511660257 [SCORE] : 0.5755908330281575\n",
      "[99/1000]\n",
      "- [VAL] LOSS : 0.14436949789524078 [SCORE] : 0.9484702348709106\n",
      "[100/1000]\n",
      "- [TRAIN] LOSS : 0.135955814520518 [SCORE] : 0.5755908330281575\n",
      "[100/1000]\n",
      "- [VAL] LOSS : 0.14109328389167786 [SCORE] : 0.9484702348709106\n",
      "[101/1000]\n",
      "- [TRAIN] LOSS : 0.1325181653102239 [SCORE] : 0.5755908330281575\n",
      "[101/1000]\n",
      "- [VAL] LOSS : 0.1365155428647995 [SCORE] : 0.9484702348709106\n",
      "[102/1000]\n",
      "- [TRAIN] LOSS : 0.1299974391857783 [SCORE] : 0.5755908330281575\n",
      "[102/1000]\n",
      "- [VAL] LOSS : 0.13333900272846222 [SCORE] : 0.9484702348709106\n",
      "[103/1000]\n",
      "- [TRAIN] LOSS : 0.1269790917634964 [SCORE] : 0.5755908330281575\n",
      "[103/1000]\n",
      "- [VAL] LOSS : 0.12955500185489655 [SCORE] : 0.9484702348709106\n",
      "[104/1000]\n",
      "- [TRAIN] LOSS : 0.1244841307401657 [SCORE] : 0.5755908330281575\n",
      "[104/1000]\n",
      "- [VAL] LOSS : 0.12629087269306183 [SCORE] : 0.9484702348709106\n",
      "[105/1000]\n",
      "- [TRAIN] LOSS : 0.12171004265546799 [SCORE] : 0.5755908330281575\n",
      "[105/1000]\n",
      "- [VAL] LOSS : 0.12252062559127808 [SCORE] : 0.9484702348709106\n",
      "[106/1000]\n",
      "- [TRAIN] LOSS : 0.11963668366273245 [SCORE] : 0.5755908330281575\n",
      "[106/1000]\n",
      "- [VAL] LOSS : 0.11990823596715927 [SCORE] : 1.0\n",
      "[107/1000]\n",
      "- [TRAIN] LOSS : 0.11699530978997548 [SCORE] : 0.5755908330281575\n",
      "[107/1000]\n",
      "- [VAL] LOSS : 0.1165446788072586 [SCORE] : 0.9484702348709106\n",
      "[108/1000]\n",
      "- [TRAIN] LOSS : 0.11502143045266469 [SCORE] : 0.5755908330281575\n",
      "[108/1000]\n",
      "- [VAL] LOSS : 0.1139478087425232 [SCORE] : 1.0\n",
      "[109/1000]\n",
      "- [TRAIN] LOSS : 0.11267026116450628 [SCORE] : 0.5755908330281575\n",
      "[109/1000]\n",
      "- [VAL] LOSS : 0.11091065406799316 [SCORE] : 0.9484702348709106\n",
      "[110/1000]\n",
      "- [TRAIN] LOSS : 0.11090090523163477 [SCORE] : 0.5755908330281575\n",
      "[110/1000]\n",
      "- [VAL] LOSS : 0.10861312597990036 [SCORE] : 1.0\n",
      "[111/1000]\n",
      "- [TRAIN] LOSS : 0.10870071947574615 [SCORE] : 0.5755908330281575\n",
      "[111/1000]\n",
      "- [VAL] LOSS : 0.1057378426194191 [SCORE] : 0.9484702348709106\n",
      "[112/1000]\n",
      "- [TRAIN] LOSS : 0.10701521734396617 [SCORE] : 0.5755908330281575\n",
      "[112/1000]\n",
      "- [VAL] LOSS : 0.10349749773740768 [SCORE] : 1.0\n",
      "[113/1000]\n",
      "- [TRAIN] LOSS : 0.10528905888398489 [SCORE] : 0.5755908330281575\n",
      "[113/1000]\n",
      "- [VAL] LOSS : 0.10135924071073532 [SCORE] : 1.0\n",
      "[114/1000]\n",
      "- [TRAIN] LOSS : 0.10340466499328613 [SCORE] : 0.5755908330281575\n",
      "[114/1000]\n",
      "- [VAL] LOSS : 0.0989270806312561 [SCORE] : 1.0\n",
      "[115/1000]\n",
      "- [TRAIN] LOSS : 0.1019644742210706 [SCORE] : 0.5755908330281575\n",
      "[115/1000]\n",
      "- [VAL] LOSS : 0.09694496542215347 [SCORE] : 1.0\n",
      "[116/1000]\n",
      "- [TRAIN] LOSS : 0.10031734158595403 [SCORE] : 0.5755908330281575\n",
      "[116/1000]\n",
      "- [VAL] LOSS : 0.0948357805609703 [SCORE] : 1.0\n",
      "[117/1000]\n",
      "- [TRAIN] LOSS : 0.09889027873675028 [SCORE] : 0.5755908330281575\n",
      "[117/1000]\n",
      "- [VAL] LOSS : 0.09296663105487823 [SCORE] : 1.0\n",
      "[118/1000]\n",
      "- [TRAIN] LOSS : 0.09756035407384236 [SCORE] : 0.5755908330281575\n",
      "[118/1000]\n",
      "- [VAL] LOSS : 0.09128411114215851 [SCORE] : 1.0\n",
      "[119/1000]\n",
      "- [TRAIN] LOSS : 0.09603999505440394 [SCORE] : 0.5755908330281575\n",
      "[119/1000]\n",
      "- [VAL] LOSS : 0.08934049308300018 [SCORE] : 1.0\n",
      "[120/1000]\n",
      "- [TRAIN] LOSS : 0.09483392139275869 [SCORE] : 0.5755908330281575\n",
      "[120/1000]\n",
      "- [VAL] LOSS : 0.08767806738615036 [SCORE] : 1.0\n",
      "[121/1000]\n",
      "- [TRAIN] LOSS : 0.09369248648484547 [SCORE] : 0.5755908330281575\n",
      "[121/1000]\n",
      "- [VAL] LOSS : 0.08621538430452347 [SCORE] : 1.0\n",
      "[122/1000]\n",
      "- [TRAIN] LOSS : 0.09234338452418646 [SCORE] : 0.5755908330281575\n",
      "[122/1000]\n",
      "- [VAL] LOSS : 0.08452197164297104 [SCORE] : 1.0\n",
      "[123/1000]\n",
      "- [TRAIN] LOSS : 0.0912713368733724 [SCORE] : 0.5755908330281575\n",
      "[123/1000]\n",
      "- [VAL] LOSS : 0.08301323652267456 [SCORE] : 1.0\n",
      "[124/1000]\n",
      "- [TRAIN] LOSS : 0.09023846884568533 [SCORE] : 0.5755908330281575\n",
      "[124/1000]\n",
      "- [VAL] LOSS : 0.08164869248867035 [SCORE] : 1.0\n",
      "[125/1000]\n",
      "- [TRAIN] LOSS : 0.08916943917671839 [SCORE] : 0.5755908330281575\n",
      "[125/1000]\n",
      "- [VAL] LOSS : 0.08029554784297943 [SCORE] : 1.0\n",
      "[126/1000]\n",
      "- [TRAIN] LOSS : 0.08815469518303871 [SCORE] : 0.5755908330281575\n",
      "[126/1000]\n",
      "- [VAL] LOSS : 0.07896871119737625 [SCORE] : 1.0\n",
      "[127/1000]\n",
      "- [TRAIN] LOSS : 0.08721129174033801 [SCORE] : 0.5755908330281575\n",
      "[127/1000]\n",
      "- [VAL] LOSS : 0.07771033048629761 [SCORE] : 1.0\n",
      "[128/1000]\n",
      "- [TRAIN] LOSS : 0.0863002061843872 [SCORE] : 0.5755908330281575\n",
      "[128/1000]\n",
      "- [VAL] LOSS : 0.07650772482156754 [SCORE] : 1.0\n",
      "[129/1000]\n",
      "- [TRAIN] LOSS : 0.08541502902905146 [SCORE] : 0.5755908330281575\n",
      "[129/1000]\n",
      "- [VAL] LOSS : 0.0753437727689743 [SCORE] : 1.0\n",
      "[130/1000]\n",
      "- [TRAIN] LOSS : 0.08456804330150286 [SCORE] : 0.5755908330281575\n",
      "[130/1000]\n",
      "- [VAL] LOSS : 0.07422059029340744 [SCORE] : 1.0\n",
      "[131/1000]\n",
      "- [TRAIN] LOSS : 0.08375830575823784 [SCORE] : 0.5755908330281575\n",
      "[131/1000]\n",
      "- [VAL] LOSS : 0.07314102351665497 [SCORE] : 1.0\n",
      "[132/1000]\n",
      "- [TRAIN] LOSS : 0.08297868619362513 [SCORE] : 0.5755908330281575\n",
      "[132/1000]\n",
      "- [VAL] LOSS : 0.0721011534333229 [SCORE] : 1.0\n",
      "[133/1000]\n",
      "- [TRAIN] LOSS : 0.08222764159242311 [SCORE] : 0.5755908330281575\n",
      "[133/1000]\n",
      "- [VAL] LOSS : 0.07109769433736801 [SCORE] : 1.0\n",
      "[134/1000]\n",
      "- [TRAIN] LOSS : 0.08150574366251627 [SCORE] : 0.5755908330281575\n",
      "[134/1000]\n",
      "- [VAL] LOSS : 0.07012946158647537 [SCORE] : 1.0\n",
      "[135/1000]\n",
      "- [TRAIN] LOSS : 0.08081157033642133 [SCORE] : 0.5755908330281575\n",
      "[135/1000]\n",
      "- [VAL] LOSS : 0.06919541209936142 [SCORE] : 1.0\n",
      "[136/1000]\n",
      "- [TRAIN] LOSS : 0.08014291673898696 [SCORE] : 0.5755908330281575\n",
      "[136/1000]\n",
      "- [VAL] LOSS : 0.06829363107681274 [SCORE] : 1.0\n",
      "[137/1000]\n",
      "- [TRAIN] LOSS : 0.0794987623890241 [SCORE] : 0.5755908330281575\n",
      "[137/1000]\n",
      "- [VAL] LOSS : 0.06742246448993683 [SCORE] : 1.0\n",
      "[138/1000]\n",
      "- [TRAIN] LOSS : 0.0788781168560187 [SCORE] : 0.5755908330281575\n",
      "[138/1000]\n",
      "- [VAL] LOSS : 0.0665806457400322 [SCORE] : 1.0\n",
      "[139/1000]\n",
      "- [TRAIN] LOSS : 0.0782798945903778 [SCORE] : 0.5755908330281575\n",
      "[139/1000]\n",
      "- [VAL] LOSS : 0.06576787680387497 [SCORE] : 1.0\n",
      "[140/1000]\n",
      "- [TRAIN] LOSS : 0.07770293851693472 [SCORE] : 0.5755908330281575\n",
      "[140/1000]\n",
      "- [VAL] LOSS : 0.064982108771801 [SCORE] : 1.0\n",
      "[141/1000]\n",
      "- [TRAIN] LOSS : 0.07714626888434092 [SCORE] : 0.5755908330281575\n",
      "[141/1000]\n",
      "- [VAL] LOSS : 0.0642218217253685 [SCORE] : 1.0\n",
      "[142/1000]\n",
      "- [TRAIN] LOSS : 0.07660901471972466 [SCORE] : 0.5755908330281575\n",
      "[142/1000]\n",
      "- [VAL] LOSS : 0.06348586082458496 [SCORE] : 1.0\n",
      "[143/1000]\n",
      "- [TRAIN] LOSS : 0.0760902946194013 [SCORE] : 0.5755908330281575\n",
      "[143/1000]\n",
      "- [VAL] LOSS : 0.06277327984571457 [SCORE] : 1.0\n",
      "[144/1000]\n",
      "- [TRAIN] LOSS : 0.07558923363685607 [SCORE] : 0.5755908330281575\n",
      "[144/1000]\n",
      "- [VAL] LOSS : 0.062082935124635696 [SCORE] : 1.0\n",
      "[145/1000]\n",
      "- [TRAIN] LOSS : 0.07508696218331655 [SCORE] : 0.5755908330281575\n",
      "[145/1000]\n",
      "- [VAL] LOSS : 0.061410389840602875 [SCORE] : 1.0\n",
      "[146/1000]\n",
      "- [TRAIN] LOSS : 0.0746311143040657 [SCORE] : 0.5755908330281575\n",
      "[146/1000]\n",
      "- [VAL] LOSS : 0.0607646107673645 [SCORE] : 1.0\n",
      "[147/1000]\n",
      "- [TRAIN] LOSS : 0.07419066975514094 [SCORE] : 0.5755908330281575\n",
      "[147/1000]\n",
      "- [VAL] LOSS : 0.06014254316687584 [SCORE] : 1.0\n",
      "[148/1000]\n",
      "- [TRAIN] LOSS : 0.07375064417719841 [SCORE] : 0.5755908330281575\n",
      "[148/1000]\n",
      "- [VAL] LOSS : 0.059537891298532486 [SCORE] : 1.0\n",
      "[149/1000]\n",
      "- [TRAIN] LOSS : 0.07332752719521522 [SCORE] : 0.5755908330281575\n",
      "[149/1000]\n",
      "- [VAL] LOSS : 0.05894993618130684 [SCORE] : 1.0\n",
      "[150/1000]\n",
      "- [TRAIN] LOSS : 0.07291854669650395 [SCORE] : 0.5755908330281575\n",
      "[150/1000]\n",
      "- [VAL] LOSS : 0.05837855115532875 [SCORE] : 1.0\n",
      "[151/1000]\n",
      "- [TRAIN] LOSS : 0.07252341682712236 [SCORE] : 0.5755908330281575\n",
      "[151/1000]\n",
      "- [VAL] LOSS : 0.05782341584563255 [SCORE] : 1.0\n",
      "[152/1000]\n",
      "- [TRAIN] LOSS : 0.07214106693863868 [SCORE] : 0.5755908330281575\n",
      "[152/1000]\n",
      "- [VAL] LOSS : 0.05728410556912422 [SCORE] : 1.0\n",
      "[153/1000]\n",
      "- [TRAIN] LOSS : 0.07177023068070412 [SCORE] : 0.5755908330281575\n",
      "[153/1000]\n",
      "- [VAL] LOSS : 0.056759949773550034 [SCORE] : 1.0\n",
      "[154/1000]\n",
      "- [TRAIN] LOSS : 0.07141061748067538 [SCORE] : 0.5755908330281575\n",
      "[154/1000]\n",
      "- [VAL] LOSS : 0.05625023692846298 [SCORE] : 1.0\n",
      "[155/1000]\n",
      "- [TRAIN] LOSS : 0.07106103574236235 [SCORE] : 0.5755908330281575\n",
      "[155/1000]\n",
      "- [VAL] LOSS : 0.05575456842780113 [SCORE] : 1.0\n",
      "[156/1000]\n",
      "- [TRAIN] LOSS : 0.07072377428412438 [SCORE] : 0.5755908330281575\n",
      "[156/1000]\n",
      "- [VAL] LOSS : 0.05527244135737419 [SCORE] : 1.0\n",
      "[157/1000]\n",
      "- [TRAIN] LOSS : 0.07039646543562413 [SCORE] : 0.5755908330281575\n",
      "[157/1000]\n",
      "- [VAL] LOSS : 0.05480322986841202 [SCORE] : 1.0\n",
      "[158/1000]\n",
      "- [TRAIN] LOSS : 0.07007788345217705 [SCORE] : 0.5755908330281575\n",
      "[158/1000]\n",
      "- [VAL] LOSS : 0.05434640124440193 [SCORE] : 1.0\n",
      "[159/1000]\n",
      "- [TRAIN] LOSS : 0.06976853609085083 [SCORE] : 0.5755908330281575\n",
      "[159/1000]\n",
      "- [VAL] LOSS : 0.05390152335166931 [SCORE] : 1.0\n",
      "[160/1000]\n",
      "- [TRAIN] LOSS : 0.0694684191296498 [SCORE] : 0.5755908330281575\n",
      "[160/1000]\n",
      "- [VAL] LOSS : 0.05346808582544327 [SCORE] : 1.0\n",
      "[161/1000]\n",
      "- [TRAIN] LOSS : 0.06917691628138224 [SCORE] : 0.5755908330281575\n",
      "[161/1000]\n",
      "- [VAL] LOSS : 0.05304576829075813 [SCORE] : 1.0\n",
      "[162/1000]\n",
      "- [TRAIN] LOSS : 0.06889352624615033 [SCORE] : 0.5755908330281575\n",
      "[162/1000]\n",
      "- [VAL] LOSS : 0.05263404920697212 [SCORE] : 1.0\n",
      "[163/1000]\n",
      "- [TRAIN] LOSS : 0.06861804525057474 [SCORE] : 0.5755908330281575\n",
      "[163/1000]\n",
      "- [VAL] LOSS : 0.052232686430215836 [SCORE] : 1.0\n",
      "[164/1000]\n",
      "- [TRAIN] LOSS : 0.06835025623440742 [SCORE] : 0.5755908330281575\n",
      "[164/1000]\n",
      "- [VAL] LOSS : 0.051841266453266144 [SCORE] : 1.0\n",
      "[165/1000]\n",
      "- [TRAIN] LOSS : 0.06808983037869136 [SCORE] : 0.5755908330281575\n",
      "[165/1000]\n",
      "- [VAL] LOSS : 0.05145939439535141 [SCORE] : 1.0\n",
      "[166/1000]\n",
      "- [TRAIN] LOSS : 0.06783646841843922 [SCORE] : 0.5755908330281575\n",
      "[166/1000]\n",
      "- [VAL] LOSS : 0.051086850464344025 [SCORE] : 1.0\n",
      "[167/1000]\n",
      "- [TRAIN] LOSS : 0.0675898902118206 [SCORE] : 0.5755908330281575\n",
      "[167/1000]\n",
      "- [VAL] LOSS : 0.050723206251859665 [SCORE] : 1.0\n",
      "[168/1000]\n",
      "- [TRAIN] LOSS : 0.06734992538889249 [SCORE] : 0.5755908330281575\n",
      "[168/1000]\n",
      "- [VAL] LOSS : 0.05036818981170654 [SCORE] : 1.0\n",
      "[169/1000]\n",
      "- [TRAIN] LOSS : 0.06711631243427595 [SCORE] : 0.5755908330281575\n",
      "[169/1000]\n",
      "- [VAL] LOSS : 0.050021592527627945 [SCORE] : 1.0\n",
      "[170/1000]\n",
      "- [TRAIN] LOSS : 0.06688880460957686 [SCORE] : 0.5755908330281575\n",
      "[170/1000]\n",
      "- [VAL] LOSS : 0.049683827906847 [SCORE] : 1.0\n",
      "[171/1000]\n",
      "- [TRAIN] LOSS : 0.06666714176535607 [SCORE] : 0.5755908330281575\n",
      "[171/1000]\n",
      "- [VAL] LOSS : 0.049354273825883865 [SCORE] : 1.0\n",
      "[172/1000]\n",
      "- [TRAIN] LOSS : 0.06645123660564423 [SCORE] : 0.5755908330281575\n",
      "[172/1000]\n",
      "- [VAL] LOSS : 0.049032263457775116 [SCORE] : 1.0\n",
      "[173/1000]\n",
      "- [TRAIN] LOSS : 0.06624079942703247 [SCORE] : 0.5755908330281575\n",
      "[173/1000]\n",
      "- [VAL] LOSS : 0.04871752858161926 [SCORE] : 1.0\n",
      "[174/1000]\n",
      "- [TRAIN] LOSS : 0.0660357074191173 [SCORE] : 0.5755908330281575\n",
      "[174/1000]\n",
      "- [VAL] LOSS : 0.04840979725122452 [SCORE] : 1.0\n",
      "[175/1000]\n",
      "- [TRAIN] LOSS : 0.06583567423125108 [SCORE] : 0.5755908330281575\n",
      "[175/1000]\n",
      "- [VAL] LOSS : 0.04810893535614014 [SCORE] : 1.0\n",
      "[176/1000]\n",
      "- [TRAIN] LOSS : 0.06564065515995025 [SCORE] : 0.5755908330281575\n",
      "[176/1000]\n",
      "- [VAL] LOSS : 0.04781464487314224 [SCORE] : 1.0\n",
      "[177/1000]\n",
      "- [TRAIN] LOSS : 0.06545039812723795 [SCORE] : 0.5755908330281575\n",
      "[177/1000]\n",
      "- [VAL] LOSS : 0.04752681776881218 [SCORE] : 1.0\n",
      "[178/1000]\n",
      "- [TRAIN] LOSS : 0.06526478081941604 [SCORE] : 0.5755908330281575\n",
      "[178/1000]\n",
      "- [VAL] LOSS : 0.047245126217603683 [SCORE] : 1.0\n",
      "[179/1000]\n",
      "- [TRAIN] LOSS : 0.06508363125224909 [SCORE] : 0.5755908330281575\n",
      "[179/1000]\n",
      "- [VAL] LOSS : 0.046969421207904816 [SCORE] : 1.0\n",
      "[180/1000]\n",
      "- [TRAIN] LOSS : 0.064906823883454 [SCORE] : 0.5755908330281575\n",
      "[180/1000]\n",
      "- [VAL] LOSS : 0.046699605882167816 [SCORE] : 1.0\n",
      "[181/1000]\n",
      "- [TRAIN] LOSS : 0.0647342149168253 [SCORE] : 0.5755908330281575\n",
      "[181/1000]\n",
      "- [VAL] LOSS : 0.0464354045689106 [SCORE] : 1.0\n",
      "[182/1000]\n",
      "- [TRAIN] LOSS : 0.06456560467680296 [SCORE] : 0.5755908330281575\n",
      "[182/1000]\n",
      "- [VAL] LOSS : 0.04617668315768242 [SCORE] : 1.0\n",
      "[183/1000]\n",
      "- [TRAIN] LOSS : 0.06440098745127519 [SCORE] : 0.5755908330281575\n",
      "[183/1000]\n",
      "- [VAL] LOSS : 0.04592330381274223 [SCORE] : 1.0\n",
      "[184/1000]\n",
      "- [TRAIN] LOSS : 0.06424017200867335 [SCORE] : 0.5755908330281575\n",
      "[184/1000]\n",
      "- [VAL] LOSS : 0.0456751249730587 [SCORE] : 1.0\n",
      "[185/1000]\n",
      "- [TRAIN] LOSS : 0.06408300486703714 [SCORE] : 0.5755908330281575\n",
      "[185/1000]\n",
      "- [VAL] LOSS : 0.045431915670633316 [SCORE] : 1.0\n",
      "[186/1000]\n",
      "- [TRAIN] LOSS : 0.06392942517995834 [SCORE] : 0.5755908330281575\n",
      "[186/1000]\n",
      "- [VAL] LOSS : 0.04519353434443474 [SCORE] : 1.0\n",
      "[187/1000]\n",
      "- [TRAIN] LOSS : 0.06377898640930653 [SCORE] : 0.5755908330281575\n",
      "[187/1000]\n",
      "- [VAL] LOSS : 0.044960085302591324 [SCORE] : 1.0\n",
      "[188/1000]\n",
      "- [TRAIN] LOSS : 0.06363236792385578 [SCORE] : 0.5755908330281575\n",
      "[188/1000]\n",
      "- [VAL] LOSS : 0.04473116993904114 [SCORE] : 1.0\n",
      "[189/1000]\n",
      "- [TRAIN] LOSS : 0.0634890497972568 [SCORE] : 0.5755908330281575\n",
      "[189/1000]\n",
      "- [VAL] LOSS : 0.04450665041804314 [SCORE] : 1.0\n",
      "[190/1000]\n",
      "- [TRAIN] LOSS : 0.06334898931284745 [SCORE] : 0.5755908330281575\n",
      "[190/1000]\n",
      "- [VAL] LOSS : 0.04428630694746971 [SCORE] : 1.0\n",
      "[191/1000]\n",
      "- [TRAIN] LOSS : 0.06321121069292228 [SCORE] : 0.5755908330281575\n",
      "[191/1000]\n",
      "- [VAL] LOSS : 0.04407038167119026 [SCORE] : 1.0\n",
      "[192/1000]\n",
      "- [TRAIN] LOSS : 0.06307678533097108 [SCORE] : 0.5755908330281575\n",
      "[192/1000]\n",
      "- [VAL] LOSS : 0.04385867342352867 [SCORE] : 1.0\n",
      "[193/1000]\n",
      "- [TRAIN] LOSS : 0.06294558259348075 [SCORE] : 0.5755908330281575\n",
      "[193/1000]\n",
      "- [VAL] LOSS : 0.043650854378938675 [SCORE] : 1.0\n",
      "[194/1000]\n",
      "- [TRAIN] LOSS : 0.06281699078778426 [SCORE] : 0.5755908330281575\n",
      "[194/1000]\n",
      "- [VAL] LOSS : 0.04344680532813072 [SCORE] : 1.0\n",
      "[195/1000]\n",
      "- [TRAIN] LOSS : 0.06269126608967782 [SCORE] : 0.5755908330281575\n",
      "[195/1000]\n",
      "- [VAL] LOSS : 0.04324653372168541 [SCORE] : 1.0\n",
      "[196/1000]\n",
      "- [TRAIN] LOSS : 0.0625675222525994 [SCORE] : 0.5755908330281575\n",
      "[196/1000]\n",
      "- [VAL] LOSS : 0.043050073087215424 [SCORE] : 1.0\n",
      "[197/1000]\n",
      "- [TRAIN] LOSS : 0.062446757405996325 [SCORE] : 0.5755908330281575\n",
      "[197/1000]\n",
      "- [VAL] LOSS : 0.04285725951194763 [SCORE] : 1.0\n",
      "[198/1000]\n",
      "- [TRAIN] LOSS : 0.062328700348734854 [SCORE] : 0.5755908330281575\n",
      "[198/1000]\n",
      "- [VAL] LOSS : 0.04266785830259323 [SCORE] : 1.0\n",
      "[199/1000]\n",
      "- [TRAIN] LOSS : 0.06221294564505418 [SCORE] : 0.5755908330281575\n",
      "[199/1000]\n",
      "- [VAL] LOSS : 0.04248178005218506 [SCORE] : 1.0\n",
      "[200/1000]\n",
      "- [TRAIN] LOSS : 0.06209943369030953 [SCORE] : 0.5755908330281575\n",
      "[200/1000]\n",
      "- [VAL] LOSS : 0.04229910299181938 [SCORE] : 1.0\n",
      "[201/1000]\n",
      "- [TRAIN] LOSS : 0.061988199253877004 [SCORE] : 0.5755908330281575\n",
      "[201/1000]\n",
      "- [VAL] LOSS : 0.04212120920419693 [SCORE] : 1.0\n",
      "[202/1000]\n",
      "- [TRAIN] LOSS : 0.0618792528907458 [SCORE] : 0.5755908330281575\n",
      "[202/1000]\n",
      "- [VAL] LOSS : 0.04194670170545578 [SCORE] : 1.0\n",
      "[203/1000]\n",
      "- [TRAIN] LOSS : 0.061772474894920985 [SCORE] : 0.5755908330281575\n",
      "[203/1000]\n",
      "- [VAL] LOSS : 0.041775189340114594 [SCORE] : 1.0\n",
      "[204/1000]\n",
      "- [TRAIN] LOSS : 0.06166779734194279 [SCORE] : 0.5755908330281575\n",
      "[204/1000]\n",
      "- [VAL] LOSS : 0.041606638580560684 [SCORE] : 1.0\n",
      "[205/1000]\n",
      "- [TRAIN] LOSS : 0.06156513405342897 [SCORE] : 0.5755908330281575\n",
      "[205/1000]\n",
      "- [VAL] LOSS : 0.0414409376680851 [SCORE] : 1.0\n",
      "[206/1000]\n",
      "- [TRAIN] LOSS : 0.06146445112923781 [SCORE] : 0.5755908330281575\n",
      "[206/1000]\n",
      "- [VAL] LOSS : 0.04127807542681694 [SCORE] : 1.0\n",
      "[207/1000]\n",
      "- [TRAIN] LOSS : 0.06136575688918432 [SCORE] : 0.5755908330281575\n",
      "[207/1000]\n",
      "- [VAL] LOSS : 0.04111794754862785 [SCORE] : 1.0\n",
      "[208/1000]\n",
      "- [TRAIN] LOSS : 0.06126891411840916 [SCORE] : 0.5755908330281575\n",
      "[208/1000]\n",
      "- [VAL] LOSS : 0.040960509330034256 [SCORE] : 1.0\n",
      "[209/1000]\n",
      "- [TRAIN] LOSS : 0.0611739316334327 [SCORE] : 0.5755908330281575\n",
      "[209/1000]\n",
      "- [VAL] LOSS : 0.0408056378364563 [SCORE] : 1.0\n",
      "[210/1000]\n",
      "- [TRAIN] LOSS : 0.06108072983721892 [SCORE] : 0.5755908330281575\n",
      "[210/1000]\n",
      "- [VAL] LOSS : 0.040653347969055176 [SCORE] : 1.0\n",
      "[211/1000]\n",
      "- [TRAIN] LOSS : 0.0609892588108778 [SCORE] : 0.5755908330281575\n",
      "[211/1000]\n",
      "- [VAL] LOSS : 0.04050353169441223 [SCORE] : 1.0\n",
      "[212/1000]\n",
      "- [TRAIN] LOSS : 0.06089951321482658 [SCORE] : 0.5755908330281575\n",
      "[212/1000]\n",
      "- [VAL] LOSS : 0.04035614803433418 [SCORE] : 1.0\n",
      "[213/1000]\n",
      "- [TRAIN] LOSS : 0.06081138377388318 [SCORE] : 0.5755908330281575\n",
      "[213/1000]\n",
      "- [VAL] LOSS : 0.04021117091178894 [SCORE] : 1.0\n",
      "[214/1000]\n",
      "- [TRAIN] LOSS : 0.06072488234688838 [SCORE] : 0.5755908330281575\n",
      "[214/1000]\n",
      "- [VAL] LOSS : 0.040068451315164566 [SCORE] : 1.0\n",
      "[215/1000]\n",
      "- [TRAIN] LOSS : 0.060639991176625094 [SCORE] : 0.5755908330281575\n",
      "[215/1000]\n",
      "- [VAL] LOSS : 0.03992805257439613 [SCORE] : 1.0\n",
      "[216/1000]\n",
      "- [TRAIN] LOSS : 0.06055660030494134 [SCORE] : 0.5755908330281575\n",
      "[216/1000]\n",
      "- [VAL] LOSS : 0.039789799600839615 [SCORE] : 1.0\n",
      "[217/1000]\n",
      "- [TRAIN] LOSS : 0.060474683965245885 [SCORE] : 0.5755908330281575\n",
      "[217/1000]\n",
      "- [VAL] LOSS : 0.03965381532907486 [SCORE] : 1.0\n",
      "[218/1000]\n",
      "- [TRAIN] LOSS : 0.06039426103234291 [SCORE] : 0.5755908330281575\n",
      "[218/1000]\n",
      "- [VAL] LOSS : 0.03951988369226456 [SCORE] : 1.0\n",
      "[219/1000]\n",
      "- [TRAIN] LOSS : 0.060315206398566565 [SCORE] : 0.5755908330281575\n",
      "[219/1000]\n",
      "- [VAL] LOSS : 0.03938807174563408 [SCORE] : 1.0\n",
      "[220/1000]\n",
      "- [TRAIN] LOSS : 0.060237606925268965 [SCORE] : 0.5755908330281575\n",
      "[220/1000]\n",
      "- [VAL] LOSS : 0.039258260279893875 [SCORE] : 1.0\n",
      "[221/1000]\n",
      "- [TRAIN] LOSS : 0.0601613155255715 [SCORE] : 0.5755908330281575\n",
      "[221/1000]\n",
      "- [VAL] LOSS : 0.039130426943302155 [SCORE] : 1.0\n",
      "[222/1000]\n",
      "- [TRAIN] LOSS : 0.06008637696504593 [SCORE] : 0.5755908330281575\n",
      "[222/1000]\n",
      "- [VAL] LOSS : 0.03900453820824623 [SCORE] : 1.0\n",
      "[223/1000]\n",
      "- [TRAIN] LOSS : 0.060012734060486156 [SCORE] : 0.5755908330281575\n",
      "[223/1000]\n",
      "- [VAL] LOSS : 0.03888052701950073 [SCORE] : 1.0\n",
      "[224/1000]\n",
      "- [TRAIN] LOSS : 0.05994027834385633 [SCORE] : 0.5755908330281575\n",
      "[224/1000]\n",
      "- [VAL] LOSS : 0.038758400827646255 [SCORE] : 1.0\n",
      "[225/1000]\n",
      "- [TRAIN] LOSS : 0.0598691235606869 [SCORE] : 0.5755908330281575\n",
      "[225/1000]\n",
      "- [VAL] LOSS : 0.03863810747861862 [SCORE] : 1.0\n",
      "[226/1000]\n",
      "- [TRAIN] LOSS : 0.05979911871254444 [SCORE] : 0.5755908330281575\n",
      "[226/1000]\n",
      "- [VAL] LOSS : 0.03851957246661186 [SCORE] : 1.0\n",
      "[227/1000]\n",
      "- [TRAIN] LOSS : 0.05973031017929316 [SCORE] : 0.5755908330281575\n",
      "[227/1000]\n",
      "- [VAL] LOSS : 0.038402803242206573 [SCORE] : 1.0\n",
      "[228/1000]\n",
      "- [TRAIN] LOSS : 0.059662637983759245 [SCORE] : 0.5755908330281575\n",
      "[228/1000]\n",
      "- [VAL] LOSS : 0.038287680596113205 [SCORE] : 1.0\n",
      "[229/1000]\n",
      "- [TRAIN] LOSS : 0.059596085983018084 [SCORE] : 0.5755908330281575\n",
      "[229/1000]\n",
      "- [VAL] LOSS : 0.038174282759428024 [SCORE] : 1.0\n",
      "[230/1000]\n",
      "- [TRAIN] LOSS : 0.05953063530226548 [SCORE] : 0.5755908330281575\n",
      "[230/1000]\n",
      "- [VAL] LOSS : 0.038062553852796555 [SCORE] : 1.0\n",
      "[231/1000]\n",
      "- [TRAIN] LOSS : 0.059466250737508136 [SCORE] : 0.5755908330281575\n",
      "[231/1000]\n",
      "- [VAL] LOSS : 0.037952352315187454 [SCORE] : 1.0\n",
      "[232/1000]\n",
      "- [TRAIN] LOSS : 0.05940287827203671 [SCORE] : 0.5755908330281575\n",
      "[232/1000]\n",
      "- [VAL] LOSS : 0.03784378618001938 [SCORE] : 1.0\n",
      "[233/1000]\n",
      "- [TRAIN] LOSS : 0.05934055205434561 [SCORE] : 0.5755908330281575\n",
      "[233/1000]\n",
      "- [VAL] LOSS : 0.03773672133684158 [SCORE] : 1.0\n",
      "[234/1000]\n",
      "- [TRAIN] LOSS : 0.0592792114863793 [SCORE] : 0.5755908330281575\n",
      "[234/1000]\n",
      "- [VAL] LOSS : 0.037631191313266754 [SCORE] : 1.0\n",
      "[235/1000]\n",
      "- [TRAIN] LOSS : 0.05921884396423896 [SCORE] : 0.5755908330281575\n",
      "[235/1000]\n",
      "- [VAL] LOSS : 0.037527114152908325 [SCORE] : 1.0\n",
      "[236/1000]\n",
      "- [TRAIN] LOSS : 0.059159416581193607 [SCORE] : 0.5755908330281575\n",
      "[236/1000]\n",
      "- [VAL] LOSS : 0.0374244824051857 [SCORE] : 1.0\n",
      "[237/1000]\n",
      "- [TRAIN] LOSS : 0.059100947653253876 [SCORE] : 0.5755908330281575\n",
      "[237/1000]\n",
      "- [VAL] LOSS : 0.03732328861951828 [SCORE] : 1.0\n",
      "[238/1000]\n",
      "- [TRAIN] LOSS : 0.059043586378296216 [SCORE] : 0.5755908330281575\n",
      "[238/1000]\n",
      "- [VAL] LOSS : 0.037223368883132935 [SCORE] : 1.0\n",
      "[239/1000]\n",
      "- [TRAIN] LOSS : 0.058986745215952395 [SCORE] : 0.5755908330281575\n",
      "[239/1000]\n",
      "- [VAL] LOSS : 0.03712483122944832 [SCORE] : 1.0\n",
      "[240/1000]\n",
      "- [TRAIN] LOSS : 0.05893080892662207 [SCORE] : 0.5755908330281575\n",
      "[240/1000]\n",
      "- [VAL] LOSS : 0.03702782467007637 [SCORE] : 1.0\n",
      "[241/1000]\n",
      "- [TRAIN] LOSS : 0.058875866172214346 [SCORE] : 0.5755908330281575\n",
      "[241/1000]\n",
      "- [VAL] LOSS : 0.03693205490708351 [SCORE] : 1.0\n",
      "[242/1000]\n",
      "- [TRAIN] LOSS : 0.05882184331615766 [SCORE] : 0.5755908330281575\n",
      "[242/1000]\n",
      "- [VAL] LOSS : 0.03683757036924362 [SCORE] : 1.0\n",
      "[243/1000]\n",
      "- [TRAIN] LOSS : 0.058768519076208274 [SCORE] : 0.5755908330281575\n",
      "[243/1000]\n",
      "- [VAL] LOSS : 0.03674433007836342 [SCORE] : 1.0\n",
      "[244/1000]\n",
      "- [TRAIN] LOSS : 0.05871601744244496 [SCORE] : 0.5755908330281575\n",
      "[244/1000]\n",
      "- [VAL] LOSS : 0.03665236011147499 [SCORE] : 1.0\n",
      "[245/1000]\n",
      "- [TRAIN] LOSS : 0.05866427800307671 [SCORE] : 0.5755908330281575\n",
      "[245/1000]\n",
      "- [VAL] LOSS : 0.036561597138643265 [SCORE] : 1.0\n",
      "[246/1000]\n",
      "- [TRAIN] LOSS : 0.0586133257796367 [SCORE] : 0.5755908330281575\n",
      "[246/1000]\n",
      "- [VAL] LOSS : 0.03647202253341675 [SCORE] : 1.0\n",
      "[247/1000]\n",
      "- [TRAIN] LOSS : 0.05856313084562619 [SCORE] : 0.5755908330281575\n",
      "[247/1000]\n",
      "- [VAL] LOSS : 0.03638368844985962 [SCORE] : 1.0\n",
      "[248/1000]\n",
      "- [TRAIN] LOSS : 0.05851367029050986 [SCORE] : 0.5755908330281575\n",
      "[248/1000]\n",
      "- [VAL] LOSS : 0.03629642724990845 [SCORE] : 1.0\n",
      "[249/1000]\n",
      "- [TRAIN] LOSS : 0.058464910524586834 [SCORE] : 0.5755908330281575\n",
      "[249/1000]\n",
      "- [VAL] LOSS : 0.03621035814285278 [SCORE] : 1.0\n",
      "[250/1000]\n",
      "- [TRAIN] LOSS : 0.05841687110563119 [SCORE] : 0.5755908330281575\n",
      "[250/1000]\n",
      "- [VAL] LOSS : 0.03612539544701576 [SCORE] : 1.0\n",
      "[251/1000]\n",
      "- [TRAIN] LOSS : 0.058369501307606694 [SCORE] : 0.5755908330281575\n",
      "[251/1000]\n",
      "- [VAL] LOSS : 0.036041509360075 [SCORE] : 1.0\n",
      "[252/1000]\n",
      "- [TRAIN] LOSS : 0.05832284229497115 [SCORE] : 0.5755908330281575\n",
      "[252/1000]\n",
      "- [VAL] LOSS : 0.0359586738049984 [SCORE] : 1.0\n",
      "[253/1000]\n",
      "- [TRAIN] LOSS : 0.0582768149053057 [SCORE] : 0.5755908330281575\n",
      "[253/1000]\n",
      "- [VAL] LOSS : 0.03587697073817253 [SCORE] : 1.0\n",
      "[254/1000]\n",
      "- [TRAIN] LOSS : 0.05823145005851984 [SCORE] : 0.5755908330281575\n",
      "[254/1000]\n",
      "- [VAL] LOSS : 0.035796213895082474 [SCORE] : 1.0\n",
      "[255/1000]\n",
      "- [TRAIN] LOSS : 0.058186684859295686 [SCORE] : 0.5755908330281575\n",
      "[255/1000]\n",
      "- [VAL] LOSS : 0.035716522485017776 [SCORE] : 1.0\n",
      "[256/1000]\n",
      "- [TRAIN] LOSS : 0.058142570033669475 [SCORE] : 0.5755908330281575\n",
      "[256/1000]\n",
      "- [VAL] LOSS : 0.03563786298036575 [SCORE] : 1.0\n",
      "[257/1000]\n",
      "- [TRAIN] LOSS : 0.05809907161941131 [SCORE] : 0.5755908330281575\n",
      "[257/1000]\n",
      "- [VAL] LOSS : 0.035560090094804764 [SCORE] : 1.0\n",
      "[258/1000]\n",
      "- [TRAIN] LOSS : 0.05805618098626534 [SCORE] : 0.5755908330281575\n",
      "[258/1000]\n",
      "- [VAL] LOSS : 0.03548336401581764 [SCORE] : 1.0\n",
      "[259/1000]\n",
      "- [TRAIN] LOSS : 0.05801385411371787 [SCORE] : 0.5755908330281575\n",
      "[259/1000]\n",
      "- [VAL] LOSS : 0.03540753945708275 [SCORE] : 1.0\n",
      "[260/1000]\n",
      "- [TRAIN] LOSS : 0.057972087897360323 [SCORE] : 0.5755908330281575\n",
      "[260/1000]\n",
      "- [VAL] LOSS : 0.035332657396793365 [SCORE] : 1.0\n",
      "[261/1000]\n",
      "- [TRAIN] LOSS : 0.05793090630322695 [SCORE] : 0.5755908330281575\n",
      "[261/1000]\n",
      "- [VAL] LOSS : 0.03525871783494949 [SCORE] : 1.0\n",
      "[262/1000]\n",
      "- [TRAIN] LOSS : 0.05789029188454151 [SCORE] : 0.5755908330281575\n",
      "[262/1000]\n",
      "- [VAL] LOSS : 0.03518567606806755 [SCORE] : 1.0\n",
      "[263/1000]\n",
      "- [TRAIN] LOSS : 0.057850229429701965 [SCORE] : 0.5755908330281575\n",
      "[263/1000]\n",
      "- [VAL] LOSS : 0.03511347994208336 [SCORE] : 1.0\n",
      "[264/1000]\n",
      "- [TRAIN] LOSS : 0.057810657657682896 [SCORE] : 0.5755908330281575\n",
      "[264/1000]\n",
      "- [VAL] LOSS : 0.035042185336351395 [SCORE] : 1.0\n",
      "[265/1000]\n",
      "- [TRAIN] LOSS : 0.05777161214500666 [SCORE] : 0.5755908330281575\n",
      "[265/1000]\n",
      "- [VAL] LOSS : 0.03497174009680748 [SCORE] : 1.0\n",
      "[266/1000]\n",
      "- [TRAIN] LOSS : 0.057733152496318024 [SCORE] : 0.5755908330281575\n",
      "[266/1000]\n",
      "- [VAL] LOSS : 0.03490215167403221 [SCORE] : 1.0\n",
      "[267/1000]\n",
      "- [TRAIN] LOSS : 0.057695071150859194 [SCORE] : 0.5755908330281575\n",
      "[267/1000]\n",
      "- [VAL] LOSS : 0.034833379089832306 [SCORE] : 1.0\n",
      "[268/1000]\n",
      "- [TRAIN] LOSS : 0.05765758237491051 [SCORE] : 0.5755908330281575\n",
      "[268/1000]\n",
      "- [VAL] LOSS : 0.034765444695949554 [SCORE] : 1.0\n",
      "[269/1000]\n",
      "- [TRAIN] LOSS : 0.057620557770133016 [SCORE] : 0.5755908330281575\n",
      "[269/1000]\n",
      "- [VAL] LOSS : 0.03469829261302948 [SCORE] : 1.0\n",
      "[270/1000]\n",
      "- [TRAIN] LOSS : 0.05758396685123444 [SCORE] : 0.5755908330281575\n",
      "[270/1000]\n",
      "- [VAL] LOSS : 0.03463194519281387 [SCORE] : 1.0\n",
      "[271/1000]\n",
      "- [TRAIN] LOSS : 0.057547869409124056 [SCORE] : 0.5755908330281575\n",
      "[271/1000]\n",
      "- [VAL] LOSS : 0.03456633538007736 [SCORE] : 1.0\n",
      "[272/1000]\n",
      "- [TRAIN] LOSS : 0.05751223762830098 [SCORE] : 0.5755908330281575\n",
      "[272/1000]\n",
      "- [VAL] LOSS : 0.034501515328884125 [SCORE] : 1.0\n",
      "[273/1000]\n",
      "- [TRAIN] LOSS : 0.05747704481085141 [SCORE] : 0.5755908330281575\n",
      "[273/1000]\n",
      "- [VAL] LOSS : 0.034437473863363266 [SCORE] : 1.0\n",
      "[274/1000]\n",
      "- [TRAIN] LOSS : 0.05744229797273874 [SCORE] : 0.5755908330281575\n",
      "[274/1000]\n",
      "- [VAL] LOSS : 0.03437409922480583 [SCORE] : 1.0\n",
      "[275/1000]\n",
      "- [TRAIN] LOSS : 0.05740796240667502 [SCORE] : 0.5755908330281575\n",
      "[275/1000]\n",
      "- [VAL] LOSS : 0.03431151807308197 [SCORE] : 1.0\n",
      "[276/1000]\n",
      "- [TRAIN] LOSS : 0.05737407834579547 [SCORE] : 0.5755908330281575\n",
      "[276/1000]\n",
      "- [VAL] LOSS : 0.03424963727593422 [SCORE] : 1.0\n",
      "[277/1000]\n",
      "- [TRAIN] LOSS : 0.05734062405924002 [SCORE] : 0.5755908330281575\n",
      "[277/1000]\n",
      "- [VAL] LOSS : 0.034188468009233475 [SCORE] : 1.0\n",
      "[278/1000]\n",
      "- [TRAIN] LOSS : 0.057307543543477855 [SCORE] : 0.5755908330281575\n",
      "[278/1000]\n",
      "- [VAL] LOSS : 0.03412798419594765 [SCORE] : 1.0\n",
      "[279/1000]\n",
      "- [TRAIN] LOSS : 0.05727488448222478 [SCORE] : 0.5755908330281575\n",
      "[279/1000]\n",
      "- [VAL] LOSS : 0.034068215638399124 [SCORE] : 1.0\n",
      "[280/1000]\n",
      "- [TRAIN] LOSS : 0.0572426101192832 [SCORE] : 0.5755908330281575\n",
      "[280/1000]\n",
      "- [VAL] LOSS : 0.03400907292962074 [SCORE] : 1.0\n",
      "[281/1000]\n",
      "- [TRAIN] LOSS : 0.05721074230968952 [SCORE] : 0.5755908330281575\n",
      "[281/1000]\n",
      "- [VAL] LOSS : 0.03395060822367668 [SCORE] : 1.0\n",
      "[282/1000]\n",
      "- [TRAIN] LOSS : 0.05717922498782476 [SCORE] : 0.5755908330281575\n",
      "[282/1000]\n",
      "- [VAL] LOSS : 0.03389279916882515 [SCORE] : 1.0\n",
      "[283/1000]\n",
      "- [TRAIN] LOSS : 0.05714813657104969 [SCORE] : 0.5755908330281575\n",
      "[283/1000]\n",
      "- [VAL] LOSS : 0.03383561223745346 [SCORE] : 1.0\n",
      "[284/1000]\n",
      "- [TRAIN] LOSS : 0.057117371695737044 [SCORE] : 0.5755908330281575\n",
      "[284/1000]\n",
      "- [VAL] LOSS : 0.03377910703420639 [SCORE] : 1.0\n",
      "[285/1000]\n",
      "- [TRAIN] LOSS : 0.05708699679623048 [SCORE] : 0.5755908330281575\n",
      "[285/1000]\n",
      "- [VAL] LOSS : 0.03372323140501976 [SCORE] : 1.0\n",
      "[286/1000]\n",
      "- [TRAIN] LOSS : 0.05705695313711961 [SCORE] : 0.5755908330281575\n",
      "[286/1000]\n",
      "- [VAL] LOSS : 0.03366795554757118 [SCORE] : 1.0\n",
      "[287/1000]\n",
      "- [TRAIN] LOSS : 0.057027266298731165 [SCORE] : 0.5755908330281575\n",
      "[287/1000]\n",
      "- [VAL] LOSS : 0.033613283187150955 [SCORE] : 1.0\n",
      "[288/1000]\n",
      "- [TRAIN] LOSS : 0.05699790976941586 [SCORE] : 0.5755908330281575\n",
      "[288/1000]\n",
      "- [VAL] LOSS : 0.0335591696202755 [SCORE] : 1.0\n",
      "[289/1000]\n",
      "- [TRAIN] LOSS : 0.05696892763177554 [SCORE] : 0.5755908330281575\n",
      "[289/1000]\n",
      "- [VAL] LOSS : 0.03350565955042839 [SCORE] : 1.0\n",
      "[290/1000]\n",
      "- [TRAIN] LOSS : 0.05694025332729022 [SCORE] : 0.5755908330281575\n",
      "[290/1000]\n",
      "- [VAL] LOSS : 0.03345276787877083 [SCORE] : 1.0\n",
      "[291/1000]\n",
      "- [TRAIN] LOSS : 0.05691191758960486 [SCORE] : 0.5755908330281575\n",
      "[291/1000]\n",
      "- [VAL] LOSS : 0.033400435000658035 [SCORE] : 1.0\n",
      "[292/1000]\n",
      "- [TRAIN] LOSS : 0.056883901730179785 [SCORE] : 0.5755908330281575\n",
      "[292/1000]\n",
      "- [VAL] LOSS : 0.03334864228963852 [SCORE] : 1.0\n",
      "[293/1000]\n",
      "- [TRAIN] LOSS : 0.0568562259276708 [SCORE] : 0.5755908330281575\n",
      "[293/1000]\n",
      "- [VAL] LOSS : 0.033297400921583176 [SCORE] : 1.0\n",
      "[294/1000]\n",
      "- [TRAIN] LOSS : 0.0568288017064333 [SCORE] : 0.5755908330281575\n",
      "[294/1000]\n",
      "- [VAL] LOSS : 0.033246733248233795 [SCORE] : 1.0\n",
      "[295/1000]\n",
      "- [TRAIN] LOSS : 0.056801691216727096 [SCORE] : 0.5755908330281575\n",
      "[295/1000]\n",
      "- [VAL] LOSS : 0.03319661691784859 [SCORE] : 1.0\n",
      "[296/1000]\n",
      "- [TRAIN] LOSS : 0.056774933077394965 [SCORE] : 0.5755908330281575\n",
      "[296/1000]\n",
      "- [VAL] LOSS : 0.033146992325782776 [SCORE] : 1.0\n",
      "[297/1000]\n",
      "- [TRAIN] LOSS : 0.05674846917390823 [SCORE] : 0.5755908330281575\n",
      "[297/1000]\n",
      "- [VAL] LOSS : 0.03309791535139084 [SCORE] : 1.0\n",
      "[298/1000]\n",
      "- [TRAIN] LOSS : 0.05672223245104154 [SCORE] : 0.5755908330281575\n",
      "[298/1000]\n",
      "- [VAL] LOSS : 0.0330493338406086 [SCORE] : 1.0\n",
      "[299/1000]\n",
      "- [TRAIN] LOSS : 0.056696337275207044 [SCORE] : 0.5755908330281575\n",
      "[299/1000]\n",
      "- [VAL] LOSS : 0.03300124779343605 [SCORE] : 1.0\n",
      "[300/1000]\n",
      "- [TRAIN] LOSS : 0.05667071212083101 [SCORE] : 0.5755908330281575\n",
      "[300/1000]\n",
      "- [VAL] LOSS : 0.03295368328690529 [SCORE] : 1.0\n",
      "[301/1000]\n",
      "- [TRAIN] LOSS : 0.05664536068215966 [SCORE] : 0.5755908330281575\n",
      "[301/1000]\n",
      "- [VAL] LOSS : 0.032906632870435715 [SCORE] : 1.0\n",
      "[302/1000]\n",
      "- [TRAIN] LOSS : 0.05662027771274249 [SCORE] : 0.5755908330281575\n",
      "[302/1000]\n",
      "- [VAL] LOSS : 0.03286009281873703 [SCORE] : 1.0\n",
      "[303/1000]\n",
      "- [TRAIN] LOSS : 0.05659544769053658 [SCORE] : 0.5755908330281575\n",
      "[303/1000]\n",
      "- [VAL] LOSS : 0.03281394764780998 [SCORE] : 1.0\n",
      "[304/1000]\n",
      "- [TRAIN] LOSS : 0.056570888962596655 [SCORE] : 0.5755908330281575\n",
      "[304/1000]\n",
      "- [VAL] LOSS : 0.03276834636926651 [SCORE] : 1.0\n",
      "[305/1000]\n",
      "- [TRAIN] LOSS : 0.05654661621277531 [SCORE] : 0.5755908330281575\n",
      "[305/1000]\n",
      "- [VAL] LOSS : 0.032723214477300644 [SCORE] : 1.0\n",
      "[306/1000]\n",
      "- [TRAIN] LOSS : 0.05652258212988575 [SCORE] : 0.5755908330281575\n",
      "[306/1000]\n",
      "- [VAL] LOSS : 0.03267848119139671 [SCORE] : 1.0\n",
      "[307/1000]\n",
      "- [TRAIN] LOSS : 0.05649879481643438 [SCORE] : 0.5755908330281575\n",
      "[307/1000]\n",
      "- [VAL] LOSS : 0.032634276896715164 [SCORE] : 1.0\n",
      "[308/1000]\n",
      "- [TRAIN] LOSS : 0.056475254241377115 [SCORE] : 0.5755908330281575\n",
      "[308/1000]\n",
      "- [VAL] LOSS : 0.032590482383966446 [SCORE] : 1.0\n",
      "[309/1000]\n",
      "- [TRAIN] LOSS : 0.05645195565496882 [SCORE] : 0.5755908330281575\n",
      "[309/1000]\n",
      "- [VAL] LOSS : 0.03254718333482742 [SCORE] : 1.0\n",
      "[310/1000]\n",
      "- [TRAIN] LOSS : 0.05642891625563304 [SCORE] : 0.5755908330281575\n",
      "[310/1000]\n",
      "- [VAL] LOSS : 0.032504260540008545 [SCORE] : 1.0\n",
      "[311/1000]\n",
      "- [TRAIN] LOSS : 0.05640612049028278 [SCORE] : 0.5755908330281575\n",
      "[311/1000]\n",
      "- [VAL] LOSS : 0.03246177360415459 [SCORE] : 1.0\n",
      "[312/1000]\n",
      "- [TRAIN] LOSS : 0.05638354830443859 [SCORE] : 0.5755908330281575\n",
      "[312/1000]\n",
      "- [VAL] LOSS : 0.03241976723074913 [SCORE] : 1.0\n",
      "[313/1000]\n",
      "- [TRAIN] LOSS : 0.056361161669095355 [SCORE] : 0.5755908330281575\n",
      "[313/1000]\n",
      "- [VAL] LOSS : 0.03237811475992203 [SCORE] : 1.0\n",
      "[314/1000]\n",
      "- [TRAIN] LOSS : 0.056339047849178314 [SCORE] : 0.5755908330281575\n",
      "[314/1000]\n",
      "- [VAL] LOSS : 0.03233688697218895 [SCORE] : 1.0\n",
      "[315/1000]\n",
      "- [TRAIN] LOSS : 0.05631714543948571 [SCORE] : 0.5755908330281575\n",
      "[315/1000]\n",
      "- [VAL] LOSS : 0.032296087592840195 [SCORE] : 1.0\n",
      "[316/1000]\n",
      "- [TRAIN] LOSS : 0.05629545844470461 [SCORE] : 0.5755908330281575\n",
      "[316/1000]\n",
      "- [VAL] LOSS : 0.032255686819553375 [SCORE] : 1.0\n",
      "[317/1000]\n",
      "- [TRAIN] LOSS : 0.056273673009127376 [SCORE] : 0.5755908330281575\n",
      "[317/1000]\n",
      "- [VAL] LOSS : 0.03221583738923073 [SCORE] : 1.0\n",
      "[318/1000]\n",
      "- [TRAIN] LOSS : 0.056252490791181727 [SCORE] : 0.5755908330281575\n",
      "[318/1000]\n",
      "- [VAL] LOSS : 0.032176416367292404 [SCORE] : 1.0\n",
      "[319/1000]\n",
      "- [TRAIN] LOSS : 0.05623168284073472 [SCORE] : 0.5755908330281575\n",
      "[319/1000]\n",
      "- [VAL] LOSS : 0.032137252390384674 [SCORE] : 1.0\n",
      "[320/1000]\n",
      "- [TRAIN] LOSS : 0.056211209204047916 [SCORE] : 0.5755908330281575\n",
      "[320/1000]\n",
      "- [VAL] LOSS : 0.03209826722741127 [SCORE] : 1.0\n",
      "[321/1000]\n",
      "- [TRAIN] LOSS : 0.056190154918779926 [SCORE] : 0.5755908330281575\n",
      "[321/1000]\n",
      "- [VAL] LOSS : 0.03205975145101547 [SCORE] : 1.0\n",
      "[322/1000]\n",
      "- [TRAIN] LOSS : 0.05616953102871776 [SCORE] : 0.5755908330281575\n",
      "[322/1000]\n",
      "- [VAL] LOSS : 0.0320218987762928 [SCORE] : 1.0\n",
      "[323/1000]\n",
      "- [TRAIN] LOSS : 0.05614942417790492 [SCORE] : 0.5755908330281575\n",
      "[323/1000]\n",
      "- [VAL] LOSS : 0.03198428452014923 [SCORE] : 1.0\n",
      "[324/1000]\n",
      "- [TRAIN] LOSS : 0.056129513401538135 [SCORE] : 0.5755908330281575\n",
      "[324/1000]\n",
      "- [VAL] LOSS : 0.03194696456193924 [SCORE] : 1.0\n",
      "[325/1000]\n",
      "- [TRAIN] LOSS : 0.056109970745941005 [SCORE] : 0.5755908330281575\n",
      "[325/1000]\n",
      "- [VAL] LOSS : 0.031909797340631485 [SCORE] : 1.0\n",
      "[326/1000]\n",
      "- [TRAIN] LOSS : 0.056089908722788094 [SCORE] : 0.5755908330281575\n",
      "[326/1000]\n",
      "- [VAL] LOSS : 0.031873174011707306 [SCORE] : 1.0\n",
      "[327/1000]\n",
      "- [TRAIN] LOSS : 0.05607025049005945 [SCORE] : 0.5755908330281575\n",
      "[327/1000]\n",
      "- [VAL] LOSS : 0.03183706849813461 [SCORE] : 1.0\n",
      "[328/1000]\n",
      "- [TRAIN] LOSS : 0.05605111895129085 [SCORE] : 0.5755908330281575\n",
      "[328/1000]\n",
      "- [VAL] LOSS : 0.03180120140314102 [SCORE] : 1.0\n",
      "[329/1000]\n",
      "- [TRAIN] LOSS : 0.05603210907429457 [SCORE] : 0.5755908330281575\n",
      "[329/1000]\n",
      "- [VAL] LOSS : 0.03176562488079071 [SCORE] : 1.0\n",
      "[330/1000]\n",
      "- [TRAIN] LOSS : 0.05601320378482342 [SCORE] : 0.5755908330281575\n",
      "[330/1000]\n",
      "- [VAL] LOSS : 0.03173033148050308 [SCORE] : 1.0\n",
      "[331/1000]\n",
      "- [TRAIN] LOSS : 0.05599438728143771 [SCORE] : 0.5755908330281575\n",
      "[331/1000]\n",
      "- [VAL] LOSS : 0.03169542923569679 [SCORE] : 1.0\n",
      "[332/1000]\n",
      "- [TRAIN] LOSS : 0.05597577899073561 [SCORE] : 0.5755908330281575\n",
      "[332/1000]\n",
      "- [VAL] LOSS : 0.03166084364056587 [SCORE] : 1.0\n",
      "[333/1000]\n",
      "- [TRAIN] LOSS : 0.05595737254867951 [SCORE] : 0.5755908330281575\n",
      "[333/1000]\n",
      "- [VAL] LOSS : 0.0316266305744648 [SCORE] : 1.0\n",
      "[334/1000]\n",
      "- [TRAIN] LOSS : 0.0559391309817632 [SCORE] : 0.5755908330281575\n",
      "[334/1000]\n",
      "- [VAL] LOSS : 0.03159268572926521 [SCORE] : 1.0\n",
      "[335/1000]\n",
      "- [TRAIN] LOSS : 0.055921060188362993 [SCORE] : 0.5755908330281575\n",
      "[335/1000]\n",
      "- [VAL] LOSS : 0.03155910223722458 [SCORE] : 1.0\n",
      "[336/1000]\n",
      "- [TRAIN] LOSS : 0.055903149551401535 [SCORE] : 0.5755908330281575\n",
      "[336/1000]\n",
      "- [VAL] LOSS : 0.03152576461434364 [SCORE] : 1.0\n",
      "[337/1000]\n",
      "- [TRAIN] LOSS : 0.055885368337233864 [SCORE] : 0.5755908330281575\n",
      "[337/1000]\n",
      "- [VAL] LOSS : 0.03149280324578285 [SCORE] : 1.0\n",
      "[338/1000]\n",
      "- [TRAIN] LOSS : 0.05586779797449708 [SCORE] : 0.5755908330281575\n",
      "[338/1000]\n",
      "- [VAL] LOSS : 0.03146008029580116 [SCORE] : 1.0\n",
      "[339/1000]\n",
      "- [TRAIN] LOSS : 0.05585034163668752 [SCORE] : 0.5755908330281575\n",
      "[339/1000]\n",
      "- [VAL] LOSS : 0.03142770007252693 [SCORE] : 1.0\n",
      "[340/1000]\n",
      "- [TRAIN] LOSS : 0.05583304173002641 [SCORE] : 0.5755908330281575\n",
      "[340/1000]\n",
      "- [VAL] LOSS : 0.03139560669660568 [SCORE] : 1.0\n",
      "[341/1000]\n",
      "- [TRAIN] LOSS : 0.05581592917442322 [SCORE] : 0.5755908330281575\n",
      "[341/1000]\n",
      "- [VAL] LOSS : 0.03136380761861801 [SCORE] : 1.0\n",
      "[342/1000]\n",
      "- [TRAIN] LOSS : 0.055798939988017084 [SCORE] : 0.5755908330281575\n",
      "[342/1000]\n",
      "- [VAL] LOSS : 0.03133232146501541 [SCORE] : 1.0\n",
      "[343/1000]\n",
      "- [TRAIN] LOSS : 0.05578208103155096 [SCORE] : 0.5755908330281575\n",
      "[343/1000]\n",
      "- [VAL] LOSS : 0.031301118433475494 [SCORE] : 1.0\n",
      "[344/1000]\n",
      "- [TRAIN] LOSS : 0.05576539201041063 [SCORE] : 0.5755908330281575\n",
      "[344/1000]\n",
      "- [VAL] LOSS : 0.03127017989754677 [SCORE] : 1.0\n",
      "[345/1000]\n",
      "- [TRAIN] LOSS : 0.05574885169044137 [SCORE] : 0.5755908330281575\n",
      "[345/1000]\n",
      "- [VAL] LOSS : 0.031239548698067665 [SCORE] : 1.0\n",
      "[346/1000]\n",
      "- [TRAIN] LOSS : 0.055732473948349556 [SCORE] : 0.5755908330281575\n",
      "[346/1000]\n",
      "- [VAL] LOSS : 0.031209131702780724 [SCORE] : 1.0\n",
      "[347/1000]\n",
      "- [TRAIN] LOSS : 0.05571619374677539 [SCORE] : 0.5755908330281575\n",
      "[347/1000]\n",
      "- [VAL] LOSS : 0.031179049983620644 [SCORE] : 1.0\n",
      "[348/1000]\n",
      "- [TRAIN] LOSS : 0.055700058086464804 [SCORE] : 0.5755908330281575\n",
      "[348/1000]\n",
      "- [VAL] LOSS : 0.031149258837103844 [SCORE] : 1.0\n",
      "[349/1000]\n",
      "- [TRAIN] LOSS : 0.05568407960236073 [SCORE] : 0.5755908330281575\n",
      "[349/1000]\n",
      "- [VAL] LOSS : 0.03111967444419861 [SCORE] : 1.0\n",
      "[350/1000]\n",
      "- [TRAIN] LOSS : 0.05566821691269676 [SCORE] : 0.5755908330281575\n",
      "[350/1000]\n",
      "- [VAL] LOSS : 0.03109036199748516 [SCORE] : 1.0\n",
      "[351/1000]\n",
      "- [TRAIN] LOSS : 0.055652487898866336 [SCORE] : 0.5755908330281575\n",
      "[351/1000]\n",
      "- [VAL] LOSS : 0.031061379238963127 [SCORE] : 1.0\n",
      "[352/1000]\n",
      "- [TRAIN] LOSS : 0.055636902898550035 [SCORE] : 0.5755908330281575\n",
      "[352/1000]\n",
      "- [VAL] LOSS : 0.031032590195536613 [SCORE] : 1.0\n",
      "[353/1000]\n",
      "- [TRAIN] LOSS : 0.055621466723581155 [SCORE] : 0.5755908330281575\n",
      "[353/1000]\n",
      "- [VAL] LOSS : 0.031004080548882484 [SCORE] : 1.0\n",
      "[354/1000]\n",
      "- [TRAIN] LOSS : 0.055606144852936266 [SCORE] : 0.5755908330281575\n",
      "[354/1000]\n",
      "- [VAL] LOSS : 0.030975820496678352 [SCORE] : 1.0\n",
      "[355/1000]\n",
      "- [TRAIN] LOSS : 0.05559091325849295 [SCORE] : 0.5755908330281575\n",
      "[355/1000]\n",
      "- [VAL] LOSS : 0.030947845429182053 [SCORE] : 1.0\n",
      "[356/1000]\n",
      "- [TRAIN] LOSS : 0.055575834090511006 [SCORE] : 0.5755908330281575\n",
      "[356/1000]\n",
      "- [VAL] LOSS : 0.03092007525265217 [SCORE] : 1.0\n",
      "[357/1000]\n",
      "- [TRAIN] LOSS : 0.055560883693397047 [SCORE] : 0.5755908330281575\n",
      "[357/1000]\n",
      "- [VAL] LOSS : 0.03089255280792713 [SCORE] : 1.0\n",
      "[358/1000]\n",
      "- [TRAIN] LOSS : 0.05554603670413295 [SCORE] : 0.5755908330281575\n",
      "[358/1000]\n",
      "- [VAL] LOSS : 0.030865270644426346 [SCORE] : 1.0\n",
      "[359/1000]\n",
      "- [TRAIN] LOSS : 0.0555313424517711 [SCORE] : 0.5755908330281575\n",
      "[359/1000]\n",
      "- [VAL] LOSS : 0.0308382511138916 [SCORE] : 1.0\n",
      "[360/1000]\n",
      "- [TRAIN] LOSS : 0.055516738320390384 [SCORE] : 0.5755908330281575\n",
      "[360/1000]\n",
      "- [VAL] LOSS : 0.030811447650194168 [SCORE] : 1.0\n",
      "[361/1000]\n",
      "- [TRAIN] LOSS : 0.055502246630688507 [SCORE] : 0.5755908330281575\n",
      "[361/1000]\n",
      "- [VAL] LOSS : 0.030784888193011284 [SCORE] : 1.0\n",
      "[362/1000]\n",
      "- [TRAIN] LOSS : 0.055487873839835326 [SCORE] : 0.5755908330281575\n",
      "[362/1000]\n",
      "- [VAL] LOSS : 0.030758606269955635 [SCORE] : 1.0\n",
      "[363/1000]\n",
      "- [TRAIN] LOSS : 0.05547364378968875 [SCORE] : 0.5755908330281575\n",
      "[363/1000]\n",
      "- [VAL] LOSS : 0.03073248080909252 [SCORE] : 1.0\n",
      "[364/1000]\n",
      "- [TRAIN] LOSS : 0.05545946639031172 [SCORE] : 0.5755908330281575\n",
      "[364/1000]\n",
      "- [VAL] LOSS : 0.03070664405822754 [SCORE] : 1.0\n",
      "[365/1000]\n",
      "- [TRAIN] LOSS : 0.05544548621401191 [SCORE] : 0.5755908330281575\n",
      "[365/1000]\n",
      "- [VAL] LOSS : 0.03068099543452263 [SCORE] : 1.0\n",
      "[366/1000]\n",
      "- [TRAIN] LOSS : 0.055431538137296836 [SCORE] : 0.5755908330281575\n",
      "[366/1000]\n",
      "- [VAL] LOSS : 0.030655553564429283 [SCORE] : 1.0\n",
      "[367/1000]\n",
      "- [TRAIN] LOSS : 0.05541773332903783 [SCORE] : 0.5755908330281575\n",
      "[367/1000]\n",
      "- [VAL] LOSS : 0.03063035197556019 [SCORE] : 1.0\n",
      "[368/1000]\n",
      "- [TRAIN] LOSS : 0.05540400712440411 [SCORE] : 0.5755908330281575\n",
      "[368/1000]\n",
      "- [VAL] LOSS : 0.030605392530560493 [SCORE] : 1.0\n",
      "[369/1000]\n",
      "- [TRAIN] LOSS : 0.055390456194678944 [SCORE] : 0.5755908330281575\n",
      "[369/1000]\n",
      "- [VAL] LOSS : 0.030580634251236916 [SCORE] : 1.0\n",
      "[370/1000]\n",
      "- [TRAIN] LOSS : 0.05537691917270422 [SCORE] : 0.5755908330281575\n",
      "[370/1000]\n",
      "- [VAL] LOSS : 0.030556077137589455 [SCORE] : 1.0\n",
      "[371/1000]\n",
      "- [TRAIN] LOSS : 0.05536353311811884 [SCORE] : 0.5755908330281575\n",
      "[371/1000]\n",
      "- [VAL] LOSS : 0.03053176775574684 [SCORE] : 1.0\n",
      "[372/1000]\n",
      "- [TRAIN] LOSS : 0.05535023414219419 [SCORE] : 0.5755908330281575\n",
      "[372/1000]\n",
      "- [VAL] LOSS : 0.030507629737257957 [SCORE] : 1.0\n",
      "[373/1000]\n",
      "- [TRAIN] LOSS : 0.05533707638581594 [SCORE] : 0.5755908330281575\n",
      "[373/1000]\n",
      "- [VAL] LOSS : 0.03048369660973549 [SCORE] : 1.0\n",
      "[374/1000]\n",
      "- [TRAIN] LOSS : 0.05532394461333752 [SCORE] : 0.5755908330281575\n",
      "[374/1000]\n",
      "- [VAL] LOSS : 0.030459970235824585 [SCORE] : 1.0\n",
      "[375/1000]\n",
      "- [TRAIN] LOSS : 0.05531098690504829 [SCORE] : 0.5755908330281575\n",
      "[375/1000]\n",
      "- [VAL] LOSS : 0.03043643943965435 [SCORE] : 1.0\n",
      "[376/1000]\n",
      "- [TRAIN] LOSS : 0.05529807846372326 [SCORE] : 0.5755908330281575\n",
      "[376/1000]\n",
      "- [VAL] LOSS : 0.030413176864385605 [SCORE] : 1.0\n",
      "[377/1000]\n",
      "- [TRAIN] LOSS : 0.05528527923549215 [SCORE] : 0.5755908330281575\n",
      "[377/1000]\n",
      "- [VAL] LOSS : 0.030390068888664246 [SCORE] : 1.0\n",
      "[378/1000]\n",
      "- [TRAIN] LOSS : 0.055272562491397066 [SCORE] : 0.5755908330281575\n",
      "[378/1000]\n",
      "- [VAL] LOSS : 0.030367130413651466 [SCORE] : 1.0\n",
      "[379/1000]\n",
      "- [TRAIN] LOSS : 0.05525997293492158 [SCORE] : 0.5755908330281575\n",
      "[379/1000]\n",
      "- [VAL] LOSS : 0.030344361439347267 [SCORE] : 1.0\n",
      "[380/1000]\n",
      "- [TRAIN] LOSS : 0.05524747061232726 [SCORE] : 0.5755908330281575\n",
      "[380/1000]\n",
      "- [VAL] LOSS : 0.03032185137271881 [SCORE] : 1.0\n",
      "[381/1000]\n",
      "- [TRAIN] LOSS : 0.05523503155757983 [SCORE] : 0.5755908330281575\n",
      "[381/1000]\n",
      "- [VAL] LOSS : 0.030299490317702293 [SCORE] : 1.0\n",
      "[382/1000]\n",
      "- [TRAIN] LOSS : 0.05522269125406941 [SCORE] : 0.5755908330281575\n",
      "[382/1000]\n",
      "- [VAL] LOSS : 0.030277356505393982 [SCORE] : 1.0\n",
      "[383/1000]\n",
      "- [TRAIN] LOSS : 0.05521044253061215 [SCORE] : 0.5755908330281575\n",
      "[383/1000]\n",
      "- [VAL] LOSS : 0.030255382880568504 [SCORE] : 1.0\n",
      "[384/1000]\n",
      "- [TRAIN] LOSS : 0.05519826017941038 [SCORE] : 0.5755908330281575\n",
      "[384/1000]\n",
      "- [VAL] LOSS : 0.030233602970838547 [SCORE] : 1.0\n",
      "[385/1000]\n",
      "- [TRAIN] LOSS : 0.05518620156993469 [SCORE] : 0.5755908330281575\n",
      "[385/1000]\n",
      "- [VAL] LOSS : 0.03021199442446232 [SCORE] : 1.0\n",
      "[386/1000]\n",
      "- [TRAIN] LOSS : 0.05517421656598647 [SCORE] : 0.5755908330281575\n",
      "[386/1000]\n",
      "- [VAL] LOSS : 0.030190590769052505 [SCORE] : 1.0\n",
      "[387/1000]\n",
      "- [TRAIN] LOSS : 0.05516227949410677 [SCORE] : 0.5755908330281575\n",
      "[387/1000]\n",
      "- [VAL] LOSS : 0.030169321224093437 [SCORE] : 1.0\n",
      "[388/1000]\n",
      "- [TRAIN] LOSS : 0.055150458154579 [SCORE] : 0.5755908330281575\n",
      "[388/1000]\n",
      "- [VAL] LOSS : 0.03014824166893959 [SCORE] : 1.0\n",
      "[389/1000]\n",
      "- [TRAIN] LOSS : 0.05513871858517329 [SCORE] : 0.5755908330281575\n",
      "[389/1000]\n",
      "- [VAL] LOSS : 0.030127350240945816 [SCORE] : 1.0\n",
      "[390/1000]\n",
      "- [TRAIN] LOSS : 0.05512708239257336 [SCORE] : 0.5755908330281575\n",
      "[390/1000]\n",
      "- [VAL] LOSS : 0.030106645077466965 [SCORE] : 1.0\n",
      "[391/1000]\n",
      "- [TRAIN] LOSS : 0.05511554734160503 [SCORE] : 0.5755908330281575\n",
      "[391/1000]\n",
      "- [VAL] LOSS : 0.030086111277341843 [SCORE] : 1.0\n",
      "[392/1000]\n",
      "- [TRAIN] LOSS : 0.0551040000282228 [SCORE] : 0.5755908330281575\n",
      "[392/1000]\n",
      "- [VAL] LOSS : 0.03006572090089321 [SCORE] : 1.0\n",
      "[393/1000]\n",
      "- [TRAIN] LOSS : 0.055091824972381194 [SCORE] : 0.5755908330281575\n",
      "[393/1000]\n",
      "- [VAL] LOSS : 0.030046122148633003 [SCORE] : 1.0\n",
      "[394/1000]\n",
      "- [TRAIN] LOSS : 0.055081225372850894 [SCORE] : 0.5755908330281575\n",
      "[394/1000]\n",
      "- [VAL] LOSS : 0.030026432126760483 [SCORE] : 1.0\n",
      "[395/1000]\n",
      "- [TRAIN] LOSS : 0.055070542419950165 [SCORE] : 0.5755908330281575\n",
      "[395/1000]\n",
      "- [VAL] LOSS : 0.030006540939211845 [SCORE] : 1.0\n",
      "[396/1000]\n",
      "- [TRAIN] LOSS : 0.055059384740889075 [SCORE] : 0.5755908330281575\n",
      "[396/1000]\n",
      "- [VAL] LOSS : 0.02998686395585537 [SCORE] : 1.0\n",
      "[397/1000]\n",
      "- [TRAIN] LOSS : 0.055048058709750575 [SCORE] : 0.5755908330281575\n",
      "[397/1000]\n",
      "- [VAL] LOSS : 0.02996738813817501 [SCORE] : 1.0\n",
      "[398/1000]\n",
      "- [TRAIN] LOSS : 0.055036927883823715 [SCORE] : 0.5755908330281575\n",
      "[398/1000]\n",
      "- [VAL] LOSS : 0.02994818054139614 [SCORE] : 1.0\n",
      "[399/1000]\n",
      "- [TRAIN] LOSS : 0.055026000117262205 [SCORE] : 0.5755908330281575\n",
      "[399/1000]\n",
      "- [VAL] LOSS : 0.02992910146713257 [SCORE] : 1.0\n",
      "[400/1000]\n",
      "- [TRAIN] LOSS : 0.05501518370583654 [SCORE] : 0.5755908330281575\n",
      "[400/1000]\n",
      "- [VAL] LOSS : 0.02991018258035183 [SCORE] : 1.0\n",
      "[401/1000]\n",
      "- [TRAIN] LOSS : 0.05500438380986452 [SCORE] : 0.5755908330281575\n",
      "[401/1000]\n",
      "- [VAL] LOSS : 0.029891345649957657 [SCORE] : 1.0\n",
      "[402/1000]\n",
      "- [TRAIN] LOSS : 0.054993637123455605 [SCORE] : 0.5755908330281575\n",
      "[402/1000]\n",
      "- [VAL] LOSS : 0.02987273409962654 [SCORE] : 1.0\n",
      "[403/1000]\n",
      "- [TRAIN] LOSS : 0.054982938710600136 [SCORE] : 0.5755908330281575\n",
      "[403/1000]\n",
      "- [VAL] LOSS : 0.029854251071810722 [SCORE] : 1.0\n",
      "[404/1000]\n",
      "- [TRAIN] LOSS : 0.05497233957673112 [SCORE] : 0.5755908330281575\n",
      "[404/1000]\n",
      "- [VAL] LOSS : 0.029835939407348633 [SCORE] : 1.0\n",
      "[405/1000]\n",
      "- [TRAIN] LOSS : 0.05496183174351851 [SCORE] : 0.5755908330281575\n",
      "[405/1000]\n",
      "- [VAL] LOSS : 0.029817702248692513 [SCORE] : 1.0\n",
      "[406/1000]\n",
      "- [TRAIN] LOSS : 0.05495136498163144 [SCORE] : 0.5755908330281575\n",
      "[406/1000]\n",
      "- [VAL] LOSS : 0.029799671843647957 [SCORE] : 1.0\n",
      "[407/1000]\n",
      "- [TRAIN] LOSS : 0.05494099681576093 [SCORE] : 0.5755908330281575\n",
      "[407/1000]\n",
      "- [VAL] LOSS : 0.029781755059957504 [SCORE] : 1.0\n",
      "[408/1000]\n",
      "- [TRAIN] LOSS : 0.05493063209578395 [SCORE] : 0.5755908330281575\n",
      "[408/1000]\n",
      "- [VAL] LOSS : 0.029764052480459213 [SCORE] : 1.0\n",
      "[409/1000]\n",
      "- [TRAIN] LOSS : 0.05492038410156965 [SCORE] : 0.5755908330281575\n",
      "[409/1000]\n",
      "- [VAL] LOSS : 0.029746416956186295 [SCORE] : 1.0\n",
      "[410/1000]\n",
      "- [TRAIN] LOSS : 0.05491019080703457 [SCORE] : 0.5755908330281575\n",
      "[410/1000]\n",
      "- [VAL] LOSS : 0.029728934168815613 [SCORE] : 1.0\n",
      "[411/1000]\n",
      "- [TRAIN] LOSS : 0.0549000539816916 [SCORE] : 0.5755908330281575\n",
      "[411/1000]\n",
      "- [VAL] LOSS : 0.029711630195379257 [SCORE] : 1.0\n",
      "[412/1000]\n",
      "- [TRAIN] LOSS : 0.05489001041278243 [SCORE] : 0.5755908330281575\n",
      "[412/1000]\n",
      "- [VAL] LOSS : 0.02969440445303917 [SCORE] : 1.0\n",
      "[413/1000]\n",
      "- [TRAIN] LOSS : 0.0548799527498583 [SCORE] : 0.5755908330281575\n",
      "[413/1000]\n",
      "- [VAL] LOSS : 0.029677394777536392 [SCORE] : 1.0\n",
      "[414/1000]\n",
      "- [TRAIN] LOSS : 0.054867771143714585 [SCORE] : 0.5755908330281575\n",
      "[414/1000]\n",
      "- [VAL] LOSS : 0.029662249609827995 [SCORE] : 1.0\n",
      "[415/1000]\n",
      "- [TRAIN] LOSS : 0.05486222406228383 [SCORE] : 0.5755908330281575\n",
      "[415/1000]\n",
      "- [VAL] LOSS : 0.029644865542650223 [SCORE] : 1.0\n",
      "[416/1000]\n",
      "- [TRAIN] LOSS : 0.05484995457033316 [SCORE] : 0.5755908330281575\n",
      "[416/1000]\n",
      "- [VAL] LOSS : 0.029629122465848923 [SCORE] : 1.0\n",
      "[417/1000]\n",
      "- [TRAIN] LOSS : 0.05484272964919607 [SCORE] : 0.5755908330281575\n",
      "[417/1000]\n",
      "- [VAL] LOSS : 0.029611928388476372 [SCORE] : 1.0\n",
      "[418/1000]\n",
      "- [TRAIN] LOSS : 0.05482985315223535 [SCORE] : 0.5755908330281575\n",
      "[418/1000]\n",
      "- [VAL] LOSS : 0.029596727341413498 [SCORE] : 1.0\n",
      "[419/1000]\n",
      "- [TRAIN] LOSS : 0.054821503317604464 [SCORE] : 0.5755908330281575\n",
      "[419/1000]\n",
      "- [VAL] LOSS : 0.029581191018223763 [SCORE] : 1.0\n",
      "[420/1000]\n",
      "- [TRAIN] LOSS : 0.054815073994298774 [SCORE] : 0.5755908330281575\n",
      "[420/1000]\n",
      "- [VAL] LOSS : 0.02956363558769226 [SCORE] : 1.0\n",
      "[421/1000]\n",
      "- [TRAIN] LOSS : 0.05480129215866327 [SCORE] : 0.5755908330281575\n",
      "[421/1000]\n",
      "- [VAL] LOSS : 0.029548468068242073 [SCORE] : 1.0\n",
      "[422/1000]\n",
      "- [TRAIN] LOSS : 0.05479165893048048 [SCORE] : 0.5755908330281575\n",
      "[422/1000]\n",
      "- [VAL] LOSS : 0.029533792287111282 [SCORE] : 1.0\n",
      "[423/1000]\n",
      "- [TRAIN] LOSS : 0.054783827532082795 [SCORE] : 0.5755908330281575\n",
      "[423/1000]\n",
      "- [VAL] LOSS : 0.029518714174628258 [SCORE] : 1.0\n",
      "[424/1000]\n",
      "- [TRAIN] LOSS : 0.0547752830820779 [SCORE] : 0.5755908330281575\n",
      "[424/1000]\n",
      "- [VAL] LOSS : 0.02950330264866352 [SCORE] : 1.0\n",
      "[425/1000]\n",
      "- [TRAIN] LOSS : 0.05476581081748009 [SCORE] : 0.5755908330281575\n",
      "[425/1000]\n",
      "- [VAL] LOSS : 0.029488051310181618 [SCORE] : 1.0\n",
      "[426/1000]\n",
      "- [TRAIN] LOSS : 0.05475619584321976 [SCORE] : 0.5755908330281575\n",
      "[426/1000]\n",
      "- [VAL] LOSS : 0.029473086819052696 [SCORE] : 1.0\n",
      "[427/1000]\n",
      "- [TRAIN] LOSS : 0.05474687665700913 [SCORE] : 0.5755908330281575\n",
      "[427/1000]\n",
      "- [VAL] LOSS : 0.02945837378501892 [SCORE] : 1.0\n",
      "[428/1000]\n",
      "- [TRAIN] LOSS : 0.05473788268864155 [SCORE] : 0.5755908330281575\n",
      "[428/1000]\n",
      "- [VAL] LOSS : 0.029443690553307533 [SCORE] : 1.0\n",
      "[429/1000]\n",
      "- [TRAIN] LOSS : 0.0547280291095376 [SCORE] : 0.5755908330281575\n",
      "[429/1000]\n",
      "- [VAL] LOSS : 0.029429513961076736 [SCORE] : 1.0\n",
      "[430/1000]\n",
      "- [TRAIN] LOSS : 0.05472268083443244 [SCORE] : 0.5755908330281575\n",
      "[430/1000]\n",
      "- [VAL] LOSS : 0.02941315434873104 [SCORE] : 1.0\n",
      "[431/1000]\n",
      "- [TRAIN] LOSS : 0.054708774729321404 [SCORE] : 0.5755908330281575\n",
      "[431/1000]\n",
      "- [VAL] LOSS : 0.029399409890174866 [SCORE] : 1.0\n",
      "[432/1000]\n",
      "- [TRAIN] LOSS : 0.05470096046725909 [SCORE] : 0.5755908330281575\n",
      "[432/1000]\n",
      "- [VAL] LOSS : 0.029385676607489586 [SCORE] : 1.0\n",
      "[433/1000]\n",
      "- [TRAIN] LOSS : 0.054693540837615726 [SCORE] : 0.5755908330281575\n",
      "[433/1000]\n",
      "- [VAL] LOSS : 0.029371410608291626 [SCORE] : 1.0\n",
      "[434/1000]\n",
      "- [TRAIN] LOSS : 0.054684952305008964 [SCORE] : 0.5755908330281575\n",
      "[434/1000]\n",
      "- [VAL] LOSS : 0.029357166960835457 [SCORE] : 1.0\n",
      "[435/1000]\n",
      "- [TRAIN] LOSS : 0.05467578452080488 [SCORE] : 0.5755908330281575\n",
      "[435/1000]\n",
      "- [VAL] LOSS : 0.02934313751757145 [SCORE] : 1.0\n",
      "[436/1000]\n",
      "- [TRAIN] LOSS : 0.05466677170867721 [SCORE] : 0.5755908330281575\n",
      "[436/1000]\n",
      "- [VAL] LOSS : 0.029329393059015274 [SCORE] : 1.0\n",
      "[437/1000]\n",
      "- [TRAIN] LOSS : 0.05465812059119344 [SCORE] : 0.5755908330281575\n",
      "[437/1000]\n",
      "- [VAL] LOSS : 0.02931576780974865 [SCORE] : 1.0\n",
      "[438/1000]\n",
      "- [TRAIN] LOSS : 0.05464873391514023 [SCORE] : 0.5755908330281575\n",
      "[438/1000]\n",
      "- [VAL] LOSS : 0.029302546754479408 [SCORE] : 1.0\n",
      "[439/1000]\n",
      "- [TRAIN] LOSS : 0.054643852387865385 [SCORE] : 0.5755908330281575\n",
      "[439/1000]\n",
      "- [VAL] LOSS : 0.029287287965416908 [SCORE] : 1.0\n",
      "[440/1000]\n",
      "- [TRAIN] LOSS : 0.05463051594172915 [SCORE] : 0.5755908330281575\n",
      "[440/1000]\n",
      "- [VAL] LOSS : 0.029274482280015945 [SCORE] : 1.0\n",
      "[441/1000]\n",
      "- [TRAIN] LOSS : 0.05462307858591278 [SCORE] : 0.5755908330281575\n",
      "[441/1000]\n",
      "- [VAL] LOSS : 0.029261654242873192 [SCORE] : 1.0\n",
      "[442/1000]\n",
      "- [TRAIN] LOSS : 0.054618100201090175 [SCORE] : 0.5755908330281575\n",
      "[442/1000]\n",
      "- [VAL] LOSS : 0.029246926307678223 [SCORE] : 1.0\n",
      "[443/1000]\n",
      "- [TRAIN] LOSS : 0.05460607617472609 [SCORE] : 0.5755908330281575\n",
      "[443/1000]\n",
      "- [VAL] LOSS : 0.029234031215310097 [SCORE] : 1.0\n",
      "[444/1000]\n",
      "- [TRAIN] LOSS : 0.054597483730564514 [SCORE] : 0.5755908330281575\n",
      "[444/1000]\n",
      "- [VAL] LOSS : 0.029221678152680397 [SCORE] : 1.0\n",
      "[445/1000]\n",
      "- [TRAIN] LOSS : 0.05459047133723895 [SCORE] : 0.5755908330281575\n",
      "[445/1000]\n",
      "- [VAL] LOSS : 0.029208999127149582 [SCORE] : 1.0\n",
      "[446/1000]\n",
      "- [TRAIN] LOSS : 0.05458206059411168 [SCORE] : 0.5755908330281575\n",
      "[446/1000]\n",
      "- [VAL] LOSS : 0.02919640578329563 [SCORE] : 1.0\n",
      "[447/1000]\n",
      "- [TRAIN] LOSS : 0.054577256956448154 [SCORE] : 0.5755908330281575\n",
      "[447/1000]\n",
      "- [VAL] LOSS : 0.02918197773396969 [SCORE] : 1.0\n",
      "[448/1000]\n",
      "- [TRAIN] LOSS : 0.05456409857918819 [SCORE] : 0.5755908330281575\n",
      "[448/1000]\n",
      "- [VAL] LOSS : 0.029169950634241104 [SCORE] : 1.0\n",
      "[449/1000]\n",
      "- [TRAIN] LOSS : 0.05455688635508219 [SCORE] : 0.5755908330281575\n",
      "[449/1000]\n",
      "- [VAL] LOSS : 0.029157908633351326 [SCORE] : 1.0\n",
      "[450/1000]\n",
      "- [TRAIN] LOSS : 0.054550238450368244 [SCORE] : 0.5755908330281575\n",
      "[450/1000]\n",
      "- [VAL] LOSS : 0.029145488515496254 [SCORE] : 1.0\n",
      "[451/1000]\n",
      "- [TRAIN] LOSS : 0.05454253265634179 [SCORE] : 0.5755908330281575\n",
      "[451/1000]\n",
      "- [VAL] LOSS : 0.02913300134241581 [SCORE] : 1.0\n",
      "[452/1000]\n",
      "- [TRAIN] LOSS : 0.05453415761391322 [SCORE] : 0.5755908330281575\n",
      "[452/1000]\n",
      "- [VAL] LOSS : 0.029120707884430885 [SCORE] : 1.0\n",
      "[453/1000]\n",
      "- [TRAIN] LOSS : 0.05452590261896451 [SCORE] : 0.5755908330281575\n",
      "[453/1000]\n",
      "- [VAL] LOSS : 0.02910865657031536 [SCORE] : 1.0\n",
      "[454/1000]\n",
      "- [TRAIN] LOSS : 0.0545191315934062 [SCORE] : 0.5755908330281575\n",
      "[454/1000]\n",
      "- [VAL] LOSS : 0.02909569814801216 [SCORE] : 1.0\n",
      "[455/1000]\n",
      "- [TRAIN] LOSS : 0.05450821242605646 [SCORE] : 0.5755908330281575\n",
      "[455/1000]\n",
      "- [VAL] LOSS : 0.029084371402859688 [SCORE] : 1.0\n",
      "[456/1000]\n",
      "- [TRAIN] LOSS : 0.054502070602029565 [SCORE] : 0.5755908330281575\n",
      "[456/1000]\n",
      "- [VAL] LOSS : 0.029072748497128487 [SCORE] : 1.0\n",
      "[457/1000]\n",
      "- [TRAIN] LOSS : 0.05449734581634402 [SCORE] : 0.5755908330281575\n",
      "[457/1000]\n",
      "- [VAL] LOSS : 0.02905944176018238 [SCORE] : 1.0\n",
      "[458/1000]\n",
      "- [TRAIN] LOSS : 0.05448569804430008 [SCORE] : 0.5755908330281575\n",
      "[458/1000]\n",
      "- [VAL] LOSS : 0.029048021882772446 [SCORE] : 1.0\n",
      "[459/1000]\n",
      "- [TRAIN] LOSS : 0.054477633287509285 [SCORE] : 0.5755908330281575\n",
      "[459/1000]\n",
      "- [VAL] LOSS : 0.029037075117230415 [SCORE] : 1.0\n",
      "[460/1000]\n",
      "- [TRAIN] LOSS : 0.05447129548216859 [SCORE] : 0.5755908330281575\n",
      "[460/1000]\n",
      "- [VAL] LOSS : 0.029025809839367867 [SCORE] : 1.0\n",
      "[461/1000]\n",
      "- [TRAIN] LOSS : 0.05446558178712924 [SCORE] : 0.5755908330281575\n",
      "[461/1000]\n",
      "- [VAL] LOSS : 0.02901328168809414 [SCORE] : 1.0\n",
      "[462/1000]\n",
      "- [TRAIN] LOSS : 0.05445477869361639 [SCORE] : 0.5755908330281575\n",
      "[462/1000]\n",
      "- [VAL] LOSS : 0.029002370312809944 [SCORE] : 1.0\n",
      "[463/1000]\n",
      "- [TRAIN] LOSS : 0.054448444458345575 [SCORE] : 0.5755908330281575\n",
      "[463/1000]\n",
      "- [VAL] LOSS : 0.02899138443171978 [SCORE] : 1.0\n",
      "[464/1000]\n",
      "- [TRAIN] LOSS : 0.054443788218001526 [SCORE] : 0.5755908330281575\n",
      "[464/1000]\n",
      "- [VAL] LOSS : 0.02897883579134941 [SCORE] : 1.0\n",
      "[465/1000]\n",
      "- [TRAIN] LOSS : 0.05443175078059236 [SCORE] : 0.5755908330281575\n",
      "[465/1000]\n",
      "- [VAL] LOSS : 0.028968339785933495 [SCORE] : 1.0\n",
      "[466/1000]\n",
      "- [TRAIN] LOSS : 0.05442546770597498 [SCORE] : 0.5755908330281575\n",
      "[466/1000]\n",
      "- [VAL] LOSS : 0.02895784191787243 [SCORE] : 1.0\n",
      "[467/1000]\n",
      "- [TRAIN] LOSS : 0.05441946464901169 [SCORE] : 0.5755908330281575\n",
      "[467/1000]\n",
      "- [VAL] LOSS : 0.028946880251169205 [SCORE] : 1.0\n",
      "[468/1000]\n",
      "- [TRAIN] LOSS : 0.05441425535827875 [SCORE] : 0.5755908330281575\n",
      "[468/1000]\n",
      "- [VAL] LOSS : 0.02893460914492607 [SCORE] : 1.0\n",
      "[469/1000]\n",
      "- [TRAIN] LOSS : 0.054402074435104926 [SCORE] : 0.5755908330281575\n",
      "[469/1000]\n",
      "- [VAL] LOSS : 0.028924565762281418 [SCORE] : 1.0\n",
      "[470/1000]\n",
      "- [TRAIN] LOSS : 0.054395957353214425 [SCORE] : 0.5755908330281575\n",
      "[470/1000]\n",
      "- [VAL] LOSS : 0.028914397582411766 [SCORE] : 1.0\n",
      "[471/1000]\n",
      "- [TRAIN] LOSS : 0.054392178139338894 [SCORE] : 0.5755908330281575\n",
      "[471/1000]\n",
      "- [VAL] LOSS : 0.028902549296617508 [SCORE] : 1.0\n",
      "[472/1000]\n",
      "- [TRAIN] LOSS : 0.054380798464020094 [SCORE] : 0.5755908330281575\n",
      "[472/1000]\n",
      "- [VAL] LOSS : 0.028892528265714645 [SCORE] : 1.0\n",
      "[473/1000]\n",
      "- [TRAIN] LOSS : 0.05437456456323465 [SCORE] : 0.5755908330281575\n",
      "[473/1000]\n",
      "- [VAL] LOSS : 0.028882507234811783 [SCORE] : 1.0\n",
      "[474/1000]\n",
      "- [TRAIN] LOSS : 0.054368613629291455 [SCORE] : 0.5755908330281575\n",
      "[474/1000]\n",
      "- [VAL] LOSS : 0.028872167691588402 [SCORE] : 1.0\n",
      "[475/1000]\n",
      "- [TRAIN] LOSS : 0.054363653312126795 [SCORE] : 0.5755908330281575\n",
      "[475/1000]\n",
      "- [VAL] LOSS : 0.02886061742901802 [SCORE] : 1.0\n",
      "[476/1000]\n",
      "- [TRAIN] LOSS : 0.05435182681928078 [SCORE] : 0.5755908330281575\n",
      "[476/1000]\n",
      "- [VAL] LOSS : 0.028851091861724854 [SCORE] : 1.0\n",
      "[477/1000]\n",
      "- [TRAIN] LOSS : 0.054345966503024104 [SCORE] : 0.5755908330281575\n",
      "[477/1000]\n",
      "- [VAL] LOSS : 0.028841469436883926 [SCORE] : 1.0\n",
      "[478/1000]\n",
      "- [TRAIN] LOSS : 0.05434237662702799 [SCORE] : 0.5755908330281575\n",
      "[478/1000]\n",
      "- [VAL] LOSS : 0.02883024886250496 [SCORE] : 1.0\n",
      "[479/1000]\n",
      "- [TRAIN] LOSS : 0.054331318102777006 [SCORE] : 0.5755908330281575\n",
      "[479/1000]\n",
      "- [VAL] LOSS : 0.02882075496017933 [SCORE] : 1.0\n",
      "[480/1000]\n",
      "- [TRAIN] LOSS : 0.05432452543949087 [SCORE] : 0.5755908330281575\n",
      "[480/1000]\n",
      "- [VAL] LOSS : 0.028811592608690262 [SCORE] : 1.0\n",
      "[481/1000]\n",
      "- [TRAIN] LOSS : 0.054322040360420945 [SCORE] : 0.5755908330281575\n",
      "[481/1000]\n",
      "- [VAL] LOSS : 0.02880045399069786 [SCORE] : 1.0\n",
      "[482/1000]\n",
      "- [TRAIN] LOSS : 0.05431191554913918 [SCORE] : 0.5755908330281575\n",
      "[482/1000]\n",
      "- [VAL] LOSS : 0.02879071608185768 [SCORE] : 1.0\n",
      "[483/1000]\n",
      "- [TRAIN] LOSS : 0.0543042853474617 [SCORE] : 0.5755908330281575\n",
      "[483/1000]\n",
      "- [VAL] LOSS : 0.028781533241271973 [SCORE] : 1.0\n",
      "[484/1000]\n",
      "- [TRAIN] LOSS : 0.05429743615289529 [SCORE] : 0.5755908330281575\n",
      "[484/1000]\n",
      "- [VAL] LOSS : 0.028772534802556038 [SCORE] : 1.0\n",
      "[485/1000]\n",
      "- [TRAIN] LOSS : 0.05429461986447374 [SCORE] : 0.5755908330281575\n",
      "[485/1000]\n",
      "- [VAL] LOSS : 0.028761710971593857 [SCORE] : 1.0\n",
      "[486/1000]\n",
      "- [TRAIN] LOSS : 0.054283719261487326 [SCORE] : 0.5755908330281575\n",
      "[486/1000]\n",
      "- [VAL] LOSS : 0.02875262312591076 [SCORE] : 1.0\n",
      "[487/1000]\n",
      "- [TRAIN] LOSS : 0.05427768708517154 [SCORE] : 0.5755908330281575\n",
      "[487/1000]\n",
      "- [VAL] LOSS : 0.028743652626872063 [SCORE] : 1.0\n",
      "[488/1000]\n",
      "- [TRAIN] LOSS : 0.0542738682590425 [SCORE] : 0.5755908330281575\n",
      "[488/1000]\n",
      "- [VAL] LOSS : 0.02873322181403637 [SCORE] : 1.0\n",
      "[489/1000]\n",
      "- [TRAIN] LOSS : 0.054263172981639705 [SCORE] : 0.5755908330281575\n",
      "[489/1000]\n",
      "- [VAL] LOSS : 0.02872450463473797 [SCORE] : 1.0\n",
      "[490/1000]\n",
      "- [TRAIN] LOSS : 0.05425951760262251 [SCORE] : 0.5755908330281575\n",
      "[490/1000]\n",
      "- [VAL] LOSS : 0.028714628890156746 [SCORE] : 1.0\n",
      "[491/1000]\n",
      "- [TRAIN] LOSS : 0.054249910606692237 [SCORE] : 0.5755908330281575\n",
      "[491/1000]\n",
      "- [VAL] LOSS : 0.02870607003569603 [SCORE] : 1.0\n",
      "[492/1000]\n",
      "- [TRAIN] LOSS : 0.05424671061336994 [SCORE] : 0.5755908330281575\n",
      "[492/1000]\n",
      "- [VAL] LOSS : 0.0286962129175663 [SCORE] : 1.0\n",
      "[493/1000]\n",
      "- [TRAIN] LOSS : 0.05423783309136828 [SCORE] : 0.5755908330281575\n",
      "[493/1000]\n",
      "- [VAL] LOSS : 0.028687411919236183 [SCORE] : 1.0\n",
      "[494/1000]\n",
      "- [TRAIN] LOSS : 0.05423044205332796 [SCORE] : 0.5755908330281575\n",
      "[494/1000]\n",
      "- [VAL] LOSS : 0.028679216280579567 [SCORE] : 1.0\n",
      "[495/1000]\n",
      "- [TRAIN] LOSS : 0.054228023843218884 [SCORE] : 0.5755908330281575\n",
      "[495/1000]\n",
      "- [VAL] LOSS : 0.02866937220096588 [SCORE] : 1.0\n",
      "[496/1000]\n",
      "- [TRAIN] LOSS : 0.054217911403005324 [SCORE] : 0.5755908330281575\n",
      "[496/1000]\n",
      "- [VAL] LOSS : 0.028660910204052925 [SCORE] : 1.0\n",
      "[497/1000]\n",
      "- [TRAIN] LOSS : 0.05421231168632706 [SCORE] : 0.5755908330281575\n",
      "[497/1000]\n",
      "- [VAL] LOSS : 0.02865249291062355 [SCORE] : 1.0\n",
      "[498/1000]\n",
      "- [TRAIN] LOSS : 0.054208603066702686 [SCORE] : 0.5755908330281575\n",
      "[498/1000]\n",
      "- [VAL] LOSS : 0.028642820194363594 [SCORE] : 1.0\n",
      "[499/1000]\n",
      "- [TRAIN] LOSS : 0.05419819109762709 [SCORE] : 0.5755908330281575\n",
      "[499/1000]\n",
      "- [VAL] LOSS : 0.02863476611673832 [SCORE] : 1.0\n",
      "[500/1000]\n",
      "- [TRAIN] LOSS : 0.05419475920498371 [SCORE] : 0.5755908330281575\n",
      "[500/1000]\n",
      "- [VAL] LOSS : 0.0286256056278944 [SCORE] : 1.0\n",
      "[501/1000]\n",
      "- [TRAIN] LOSS : 0.05418552374467254 [SCORE] : 0.5755908330281575\n",
      "[501/1000]\n",
      "- [VAL] LOSS : 0.028617681935429573 [SCORE] : 1.0\n",
      "[502/1000]\n",
      "- [TRAIN] LOSS : 0.054182538359115524 [SCORE] : 0.5755908330281575\n",
      "[502/1000]\n",
      "- [VAL] LOSS : 0.02860851399600506 [SCORE] : 1.0\n",
      "[503/1000]\n",
      "- [TRAIN] LOSS : 0.05417319843545556 [SCORE] : 0.5755908330281575\n",
      "[503/1000]\n",
      "- [VAL] LOSS : 0.02860063500702381 [SCORE] : 1.0\n",
      "[504/1000]\n",
      "- [TRAIN] LOSS : 0.05416816010450323 [SCORE] : 0.5755908330281575\n",
      "[504/1000]\n",
      "- [VAL] LOSS : 0.0285926666110754 [SCORE] : 1.0\n",
      "[505/1000]\n",
      "- [TRAIN] LOSS : 0.05416469100552301 [SCORE] : 0.5755908330281575\n",
      "[505/1000]\n",
      "- [VAL] LOSS : 0.02858349122107029 [SCORE] : 1.0\n",
      "[506/1000]\n",
      "- [TRAIN] LOSS : 0.054154443880543114 [SCORE] : 0.5755908330281575\n",
      "[506/1000]\n",
      "- [VAL] LOSS : 0.028575818985700607 [SCORE] : 1.0\n",
      "[507/1000]\n",
      "- [TRAIN] LOSS : 0.05415107388980687 [SCORE] : 0.5755908330281575\n",
      "[507/1000]\n",
      "- [VAL] LOSS : 0.0285671204328537 [SCORE] : 1.0\n",
      "[508/1000]\n",
      "- [TRAIN] LOSS : 0.054142023483291266 [SCORE] : 0.5755908330281575\n",
      "[508/1000]\n",
      "- [VAL] LOSS : 0.028559697791934013 [SCORE] : 1.0\n",
      "[509/1000]\n",
      "- [TRAIN] LOSS : 0.05413923334951202 [SCORE] : 0.5755908330281575\n",
      "[509/1000]\n",
      "- [VAL] LOSS : 0.02855101227760315 [SCORE] : 1.0\n",
      "[510/1000]\n",
      "- [TRAIN] LOSS : 0.054130141933759056 [SCORE] : 0.5755908330281575\n",
      "[510/1000]\n",
      "- [VAL] LOSS : 0.028543587774038315 [SCORE] : 1.0\n",
      "[511/1000]\n",
      "- [TRAIN] LOSS : 0.054127086295435826 [SCORE] : 0.5755908330281575\n",
      "[511/1000]\n",
      "- [VAL] LOSS : 0.02853504754602909 [SCORE] : 1.0\n",
      "[512/1000]\n",
      "- [TRAIN] LOSS : 0.0541179583252718 [SCORE] : 0.5755908330281575\n",
      "[512/1000]\n",
      "- [VAL] LOSS : 0.028527725487947464 [SCORE] : 1.0\n",
      "[513/1000]\n",
      "- [TRAIN] LOSS : 0.05411500772461295 [SCORE] : 0.5755908330281575\n",
      "[513/1000]\n",
      "- [VAL] LOSS : 0.02851930633187294 [SCORE] : 1.0\n",
      "[514/1000]\n",
      "- [TRAIN] LOSS : 0.054106784891337155 [SCORE] : 0.5755908330281575\n",
      "[514/1000]\n",
      "- [VAL] LOSS : 0.028511885553598404 [SCORE] : 1.0\n",
      "[515/1000]\n",
      "- [TRAIN] LOSS : 0.05410002678011854 [SCORE] : 0.5755908330281575\n",
      "[515/1000]\n",
      "- [VAL] LOSS : 0.028504952788352966 [SCORE] : 1.0\n",
      "[516/1000]\n",
      "- [TRAIN] LOSS : 0.05409802321034173 [SCORE] : 0.5755908330281575\n",
      "[516/1000]\n",
      "- [VAL] LOSS : 0.02849649451673031 [SCORE] : 1.0\n",
      "[517/1000]\n",
      "- [TRAIN] LOSS : 0.054088647942990065 [SCORE] : 0.5755908330281575\n",
      "[517/1000]\n",
      "- [VAL] LOSS : 0.028489306569099426 [SCORE] : 1.0\n",
      "[518/1000]\n",
      "- [TRAIN] LOSS : 0.054085253893087305 [SCORE] : 0.5755908330281575\n",
      "[518/1000]\n",
      "- [VAL] LOSS : 0.028481263667345047 [SCORE] : 1.0\n",
      "[519/1000]\n",
      "- [TRAIN] LOSS : 0.05407620929181576 [SCORE] : 0.5755908330281575\n",
      "[519/1000]\n",
      "- [VAL] LOSS : 0.028474364429712296 [SCORE] : 1.0\n",
      "[520/1000]\n",
      "- [TRAIN] LOSS : 0.05407359541083376 [SCORE] : 0.5755908330281575\n",
      "[520/1000]\n",
      "- [VAL] LOSS : 0.028466427698731422 [SCORE] : 1.0\n",
      "[521/1000]\n",
      "- [TRAIN] LOSS : 0.05406487270568808 [SCORE] : 0.5755908330281575\n",
      "[521/1000]\n",
      "- [VAL] LOSS : 0.028459599241614342 [SCORE] : 1.0\n",
      "[522/1000]\n",
      "- [TRAIN] LOSS : 0.054062095719079176 [SCORE] : 0.5755908330281575\n",
      "[522/1000]\n",
      "- [VAL] LOSS : 0.028451669961214066 [SCORE] : 1.0\n",
      "[523/1000]\n",
      "- [TRAIN] LOSS : 0.05405330169014633 [SCORE] : 0.5755908330281575\n",
      "[523/1000]\n",
      "- [VAL] LOSS : 0.028444970026612282 [SCORE] : 1.0\n",
      "[524/1000]\n",
      "- [TRAIN] LOSS : 0.05405052239075303 [SCORE] : 0.5755908330281575\n",
      "[524/1000]\n",
      "- [VAL] LOSS : 0.028437232598662376 [SCORE] : 1.0\n",
      "[525/1000]\n",
      "- [TRAIN] LOSS : 0.05404178709723055 [SCORE] : 0.5755908330281575\n",
      "[525/1000]\n",
      "- [VAL] LOSS : 0.02843061275780201 [SCORE] : 1.0\n",
      "[526/1000]\n",
      "- [TRAIN] LOSS : 0.054039116591836016 [SCORE] : 0.5755908330281575\n",
      "[526/1000]\n",
      "- [VAL] LOSS : 0.028422996401786804 [SCORE] : 1.0\n",
      "[527/1000]\n",
      "- [TRAIN] LOSS : 0.054030405807619294 [SCORE] : 0.5755908330281575\n",
      "[527/1000]\n",
      "- [VAL] LOSS : 0.028416475281119347 [SCORE] : 1.0\n",
      "[528/1000]\n",
      "- [TRAIN] LOSS : 0.054027752376471956 [SCORE] : 0.5755908330281575\n",
      "[528/1000]\n",
      "- [VAL] LOSS : 0.028408924117684364 [SCORE] : 1.0\n",
      "[529/1000]\n",
      "- [TRAIN] LOSS : 0.054019144844884674 [SCORE] : 0.5755908330281575\n",
      "[529/1000]\n",
      "- [VAL] LOSS : 0.02840253710746765 [SCORE] : 1.0\n",
      "[530/1000]\n",
      "- [TRAIN] LOSS : 0.05401653020332257 [SCORE] : 0.5755908330281575\n",
      "[530/1000]\n",
      "- [VAL] LOSS : 0.028395112603902817 [SCORE] : 1.0\n",
      "[531/1000]\n",
      "- [TRAIN] LOSS : 0.05400876956991851 [SCORE] : 0.5755908330281575\n",
      "[531/1000]\n",
      "- [VAL] LOSS : 0.02838856540620327 [SCORE] : 1.0\n",
      "[532/1000]\n",
      "- [TRAIN] LOSS : 0.05400404101237655 [SCORE] : 0.5755908330281575\n",
      "[532/1000]\n",
      "- [VAL] LOSS : 0.02838158793747425 [SCORE] : 1.0\n",
      "[533/1000]\n",
      "- [TRAIN] LOSS : 0.05399673590436578 [SCORE] : 0.5755908330281575\n",
      "[533/1000]\n",
      "- [VAL] LOSS : 0.028375403955578804 [SCORE] : 1.0\n",
      "[534/1000]\n",
      "- [TRAIN] LOSS : 0.053994737146422264 [SCORE] : 0.5755908330281575\n",
      "[534/1000]\n",
      "- [VAL] LOSS : 0.028368128463625908 [SCORE] : 1.0\n",
      "[535/1000]\n",
      "- [TRAIN] LOSS : 0.053987773073216276 [SCORE] : 0.5755908330281575\n",
      "[535/1000]\n",
      "- [VAL] LOSS : 0.02836105413734913 [SCORE] : 1.0\n",
      "[536/1000]\n",
      "- [TRAIN] LOSS : 0.053980198114489515 [SCORE] : 0.5755908330281575\n",
      "[536/1000]\n",
      "- [VAL] LOSS : 0.028354933485388756 [SCORE] : 1.0\n",
      "[537/1000]\n",
      "- [TRAIN] LOSS : 0.05397722600027919 [SCORE] : 0.5755908330281575\n",
      "[537/1000]\n",
      "- [VAL] LOSS : 0.028348062187433243 [SCORE] : 1.0\n",
      "[538/1000]\n",
      "- [TRAIN] LOSS : 0.053969155981515846 [SCORE] : 0.5755908330281575\n",
      "[538/1000]\n",
      "- [VAL] LOSS : 0.028342172503471375 [SCORE] : 1.0\n",
      "[539/1000]\n",
      "- [TRAIN] LOSS : 0.05396619280800223 [SCORE] : 0.5755908330281575\n",
      "[539/1000]\n",
      "- [VAL] LOSS : 0.02833542786538601 [SCORE] : 1.0\n",
      "[540/1000]\n",
      "- [TRAIN] LOSS : 0.05396171685618659 [SCORE] : 0.5755908330281575\n",
      "[540/1000]\n",
      "- [VAL] LOSS : 0.028328359127044678 [SCORE] : 1.0\n",
      "[541/1000]\n",
      "- [TRAIN] LOSS : 0.05395251726731658 [SCORE] : 0.5755908330281575\n",
      "[541/1000]\n",
      "- [VAL] LOSS : 0.028322666883468628 [SCORE] : 1.0\n",
      "[542/1000]\n",
      "- [TRAIN] LOSS : 0.05395030630752444 [SCORE] : 0.5755908330281575\n",
      "[542/1000]\n",
      "- [VAL] LOSS : 0.02831597626209259 [SCORE] : 1.0\n",
      "[543/1000]\n",
      "- [TRAIN] LOSS : 0.053942561397949854 [SCORE] : 0.5755908330281575\n",
      "[543/1000]\n",
      "- [VAL] LOSS : 0.02831023372709751 [SCORE] : 1.0\n",
      "[544/1000]\n",
      "- [TRAIN] LOSS : 0.05394027694128454 [SCORE] : 0.5755908330281575\n",
      "[544/1000]\n",
      "- [VAL] LOSS : 0.028303487226366997 [SCORE] : 1.0\n",
      "[545/1000]\n",
      "- [TRAIN] LOSS : 0.05393198559371134 [SCORE] : 0.5755908330281575\n",
      "[545/1000]\n",
      "- [VAL] LOSS : 0.028297772631049156 [SCORE] : 1.0\n",
      "[546/1000]\n",
      "- [TRAIN] LOSS : 0.05392944387470682 [SCORE] : 0.5755908330281575\n",
      "[546/1000]\n",
      "- [VAL] LOSS : 0.028291212394833565 [SCORE] : 1.0\n",
      "[547/1000]\n",
      "- [TRAIN] LOSS : 0.05392292321970065 [SCORE] : 0.5755908330281575\n",
      "[547/1000]\n",
      "- [VAL] LOSS : 0.028284823521971703 [SCORE] : 1.0\n",
      "[548/1000]\n",
      "- [TRAIN] LOSS : 0.05391519462379316 [SCORE] : 0.5755908330281575\n",
      "[548/1000]\n",
      "- [VAL] LOSS : 0.028279483318328857 [SCORE] : 1.0\n",
      "[549/1000]\n",
      "- [TRAIN] LOSS : 0.053913789646079145 [SCORE] : 0.5755908330281575\n",
      "[549/1000]\n",
      "- [VAL] LOSS : 0.02827306091785431 [SCORE] : 1.0\n",
      "[550/1000]\n",
      "- [TRAIN] LOSS : 0.05390848408763607 [SCORE] : 0.5755908330281575\n",
      "[550/1000]\n",
      "- [VAL] LOSS : 0.028266580775380135 [SCORE] : 1.0\n",
      "[551/1000]\n",
      "- [TRAIN] LOSS : 0.053899335643897456 [SCORE] : 0.5755908330281575\n",
      "[551/1000]\n",
      "- [VAL] LOSS : 0.028261389583349228 [SCORE] : 1.0\n",
      "[552/1000]\n",
      "- [TRAIN] LOSS : 0.053896750307952365 [SCORE] : 0.5755908330281575\n",
      "[552/1000]\n",
      "- [VAL] LOSS : 0.028255434706807137 [SCORE] : 1.0\n",
      "[553/1000]\n",
      "- [TRAIN] LOSS : 0.05389317098694543 [SCORE] : 0.5755908330281575\n",
      "[553/1000]\n",
      "- [VAL] LOSS : 0.028249001130461693 [SCORE] : 1.0\n",
      "[554/1000]\n",
      "- [TRAIN] LOSS : 0.05388456087869902 [SCORE] : 0.5755908330281575\n",
      "[554/1000]\n",
      "- [VAL] LOSS : 0.02824380435049534 [SCORE] : 1.0\n",
      "[555/1000]\n",
      "- [TRAIN] LOSS : 0.05388242369517684 [SCORE] : 0.5755908330281575\n",
      "[555/1000]\n",
      "- [VAL] LOSS : 0.02823774702847004 [SCORE] : 1.0\n",
      "[556/1000]\n",
      "- [TRAIN] LOSS : 0.0538747978862375 [SCORE] : 0.5755908330281575\n",
      "[556/1000]\n",
      "- [VAL] LOSS : 0.028232524171471596 [SCORE] : 1.0\n",
      "[557/1000]\n",
      "- [TRAIN] LOSS : 0.05387268364429474 [SCORE] : 0.5755908330281575\n",
      "[557/1000]\n",
      "- [VAL] LOSS : 0.028226440772414207 [SCORE] : 1.0\n",
      "[558/1000]\n",
      "- [TRAIN] LOSS : 0.05386629708421727 [SCORE] : 0.5755908330281575\n",
      "[558/1000]\n",
      "- [VAL] LOSS : 0.02822055108845234 [SCORE] : 1.0\n",
      "[559/1000]\n",
      "- [TRAIN] LOSS : 0.05385870495811105 [SCORE] : 0.5755908330281575\n",
      "[559/1000]\n",
      "- [VAL] LOSS : 0.028215667232871056 [SCORE] : 1.0\n",
      "[560/1000]\n",
      "- [TRAIN] LOSS : 0.05385740202230712 [SCORE] : 0.5755908330281575\n",
      "[560/1000]\n",
      "- [VAL] LOSS : 0.02820974588394165 [SCORE] : 1.0\n",
      "[561/1000]\n",
      "- [TRAIN] LOSS : 0.053852280136197805 [SCORE] : 0.5755908330281575\n",
      "[561/1000]\n",
      "- [VAL] LOSS : 0.02820374257862568 [SCORE] : 1.0\n",
      "[562/1000]\n",
      "- [TRAIN] LOSS : 0.053843391795332235 [SCORE] : 0.5755908330281575\n",
      "[562/1000]\n",
      "- [VAL] LOSS : 0.02819901518523693 [SCORE] : 1.0\n",
      "[563/1000]\n",
      "- [TRAIN] LOSS : 0.053840994260584314 [SCORE] : 0.5755908330281575\n",
      "[563/1000]\n",
      "- [VAL] LOSS : 0.028193553909659386 [SCORE] : 1.0\n",
      "[564/1000]\n",
      "- [TRAIN] LOSS : 0.053837600133071345 [SCORE] : 0.5755908330281575\n",
      "[564/1000]\n",
      "- [VAL] LOSS : 0.02818768098950386 [SCORE] : 1.0\n",
      "[565/1000]\n",
      "- [TRAIN] LOSS : 0.05382920199384292 [SCORE] : 0.5755908330281575\n",
      "[565/1000]\n",
      "- [VAL] LOSS : 0.028182899579405785 [SCORE] : 1.0\n",
      "[566/1000]\n",
      "- [TRAIN] LOSS : 0.05382720474153757 [SCORE] : 0.5755908330281575\n",
      "[566/1000]\n",
      "- [VAL] LOSS : 0.028177324682474136 [SCORE] : 1.0\n",
      "[567/1000]\n",
      "- [TRAIN] LOSS : 0.05382142793387175 [SCORE] : 0.5755908330281575\n",
      "[567/1000]\n",
      "- [VAL] LOSS : 0.02817183919250965 [SCORE] : 1.0\n",
      "[568/1000]\n",
      "- [TRAIN] LOSS : 0.05381419306310515 [SCORE] : 0.5755908330281575\n",
      "[568/1000]\n",
      "- [VAL] LOSS : 0.028167277574539185 [SCORE] : 1.0\n",
      "[569/1000]\n",
      "- [TRAIN] LOSS : 0.053812908846884965 [SCORE] : 0.5755908330281575\n",
      "[569/1000]\n",
      "- [VAL] LOSS : 0.028161708265542984 [SCORE] : 1.0\n",
      "[570/1000]\n",
      "- [TRAIN] LOSS : 0.05380708848436674 [SCORE] : 0.5755908330281575\n",
      "[570/1000]\n",
      "- [VAL] LOSS : 0.02815631404519081 [SCORE] : 1.0\n",
      "[571/1000]\n",
      "- [TRAIN] LOSS : 0.053801206530382235 [SCORE] : 0.5755908330281575\n",
      "[571/1000]\n",
      "- [VAL] LOSS : 0.0281511340290308 [SCORE] : 1.0\n",
      "[572/1000]\n",
      "- [TRAIN] LOSS : 0.053797001282994944 [SCORE] : 0.5755908330281575\n",
      "[572/1000]\n",
      "- [VAL] LOSS : 0.028145836666226387 [SCORE] : 1.0\n",
      "[573/1000]\n",
      "- [TRAIN] LOSS : 0.05378968419196705 [SCORE] : 0.5755908330281575\n",
      "[573/1000]\n",
      "- [VAL] LOSS : 0.028141524642705917 [SCORE] : 1.0\n",
      "[574/1000]\n",
      "- [TRAIN] LOSS : 0.05378862257736425 [SCORE] : 0.5755908330281575\n",
      "[574/1000]\n",
      "- [VAL] LOSS : 0.02813623659312725 [SCORE] : 1.0\n",
      "[575/1000]\n",
      "- [TRAIN] LOSS : 0.05378305213525891 [SCORE] : 0.5755908330281575\n",
      "[575/1000]\n",
      "- [VAL] LOSS : 0.028131017461419106 [SCORE] : 1.0\n",
      "[576/1000]\n",
      "- [TRAIN] LOSS : 0.053775728493928907 [SCORE] : 0.5755908330281575\n",
      "[576/1000]\n",
      "- [VAL] LOSS : 0.02812671661376953 [SCORE] : 1.0\n",
      "[577/1000]\n",
      "- [TRAIN] LOSS : 0.053774342065056165 [SCORE] : 0.5755908330281575\n",
      "[577/1000]\n",
      "- [VAL] LOSS : 0.02812148630619049 [SCORE] : 1.0\n",
      "[578/1000]\n",
      "- [TRAIN] LOSS : 0.053769416098172464 [SCORE] : 0.5755908330281575\n",
      "[578/1000]\n",
      "- [VAL] LOSS : 0.02811623550951481 [SCORE] : 1.0\n",
      "[579/1000]\n",
      "- [TRAIN] LOSS : 0.05376095737641056 [SCORE] : 0.5755908330281575\n",
      "[579/1000]\n",
      "- [VAL] LOSS : 0.028112148866057396 [SCORE] : 1.0\n",
      "[580/1000]\n",
      "- [TRAIN] LOSS : 0.053758791120102005 [SCORE] : 0.5755908330281575\n",
      "[580/1000]\n",
      "- [VAL] LOSS : 0.02810739353299141 [SCORE] : 1.0\n",
      "[581/1000]\n",
      "- [TRAIN] LOSS : 0.053755648744603 [SCORE] : 0.5755908330281575\n",
      "[581/1000]\n",
      "- [VAL] LOSS : 0.02810215950012207 [SCORE] : 1.0\n",
      "[582/1000]\n",
      "- [TRAIN] LOSS : 0.05374918229257067 [SCORE] : 0.5755908330281575\n",
      "[582/1000]\n",
      "- [VAL] LOSS : 0.028097378090023994 [SCORE] : 1.0\n",
      "[583/1000]\n",
      "- [TRAIN] LOSS : 0.053743786194051305 [SCORE] : 0.5755908330281575\n",
      "[583/1000]\n",
      "- [VAL] LOSS : 0.028092727065086365 [SCORE] : 1.0\n",
      "[584/1000]\n",
      "- [TRAIN] LOSS : 0.05373947668510179 [SCORE] : 0.5755908330281575\n",
      "[584/1000]\n",
      "- [VAL] LOSS : 0.028088074177503586 [SCORE] : 1.0\n",
      "[585/1000]\n",
      "- [TRAIN] LOSS : 0.053736055409535764 [SCORE] : 0.5755908330281575\n",
      "[585/1000]\n",
      "- [VAL] LOSS : 0.02808322198688984 [SCORE] : 1.0\n",
      "[586/1000]\n",
      "- [TRAIN] LOSS : 0.05372880422510207 [SCORE] : 0.5755908330281575\n",
      "[586/1000]\n",
      "- [VAL] LOSS : 0.028079265728592873 [SCORE] : 1.0\n",
      "[587/1000]\n",
      "- [TRAIN] LOSS : 0.05372753664851189 [SCORE] : 0.5755908330281575\n",
      "[587/1000]\n",
      "- [VAL] LOSS : 0.028074489906430244 [SCORE] : 1.0\n",
      "[588/1000]\n",
      "- [TRAIN] LOSS : 0.05372201983506481 [SCORE] : 0.5755908330281575\n",
      "[588/1000]\n",
      "- [VAL] LOSS : 0.02806974947452545 [SCORE] : 1.0\n",
      "[589/1000]\n",
      "- [TRAIN] LOSS : 0.05371657974707584 [SCORE] : 0.5755908330281575\n",
      "[589/1000]\n",
      "- [VAL] LOSS : 0.02806526981294155 [SCORE] : 1.0\n",
      "[590/1000]\n",
      "- [TRAIN] LOSS : 0.05371114757532875 [SCORE] : 0.5755908330281575\n",
      "[590/1000]\n",
      "- [VAL] LOSS : 0.028061311691999435 [SCORE] : 1.0\n",
      "[591/1000]\n",
      "- [TRAIN] LOSS : 0.053709131696571905 [SCORE] : 0.5755908330281575\n",
      "[591/1000]\n",
      "- [VAL] LOSS : 0.028056666254997253 [SCORE] : 1.0\n",
      "[592/1000]\n",
      "- [TRAIN] LOSS : 0.05370350858817498 [SCORE] : 0.5755908330281575\n",
      "[592/1000]\n",
      "- [VAL] LOSS : 0.028052205219864845 [SCORE] : 1.0\n",
      "[593/1000]\n",
      "- [TRAIN] LOSS : 0.053698244799549384 [SCORE] : 0.5755908330281575\n",
      "[593/1000]\n",
      "- [VAL] LOSS : 0.02804785780608654 [SCORE] : 1.0\n",
      "[594/1000]\n",
      "- [TRAIN] LOSS : 0.05369380709404747 [SCORE] : 0.5755908330281575\n",
      "[594/1000]\n",
      "- [VAL] LOSS : 0.02804354764521122 [SCORE] : 1.0\n",
      "[595/1000]\n",
      "- [TRAIN] LOSS : 0.053690412050733965 [SCORE] : 0.5755908330281575\n",
      "[595/1000]\n",
      "- [VAL] LOSS : 0.02803904004395008 [SCORE] : 1.0\n",
      "[596/1000]\n",
      "- [TRAIN] LOSS : 0.05368332262150943 [SCORE] : 0.5755908330281575\n",
      "[596/1000]\n",
      "- [VAL] LOSS : 0.028035402297973633 [SCORE] : 1.0\n",
      "[597/1000]\n",
      "- [TRAIN] LOSS : 0.05368219711817801 [SCORE] : 0.5755908330281575\n",
      "[597/1000]\n",
      "- [VAL] LOSS : 0.028031041845679283 [SCORE] : 1.0\n",
      "[598/1000]\n",
      "- [TRAIN] LOSS : 0.05367688521121939 [SCORE] : 0.5755908330281575\n",
      "[598/1000]\n",
      "- [VAL] LOSS : 0.0280266422778368 [SCORE] : 1.0\n",
      "[599/1000]\n",
      "- [TRAIN] LOSS : 0.0536715736767898 [SCORE] : 0.5755908330281575\n",
      "[599/1000]\n",
      "- [VAL] LOSS : 0.028022458776831627 [SCORE] : 1.0\n",
      "[600/1000]\n",
      "- [TRAIN] LOSS : 0.05366700504285594 [SCORE] : 0.5755908330281575\n",
      "[600/1000]\n",
      "- [VAL] LOSS : 0.028018338605761528 [SCORE] : 1.0\n",
      "[601/1000]\n",
      "- [TRAIN] LOSS : 0.05366360709692041 [SCORE] : 0.5755908330281575\n",
      "[601/1000]\n",
      "- [VAL] LOSS : 0.028014089912176132 [SCORE] : 1.0\n",
      "[602/1000]\n",
      "- [TRAIN] LOSS : 0.05365671472003063 [SCORE] : 0.5755908330281575\n",
      "[602/1000]\n",
      "- [VAL] LOSS : 0.028010640293359756 [SCORE] : 1.0\n",
      "[603/1000]\n",
      "- [TRAIN] LOSS : 0.05365568079675238 [SCORE] : 0.5755908330281575\n",
      "[603/1000]\n",
      "- [VAL] LOSS : 0.028006460517644882 [SCORE] : 1.0\n",
      "[604/1000]\n",
      "- [TRAIN] LOSS : 0.05365045067543785 [SCORE] : 0.5755908330281575\n",
      "[604/1000]\n",
      "- [VAL] LOSS : 0.02800229750573635 [SCORE] : 1.0\n",
      "[605/1000]\n",
      "- [TRAIN] LOSS : 0.05364519578094284 [SCORE] : 0.5755908330281575\n",
      "[605/1000]\n",
      "- [VAL] LOSS : 0.027998264878988266 [SCORE] : 1.0\n",
      "[606/1000]\n",
      "- [TRAIN] LOSS : 0.05364068280905485 [SCORE] : 0.5755908330281575\n",
      "[606/1000]\n",
      "- [VAL] LOSS : 0.027994370087981224 [SCORE] : 1.0\n",
      "[607/1000]\n",
      "- [TRAIN] LOSS : 0.05363740228737394 [SCORE] : 0.5755908330281575\n",
      "[607/1000]\n",
      "- [VAL] LOSS : 0.027990326285362244 [SCORE] : 1.0\n",
      "[608/1000]\n",
      "- [TRAIN] LOSS : 0.05363206130762895 [SCORE] : 0.5755908330281575\n",
      "[608/1000]\n",
      "- [VAL] LOSS : 0.027986424043774605 [SCORE] : 1.0\n",
      "[609/1000]\n",
      "- [TRAIN] LOSS : 0.053627602228273945 [SCORE] : 0.5755908330281575\n",
      "[609/1000]\n",
      "- [VAL] LOSS : 0.02798263356089592 [SCORE] : 1.0\n",
      "[610/1000]\n",
      "- [TRAIN] LOSS : 0.053624460473656654 [SCORE] : 0.5755908330281575\n",
      "[610/1000]\n",
      "- [VAL] LOSS : 0.02797873504459858 [SCORE] : 1.0\n",
      "[611/1000]\n",
      "- [TRAIN] LOSS : 0.05361768238556882 [SCORE] : 0.5755908330281575\n",
      "[611/1000]\n",
      "- [VAL] LOSS : 0.02797546796500683 [SCORE] : 1.0\n",
      "[612/1000]\n",
      "- [TRAIN] LOSS : 0.05361673735703031 [SCORE] : 0.5755908330281575\n",
      "[612/1000]\n",
      "- [VAL] LOSS : 0.02797156386077404 [SCORE] : 1.0\n",
      "[613/1000]\n",
      "- [TRAIN] LOSS : 0.05361162160212795 [SCORE] : 0.5755908330281575\n",
      "[613/1000]\n",
      "- [VAL] LOSS : 0.027967678382992744 [SCORE] : 1.0\n",
      "[614/1000]\n",
      "- [TRAIN] LOSS : 0.05360648746912678 [SCORE] : 0.5755908330281575\n",
      "[614/1000]\n",
      "- [VAL] LOSS : 0.02796401083469391 [SCORE] : 1.0\n",
      "[615/1000]\n",
      "- [TRAIN] LOSS : 0.05360209532082081 [SCORE] : 0.5755908330281575\n",
      "[615/1000]\n",
      "- [VAL] LOSS : 0.027960356324911118 [SCORE] : 1.0\n",
      "[616/1000]\n",
      "- [TRAIN] LOSS : 0.05359889166429639 [SCORE] : 0.5755908330281575\n",
      "[616/1000]\n",
      "- [VAL] LOSS : 0.027956586331129074 [SCORE] : 1.0\n",
      "[617/1000]\n",
      "- [TRAIN] LOSS : 0.053593664150685075 [SCORE] : 0.5755908330281575\n",
      "[617/1000]\n",
      "- [VAL] LOSS : 0.027952980250120163 [SCORE] : 1.0\n",
      "[618/1000]\n",
      "- [TRAIN] LOSS : 0.05358936783547203 [SCORE] : 0.5755908330281575\n",
      "[618/1000]\n",
      "- [VAL] LOSS : 0.02794945240020752 [SCORE] : 1.0\n",
      "[619/1000]\n",
      "- [TRAIN] LOSS : 0.05358628688069681 [SCORE] : 0.5755908330281575\n",
      "[619/1000]\n",
      "- [VAL] LOSS : 0.027945777401328087 [SCORE] : 1.0\n",
      "[620/1000]\n",
      "- [TRAIN] LOSS : 0.053581155945236485 [SCORE] : 0.5755908330281575\n",
      "[620/1000]\n",
      "- [VAL] LOSS : 0.027942245826125145 [SCORE] : 1.0\n",
      "[621/1000]\n",
      "- [TRAIN] LOSS : 0.05357681857421994 [SCORE] : 0.5755908330281575\n",
      "[621/1000]\n",
      "- [VAL] LOSS : 0.02793879620730877 [SCORE] : 1.0\n",
      "[622/1000]\n",
      "- [TRAIN] LOSS : 0.05357378176413476 [SCORE] : 0.5755908330281575\n",
      "[622/1000]\n",
      "- [VAL] LOSS : 0.02793523482978344 [SCORE] : 1.0\n",
      "[623/1000]\n",
      "- [TRAIN] LOSS : 0.05356722317325572 [SCORE] : 0.5755908330281575\n",
      "[623/1000]\n",
      "- [VAL] LOSS : 0.027932362630963326 [SCORE] : 1.0\n",
      "[624/1000]\n",
      "- [TRAIN] LOSS : 0.053566308490311104 [SCORE] : 0.5755908330281575\n",
      "[624/1000]\n",
      "- [VAL] LOSS : 0.02792881429195404 [SCORE] : 1.0\n",
      "[625/1000]\n",
      "- [TRAIN] LOSS : 0.05356133187500139 [SCORE] : 0.5755908330281575\n",
      "[625/1000]\n",
      "- [VAL] LOSS : 0.027925293892621994 [SCORE] : 1.0\n",
      "[626/1000]\n",
      "- [TRAIN] LOSS : 0.053556376090273264 [SCORE] : 0.5755908330281575\n",
      "[626/1000]\n",
      "- [VAL] LOSS : 0.02792198769748211 [SCORE] : 1.0\n",
      "[627/1000]\n",
      "- [TRAIN] LOSS : 0.053552148522188266 [SCORE] : 0.5755908330281575\n",
      "[627/1000]\n",
      "- [VAL] LOSS : 0.02791866846382618 [SCORE] : 1.0\n",
      "[628/1000]\n",
      "- [TRAIN] LOSS : 0.05354906319019695 [SCORE] : 0.5755908330281575\n",
      "[628/1000]\n",
      "- [VAL] LOSS : 0.0279152262955904 [SCORE] : 1.0\n",
      "[629/1000]\n",
      "- [TRAIN] LOSS : 0.05354398496759435 [SCORE] : 0.5755908330281575\n",
      "[629/1000]\n",
      "- [VAL] LOSS : 0.027911989018321037 [SCORE] : 1.0\n",
      "[630/1000]\n",
      "- [TRAIN] LOSS : 0.05353976835807164 [SCORE] : 0.5755908330281575\n",
      "[630/1000]\n",
      "- [VAL] LOSS : 0.02790875919163227 [SCORE] : 1.0\n",
      "[631/1000]\n",
      "- [TRAIN] LOSS : 0.053536851952473326 [SCORE] : 0.5755908330281575\n",
      "[631/1000]\n",
      "- [VAL] LOSS : 0.02790548838675022 [SCORE] : 1.0\n",
      "[632/1000]\n",
      "- [TRAIN] LOSS : 0.05353187870544692 [SCORE] : 0.5755908330281575\n",
      "[632/1000]\n",
      "- [VAL] LOSS : 0.02790229022502899 [SCORE] : 1.0\n",
      "[633/1000]\n",
      "- [TRAIN] LOSS : 0.05352768469601869 [SCORE] : 0.5755908330281575\n",
      "[633/1000]\n",
      "- [VAL] LOSS : 0.02789916656911373 [SCORE] : 1.0\n",
      "[634/1000]\n",
      "- [TRAIN] LOSS : 0.053524006406466165 [SCORE] : 0.5755908330281575\n",
      "[634/1000]\n",
      "- [VAL] LOSS : 0.027896031737327576 [SCORE] : 1.0\n",
      "[635/1000]\n",
      "- [TRAIN] LOSS : 0.05352098901445667 [SCORE] : 0.5755908330281575\n",
      "[635/1000]\n",
      "- [VAL] LOSS : 0.02789275161921978 [SCORE] : 1.0\n",
      "[636/1000]\n",
      "- [TRAIN] LOSS : 0.05351577495845656 [SCORE] : 0.5755908330281575\n",
      "[636/1000]\n",
      "- [VAL] LOSS : 0.02788965404033661 [SCORE] : 1.0\n",
      "[637/1000]\n",
      "- [TRAIN] LOSS : 0.05351154195765654 [SCORE] : 0.5755908330281575\n",
      "[637/1000]\n",
      "- [VAL] LOSS : 0.02788669429719448 [SCORE] : 1.0\n",
      "[638/1000]\n",
      "- [TRAIN] LOSS : 0.053508716216310856 [SCORE] : 0.5755908330281575\n",
      "[638/1000]\n",
      "- [VAL] LOSS : 0.02788354828953743 [SCORE] : 1.0\n",
      "[639/1000]\n",
      "- [TRAIN] LOSS : 0.05350383262460431 [SCORE] : 0.5755908330281575\n",
      "[639/1000]\n",
      "- [VAL] LOSS : 0.02788059413433075 [SCORE] : 1.0\n",
      "[640/1000]\n",
      "- [TRAIN] LOSS : 0.053499754735579094 [SCORE] : 0.5755908330281575\n",
      "[640/1000]\n",
      "- [VAL] LOSS : 0.027877643704414368 [SCORE] : 1.0\n",
      "[641/1000]\n",
      "- [TRAIN] LOSS : 0.05349614036579927 [SCORE] : 0.5755908330281575\n",
      "[641/1000]\n",
      "- [VAL] LOSS : 0.027874667197465897 [SCORE] : 1.0\n",
      "[642/1000]\n",
      "- [TRAIN] LOSS : 0.05349314614819984 [SCORE] : 0.5755908330281575\n",
      "[642/1000]\n",
      "- [VAL] LOSS : 0.027871590107679367 [SCORE] : 1.0\n",
      "[643/1000]\n",
      "- [TRAIN] LOSS : 0.05348803581049045 [SCORE] : 0.5755908330281575\n",
      "[643/1000]\n",
      "- [VAL] LOSS : 0.02786870114505291 [SCORE] : 1.0\n",
      "[644/1000]\n",
      "- [TRAIN] LOSS : 0.05348389026088019 [SCORE] : 0.5755908330281575\n",
      "[644/1000]\n",
      "- [VAL] LOSS : 0.027865847572684288 [SCORE] : 1.0\n",
      "[645/1000]\n",
      "- [TRAIN] LOSS : 0.05348114504789313 [SCORE] : 0.5755908330281575\n",
      "[645/1000]\n",
      "- [VAL] LOSS : 0.027862951159477234 [SCORE] : 1.0\n",
      "[646/1000]\n",
      "- [TRAIN] LOSS : 0.05347630763426423 [SCORE] : 0.5755908330281575\n",
      "[646/1000]\n",
      "- [VAL] LOSS : 0.027860119938850403 [SCORE] : 1.0\n",
      "[647/1000]\n",
      "- [TRAIN] LOSS : 0.053472286105776826 [SCORE] : 0.5755908330281575\n",
      "[647/1000]\n",
      "- [VAL] LOSS : 0.027857335284352303 [SCORE] : 1.0\n",
      "[648/1000]\n",
      "- [TRAIN] LOSS : 0.05346873028514286 [SCORE] : 0.5755908330281575\n",
      "[648/1000]\n",
      "- [VAL] LOSS : 0.027854571118950844 [SCORE] : 1.0\n",
      "[649/1000]\n",
      "- [TRAIN] LOSS : 0.05346581206346552 [SCORE] : 0.5755908330281575\n",
      "[649/1000]\n",
      "- [VAL] LOSS : 0.027851652354002 [SCORE] : 1.0\n",
      "[650/1000]\n",
      "- [TRAIN] LOSS : 0.053460754066084824 [SCORE] : 0.5755908330281575\n",
      "[650/1000]\n",
      "- [VAL] LOSS : 0.027848944067955017 [SCORE] : 1.0\n",
      "[651/1000]\n",
      "- [TRAIN] LOSS : 0.05345672396942973 [SCORE] : 0.5755908330281575\n",
      "[651/1000]\n",
      "- [VAL] LOSS : 0.027846312150359154 [SCORE] : 1.0\n",
      "[652/1000]\n",
      "- [TRAIN] LOSS : 0.05345398873711626 [SCORE] : 0.5755908330281575\n",
      "[652/1000]\n",
      "- [VAL] LOSS : 0.027843525633215904 [SCORE] : 1.0\n",
      "[653/1000]\n",
      "- [TRAIN] LOSS : 0.05344923539087176 [SCORE] : 0.5755908330281575\n",
      "[653/1000]\n",
      "- [VAL] LOSS : 0.027840863913297653 [SCORE] : 1.0\n",
      "[654/1000]\n",
      "- [TRAIN] LOSS : 0.05344534986652434 [SCORE] : 0.5755908330281575\n",
      "[654/1000]\n",
      "- [VAL] LOSS : 0.02783825993537903 [SCORE] : 1.0\n",
      "[655/1000]\n",
      "- [TRAIN] LOSS : 0.05344180460087955 [SCORE] : 0.5755908330281575\n",
      "[655/1000]\n",
      "- [VAL] LOSS : 0.02783566154539585 [SCORE] : 1.0\n",
      "[656/1000]\n",
      "- [TRAIN] LOSS : 0.05343899264310797 [SCORE] : 0.5755908330281575\n",
      "[656/1000]\n",
      "- [VAL] LOSS : 0.02783294953405857 [SCORE] : 1.0\n",
      "[657/1000]\n",
      "- [TRAIN] LOSS : 0.05343398324524363 [SCORE] : 0.5755908330281575\n",
      "[657/1000]\n",
      "- [VAL] LOSS : 0.027830393984913826 [SCORE] : 1.0\n",
      "[658/1000]\n",
      "- [TRAIN] LOSS : 0.05343002763887247 [SCORE] : 0.5755908330281575\n",
      "[658/1000]\n",
      "- [VAL] LOSS : 0.027827871963381767 [SCORE] : 1.0\n",
      "[659/1000]\n",
      "- [TRAIN] LOSS : 0.05342661887407303 [SCORE] : 0.5755908330281575\n",
      "[659/1000]\n",
      "- [VAL] LOSS : 0.02782539650797844 [SCORE] : 1.0\n",
      "[660/1000]\n",
      "- [TRAIN] LOSS : 0.05342384879477322 [SCORE] : 0.5755908330281575\n",
      "[660/1000]\n",
      "- [VAL] LOSS : 0.02782272920012474 [SCORE] : 1.0\n",
      "[661/1000]\n",
      "- [TRAIN] LOSS : 0.053418949836244185 [SCORE] : 0.5755908330281575\n",
      "[661/1000]\n",
      "- [VAL] LOSS : 0.027820292860269547 [SCORE] : 1.0\n",
      "[662/1000]\n",
      "- [TRAIN] LOSS : 0.053414963620404404 [SCORE] : 0.5755908330281575\n",
      "[662/1000]\n",
      "- [VAL] LOSS : 0.027817877009510994 [SCORE] : 1.0\n",
      "[663/1000]\n",
      "- [TRAIN] LOSS : 0.05341230249032378 [SCORE] : 0.5755908330281575\n",
      "[663/1000]\n",
      "- [VAL] LOSS : 0.02781539224088192 [SCORE] : 1.0\n",
      "[664/1000]\n",
      "- [TRAIN] LOSS : 0.05340768176441391 [SCORE] : 0.5755908330281575\n",
      "[664/1000]\n",
      "- [VAL] LOSS : 0.027812981978058815 [SCORE] : 1.0\n",
      "[665/1000]\n",
      "- [TRAIN] LOSS : 0.053403836706032354 [SCORE] : 0.5755908330281575\n",
      "[665/1000]\n",
      "- [VAL] LOSS : 0.027810633182525635 [SCORE] : 1.0\n",
      "[666/1000]\n",
      "- [TRAIN] LOSS : 0.053400437875340385 [SCORE] : 0.5755908330281575\n",
      "[666/1000]\n",
      "- [VAL] LOSS : 0.027808289974927902 [SCORE] : 1.0\n",
      "[667/1000]\n",
      "- [TRAIN] LOSS : 0.05339771326010426 [SCORE] : 0.5755908330281575\n",
      "[667/1000]\n",
      "- [VAL] LOSS : 0.02780580334365368 [SCORE] : 1.0\n",
      "[668/1000]\n",
      "- [TRAIN] LOSS : 0.053392854798585174 [SCORE] : 0.5755908330281575\n",
      "[668/1000]\n",
      "- [VAL] LOSS : 0.027803514152765274 [SCORE] : 1.0\n",
      "[669/1000]\n",
      "- [TRAIN] LOSS : 0.053388948335001865 [SCORE] : 0.5755908330281575\n",
      "[669/1000]\n",
      "- [VAL] LOSS : 0.027801254764199257 [SCORE] : 1.0\n",
      "[670/1000]\n",
      "- [TRAIN] LOSS : 0.05338562472412984 [SCORE] : 0.5755908330281575\n",
      "[670/1000]\n",
      "- [VAL] LOSS : 0.027798982337117195 [SCORE] : 1.0\n",
      "[671/1000]\n",
      "- [TRAIN] LOSS : 0.05338298023367922 [SCORE] : 0.5755908330281575\n",
      "[671/1000]\n",
      "- [VAL] LOSS : 0.02779662236571312 [SCORE] : 1.0\n",
      "[672/1000]\n",
      "- [TRAIN] LOSS : 0.053378139591465396 [SCORE] : 0.5755908330281575\n",
      "[672/1000]\n",
      "- [VAL] LOSS : 0.027794379740953445 [SCORE] : 1.0\n",
      "[673/1000]\n",
      "- [TRAIN] LOSS : 0.05337431367176274 [SCORE] : 0.5755908330281575\n",
      "[673/1000]\n",
      "- [VAL] LOSS : 0.02779221534729004 [SCORE] : 1.0\n",
      "[674/1000]\n",
      "- [TRAIN] LOSS : 0.05337098464369774 [SCORE] : 0.5755908330281575\n",
      "[674/1000]\n",
      "- [VAL] LOSS : 0.02779005467891693 [SCORE] : 1.0\n",
      "[675/1000]\n",
      "- [TRAIN] LOSS : 0.053368375481416784 [SCORE] : 0.5755908330281575\n",
      "[675/1000]\n",
      "- [VAL] LOSS : 0.027787763625383377 [SCORE] : 1.0\n",
      "[676/1000]\n",
      "- [TRAIN] LOSS : 0.05336359160331388 [SCORE] : 0.5755908330281575\n",
      "[676/1000]\n",
      "- [VAL] LOSS : 0.027785610407590866 [SCORE] : 1.0\n",
      "[677/1000]\n",
      "- [TRAIN] LOSS : 0.05335973522936304 [SCORE] : 0.5755908330281575\n",
      "[677/1000]\n",
      "- [VAL] LOSS : 0.027783537283539772 [SCORE] : 1.0\n",
      "[678/1000]\n",
      "- [TRAIN] LOSS : 0.053356481591860454 [SCORE] : 0.5755908330281575\n",
      "[678/1000]\n",
      "- [VAL] LOSS : 0.02778145670890808 [SCORE] : 1.0\n",
      "[679/1000]\n",
      "- [TRAIN] LOSS : 0.05335395028814673 [SCORE] : 0.5755908330281575\n",
      "[679/1000]\n",
      "- [VAL] LOSS : 0.0277792327105999 [SCORE] : 1.0\n",
      "[680/1000]\n",
      "- [TRAIN] LOSS : 0.05334916369368633 [SCORE] : 0.5755908330281575\n",
      "[680/1000]\n",
      "- [VAL] LOSS : 0.02777717448771 [SCORE] : 1.0\n",
      "[681/1000]\n",
      "- [TRAIN] LOSS : 0.05334532493725419 [SCORE] : 0.5755908330281575\n",
      "[681/1000]\n",
      "- [VAL] LOSS : 0.027775201946496964 [SCORE] : 1.0\n",
      "[682/1000]\n",
      "- [TRAIN] LOSS : 0.05334212401260932 [SCORE] : 0.5755908330281575\n",
      "[682/1000]\n",
      "- [VAL] LOSS : 0.02777319774031639 [SCORE] : 1.0\n",
      "[683/1000]\n",
      "- [TRAIN] LOSS : 0.05333957577434679 [SCORE] : 0.5755908330281575\n",
      "[683/1000]\n",
      "- [VAL] LOSS : 0.027771037071943283 [SCORE] : 1.0\n",
      "[684/1000]\n",
      "- [TRAIN] LOSS : 0.0533348446401457 [SCORE] : 0.5755908330281575\n",
      "[684/1000]\n",
      "- [VAL] LOSS : 0.027769088745117188 [SCORE] : 1.0\n",
      "[685/1000]\n",
      "- [TRAIN] LOSS : 0.05333107113838196 [SCORE] : 0.5755908330281575\n",
      "[685/1000]\n",
      "- [VAL] LOSS : 0.027767138555645943 [SCORE] : 1.0\n",
      "[686/1000]\n",
      "- [TRAIN] LOSS : 0.053327846170092626 [SCORE] : 0.5755908330281575\n",
      "[686/1000]\n",
      "- [VAL] LOSS : 0.027765188366174698 [SCORE] : 1.0\n",
      "[687/1000]\n",
      "- [TRAIN] LOSS : 0.05332536846399307 [SCORE] : 0.5755908330281575\n",
      "[687/1000]\n",
      "- [VAL] LOSS : 0.0277631264179945 [SCORE] : 1.0\n",
      "[688/1000]\n",
      "- [TRAIN] LOSS : 0.05332068013958633 [SCORE] : 0.5755908330281575\n",
      "[688/1000]\n",
      "- [VAL] LOSS : 0.027761289849877357 [SCORE] : 1.0\n",
      "[689/1000]\n",
      "- [TRAIN] LOSS : 0.05331692818241815 [SCORE] : 0.5755908330281575\n",
      "[689/1000]\n",
      "- [VAL] LOSS : 0.027759451419115067 [SCORE] : 1.0\n",
      "[690/1000]\n",
      "- [TRAIN] LOSS : 0.053313751115153234 [SCORE] : 0.5755908330281575\n",
      "[690/1000]\n",
      "- [VAL] LOSS : 0.02775760553777218 [SCORE] : 1.0\n",
      "[691/1000]\n",
      "- [TRAIN] LOSS : 0.05331126957510909 [SCORE] : 0.5755908330281575\n",
      "[691/1000]\n",
      "- [VAL] LOSS : 0.02775561437010765 [SCORE] : 1.0\n",
      "[692/1000]\n",
      "- [TRAIN] LOSS : 0.05330662958634396 [SCORE] : 0.5755908330281575\n",
      "[692/1000]\n",
      "- [VAL] LOSS : 0.02775380201637745 [SCORE] : 1.0\n",
      "[693/1000]\n",
      "- [TRAIN] LOSS : 0.0533028968454649 [SCORE] : 0.5755908330281575\n",
      "[693/1000]\n",
      "- [VAL] LOSS : 0.027752026915550232 [SCORE] : 1.0\n",
      "[694/1000]\n",
      "- [TRAIN] LOSS : 0.0532997521571815 [SCORE] : 0.5755908330281575\n",
      "[694/1000]\n",
      "- [VAL] LOSS : 0.027750289067626 [SCORE] : 1.0\n",
      "[695/1000]\n",
      "- [TRAIN] LOSS : 0.05329731761788328 [SCORE] : 0.5755908330281575\n",
      "[695/1000]\n",
      "- [VAL] LOSS : 0.02774840220808983 [SCORE] : 1.0\n",
      "[696/1000]\n",
      "- [TRAIN] LOSS : 0.05329268014368912 [SCORE] : 0.5755908330281575\n",
      "[696/1000]\n",
      "- [VAL] LOSS : 0.02774662896990776 [SCORE] : 1.0\n",
      "[697/1000]\n",
      "- [TRAIN] LOSS : 0.053289010825877385 [SCORE] : 0.5755908330281575\n",
      "[697/1000]\n",
      "- [VAL] LOSS : 0.0277449581772089 [SCORE] : 1.0\n",
      "[698/1000]\n",
      "- [TRAIN] LOSS : 0.05328587998325626 [SCORE] : 0.5755908330281575\n",
      "[698/1000]\n",
      "- [VAL] LOSS : 0.02774323895573616 [SCORE] : 1.0\n",
      "[699/1000]\n",
      "- [TRAIN] LOSS : 0.05328344157896936 [SCORE] : 0.5755908330281575\n",
      "[699/1000]\n",
      "- [VAL] LOSS : 0.02774142660200596 [SCORE] : 1.0\n",
      "[700/1000]\n",
      "- [TRAIN] LOSS : 0.05327886880065004 [SCORE] : 0.5755908330281575\n",
      "[700/1000]\n",
      "- [VAL] LOSS : 0.027739737182855606 [SCORE] : 1.0\n",
      "[701/1000]\n",
      "- [TRAIN] LOSS : 0.05327522771743436 [SCORE] : 0.5755908330281575\n",
      "[701/1000]\n",
      "- [VAL] LOSS : 0.027738139033317566 [SCORE] : 1.0\n",
      "[702/1000]\n",
      "- [TRAIN] LOSS : 0.053272108640521766 [SCORE] : 0.5755908330281575\n",
      "[702/1000]\n",
      "- [VAL] LOSS : 0.027736539021134377 [SCORE] : 1.0\n",
      "[703/1000]\n",
      "- [TRAIN] LOSS : 0.053269704710692166 [SCORE] : 0.5755908330281575\n",
      "[703/1000]\n",
      "- [VAL] LOSS : 0.027734803035855293 [SCORE] : 1.0\n",
      "[704/1000]\n",
      "- [TRAIN] LOSS : 0.05326516435792049 [SCORE] : 0.5755908330281575\n",
      "[704/1000]\n",
      "- [VAL] LOSS : 0.027733171358704567 [SCORE] : 1.0\n",
      "[705/1000]\n",
      "- [TRAIN] LOSS : 0.05326156889398893 [SCORE] : 0.5755908330281575\n",
      "[705/1000]\n",
      "- [VAL] LOSS : 0.027731653302907944 [SCORE] : 1.0\n",
      "[706/1000]\n",
      "- [TRAIN] LOSS : 0.053258492285385726 [SCORE] : 0.5755908330281575\n",
      "[706/1000]\n",
      "- [VAL] LOSS : 0.027730051428079605 [SCORE] : 1.0\n",
      "[707/1000]\n",
      "- [TRAIN] LOSS : 0.05325612056379517 [SCORE] : 0.5755908330281575\n",
      "[707/1000]\n",
      "- [VAL] LOSS : 0.02772841788828373 [SCORE] : 1.0\n",
      "[708/1000]\n",
      "- [TRAIN] LOSS : 0.05325158011789123 [SCORE] : 0.5755908330281575\n",
      "[708/1000]\n",
      "- [VAL] LOSS : 0.027726858854293823 [SCORE] : 1.0\n",
      "[709/1000]\n",
      "- [TRAIN] LOSS : 0.05324799980347355 [SCORE] : 0.5755908330281575\n",
      "[709/1000]\n",
      "- [VAL] LOSS : 0.027725396677851677 [SCORE] : 1.0\n",
      "[710/1000]\n",
      "- [TRAIN] LOSS : 0.053244950529187915 [SCORE] : 0.5755908330281575\n",
      "[710/1000]\n",
      "- [VAL] LOSS : 0.02772391401231289 [SCORE] : 1.0\n",
      "[711/1000]\n",
      "- [TRAIN] LOSS : 0.05324264426405231 [SCORE] : 0.5755908330281575\n",
      "[711/1000]\n",
      "- [VAL] LOSS : 0.027722299098968506 [SCORE] : 1.0\n",
      "[712/1000]\n",
      "- [TRAIN] LOSS : 0.05323814117970566 [SCORE] : 0.5755908330281575\n",
      "[712/1000]\n",
      "- [VAL] LOSS : 0.027720851823687553 [SCORE] : 1.0\n",
      "[713/1000]\n",
      "- [TRAIN] LOSS : 0.053234566127260526 [SCORE] : 0.5755908330281575\n",
      "[713/1000]\n",
      "- [VAL] LOSS : 0.02771945297718048 [SCORE] : 1.0\n",
      "[714/1000]\n",
      "- [TRAIN] LOSS : 0.053231552600239714 [SCORE] : 0.5755908330281575\n",
      "[714/1000]\n",
      "- [VAL] LOSS : 0.02771800570189953 [SCORE] : 1.0\n",
      "[715/1000]\n",
      "- [TRAIN] LOSS : 0.05322846995356182 [SCORE] : 0.5755908330281575\n",
      "[715/1000]\n",
      "- [VAL] LOSS : 0.02771652676165104 [SCORE] : 1.0\n",
      "[716/1000]\n",
      "- [TRAIN] LOSS : 0.05322595657780767 [SCORE] : 0.5755908330281575\n",
      "[716/1000]\n",
      "- [VAL] LOSS : 0.02771507389843464 [SCORE] : 1.0\n",
      "[717/1000]\n",
      "- [TRAIN] LOSS : 0.0532213869659851 [SCORE] : 0.5755908330281575\n",
      "[717/1000]\n",
      "- [VAL] LOSS : 0.027713680639863014 [SCORE] : 1.0\n",
      "[718/1000]\n",
      "- [TRAIN] LOSS : 0.0532178415140758 [SCORE] : 0.5755908330281575\n",
      "[718/1000]\n",
      "- [VAL] LOSS : 0.027712354436516762 [SCORE] : 1.0\n",
      "[719/1000]\n",
      "- [TRAIN] LOSS : 0.05321492200406889 [SCORE] : 0.5755908330281575\n",
      "[719/1000]\n",
      "- [VAL] LOSS : 0.027711009606719017 [SCORE] : 1.0\n",
      "[720/1000]\n",
      "- [TRAIN] LOSS : 0.05321265996123354 [SCORE] : 0.5755908330281575\n",
      "[720/1000]\n",
      "- [VAL] LOSS : 0.027709579095244408 [SCORE] : 1.0\n",
      "[721/1000]\n",
      "- [TRAIN] LOSS : 0.05320827133643131 [SCORE] : 0.5755908330281575\n",
      "[721/1000]\n",
      "- [VAL] LOSS : 0.0277082622051239 [SCORE] : 1.0\n",
      "[722/1000]\n",
      "- [TRAIN] LOSS : 0.05320474030449986 [SCORE] : 0.5755908330281575\n",
      "[722/1000]\n",
      "- [VAL] LOSS : 0.02770700864493847 [SCORE] : 1.0\n",
      "[723/1000]\n",
      "- [TRAIN] LOSS : 0.05320174841520687 [SCORE] : 0.5755908330281575\n",
      "[723/1000]\n",
      "- [VAL] LOSS : 0.02770574390888214 [SCORE] : 1.0\n",
      "[724/1000]\n",
      "- [TRAIN] LOSS : 0.05319873463983337 [SCORE] : 0.5755908330281575\n",
      "[724/1000]\n",
      "- [VAL] LOSS : 0.027704467996954918 [SCORE] : 1.0\n",
      "[725/1000]\n",
      "- [TRAIN] LOSS : 0.05319627885085841 [SCORE] : 0.5755908330281575\n",
      "[725/1000]\n",
      "- [VAL] LOSS : 0.027703147381544113 [SCORE] : 1.0\n",
      "[726/1000]\n",
      "- [TRAIN] LOSS : 0.05319180892159541 [SCORE] : 0.5755908330281575\n",
      "[726/1000]\n",
      "- [VAL] LOSS : 0.027701914310455322 [SCORE] : 1.0\n",
      "[727/1000]\n",
      "- [TRAIN] LOSS : 0.05318829685760041 [SCORE] : 0.5755908330281575\n",
      "[727/1000]\n",
      "- [VAL] LOSS : 0.027700738981366158 [SCORE] : 1.0\n",
      "[728/1000]\n",
      "- [TRAIN] LOSS : 0.05318545354530215 [SCORE] : 0.5755908330281575\n",
      "[728/1000]\n",
      "- [VAL] LOSS : 0.0276995487511158 [SCORE] : 1.0\n",
      "[729/1000]\n",
      "- [TRAIN] LOSS : 0.05318324468098581 [SCORE] : 0.5755908330281575\n",
      "[729/1000]\n",
      "- [VAL] LOSS : 0.027698274701833725 [SCORE] : 1.0\n",
      "[730/1000]\n",
      "- [TRAIN] LOSS : 0.05317887969625493 [SCORE] : 0.5755908330281575\n",
      "[730/1000]\n",
      "- [VAL] LOSS : 0.0276971273124218 [SCORE] : 1.0\n",
      "[731/1000]\n",
      "- [TRAIN] LOSS : 0.053175407958527404 [SCORE] : 0.5755908330281575\n",
      "[731/1000]\n",
      "- [VAL] LOSS : 0.02769600786268711 [SCORE] : 1.0\n",
      "[732/1000]\n",
      "- [TRAIN] LOSS : 0.05317247264708082 [SCORE] : 0.5755908330281575\n",
      "[732/1000]\n",
      "- [VAL] LOSS : 0.027694908902049065 [SCORE] : 1.0\n",
      "[733/1000]\n",
      "- [TRAIN] LOSS : 0.05316957663744688 [SCORE] : 0.5755908330281575\n",
      "[733/1000]\n",
      "- [VAL] LOSS : 0.027693774551153183 [SCORE] : 1.0\n",
      "[734/1000]\n",
      "- [TRAIN] LOSS : 0.053167175765459734 [SCORE] : 0.5755908330281575\n",
      "[734/1000]\n",
      "- [VAL] LOSS : 0.027692528441548347 [SCORE] : 1.0\n",
      "[735/1000]\n",
      "- [TRAIN] LOSS : 0.05316267500941952 [SCORE] : 0.5755908330281575\n",
      "[735/1000]\n",
      "- [VAL] LOSS : 0.027691492810845375 [SCORE] : 1.0\n",
      "[736/1000]\n",
      "- [TRAIN] LOSS : 0.0531592747506996 [SCORE] : 0.5755908330281575\n",
      "[736/1000]\n",
      "- [VAL] LOSS : 0.027690453454852104 [SCORE] : 1.0\n",
      "[737/1000]\n",
      "- [TRAIN] LOSS : 0.05315646414334575 [SCORE] : 0.5755908330281575\n",
      "[737/1000]\n",
      "- [VAL] LOSS : 0.027689408510923386 [SCORE] : 1.0\n",
      "[738/1000]\n",
      "- [TRAIN] LOSS : 0.05315354440050821 [SCORE] : 0.5755908330281575\n",
      "[738/1000]\n",
      "- [VAL] LOSS : 0.027688337489962578 [SCORE] : 1.0\n",
      "[739/1000]\n",
      "- [TRAIN] LOSS : 0.05315117614033322 [SCORE] : 0.5755908330281575\n",
      "[739/1000]\n",
      "- [VAL] LOSS : 0.02768719755113125 [SCORE] : 1.0\n",
      "[740/1000]\n",
      "- [TRAIN] LOSS : 0.05314673736381034 [SCORE] : 0.5755908330281575\n",
      "[740/1000]\n",
      "- [VAL] LOSS : 0.027686206623911858 [SCORE] : 1.0\n",
      "[741/1000]\n",
      "- [TRAIN] LOSS : 0.05314333757075171 [SCORE] : 0.5755908330281575\n",
      "[741/1000]\n",
      "- [VAL] LOSS : 0.027685221284627914 [SCORE] : 1.0\n",
      "[742/1000]\n",
      "- [TRAIN] LOSS : 0.05314058130607009 [SCORE] : 0.5755908330281575\n",
      "[742/1000]\n",
      "- [VAL] LOSS : 0.027684273198246956 [SCORE] : 1.0\n",
      "[743/1000]\n",
      "- [TRAIN] LOSS : 0.053138460793221994 [SCORE] : 0.5755908330281575\n",
      "[743/1000]\n",
      "- [VAL] LOSS : 0.027683192864060402 [SCORE] : 1.0\n",
      "[744/1000]\n",
      "- [TRAIN] LOSS : 0.0531341748777777 [SCORE] : 0.5755908330281575\n",
      "[744/1000]\n",
      "- [VAL] LOSS : 0.02768225036561489 [SCORE] : 1.0\n",
      "[745/1000]\n",
      "- [TRAIN] LOSS : 0.05313082030043006 [SCORE] : 0.5755908330281575\n",
      "[745/1000]\n",
      "- [VAL] LOSS : 0.02768131159245968 [SCORE] : 1.0\n",
      "[746/1000]\n",
      "- [TRAIN] LOSS : 0.05312796264576415 [SCORE] : 0.5755908330281575\n",
      "[746/1000]\n",
      "- [VAL] LOSS : 0.027680424973368645 [SCORE] : 1.0\n",
      "[747/1000]\n",
      "- [TRAIN] LOSS : 0.05312509775782625 [SCORE] : 0.5755908330281575\n",
      "[747/1000]\n",
      "- [VAL] LOSS : 0.02767948806285858 [SCORE] : 1.0\n",
      "[748/1000]\n",
      "- [TRAIN] LOSS : 0.053122786510114865 [SCORE] : 0.5755908330281575\n",
      "[748/1000]\n",
      "- [VAL] LOSS : 0.027678469195961952 [SCORE] : 1.0\n",
      "[749/1000]\n",
      "- [TRAIN] LOSS : 0.05311842582498987 [SCORE] : 0.5755908330281575\n",
      "[749/1000]\n",
      "- [VAL] LOSS : 0.027677565813064575 [SCORE] : 1.0\n",
      "[750/1000]\n",
      "- [TRAIN] LOSS : 0.053115088508153954 [SCORE] : 0.5755908330281575\n",
      "[750/1000]\n",
      "- [VAL] LOSS : 0.02767672762274742 [SCORE] : 1.0\n",
      "[751/1000]\n",
      "- [TRAIN] LOSS : 0.05311236693523824 [SCORE] : 0.5755908330281575\n",
      "[751/1000]\n",
      "- [VAL] LOSS : 0.027675913646817207 [SCORE] : 1.0\n",
      "[752/1000]\n",
      "- [TRAIN] LOSS : 0.053109545214101675 [SCORE] : 0.5755908330281575\n",
      "[752/1000]\n",
      "- [VAL] LOSS : 0.027675006538629532 [SCORE] : 1.0\n",
      "[753/1000]\n",
      "- [TRAIN] LOSS : 0.05310723256940643 [SCORE] : 0.5755908330281575\n",
      "[753/1000]\n",
      "- [VAL] LOSS : 0.02767406590282917 [SCORE] : 1.0\n",
      "[754/1000]\n",
      "- [TRAIN] LOSS : 0.05310287176010509 [SCORE] : 0.5755908330281575\n",
      "[754/1000]\n",
      "- [VAL] LOSS : 0.027673279866576195 [SCORE] : 1.0\n",
      "[755/1000]\n",
      "- [TRAIN] LOSS : 0.05309959067963064 [SCORE] : 0.5755908330281575\n",
      "[755/1000]\n",
      "- [VAL] LOSS : 0.027672501280903816 [SCORE] : 1.0\n",
      "[756/1000]\n",
      "- [TRAIN] LOSS : 0.05309686783390741 [SCORE] : 0.5755908330281575\n",
      "[756/1000]\n",
      "- [VAL] LOSS : 0.02767173759639263 [SCORE] : 1.0\n",
      "[757/1000]\n",
      "- [TRAIN] LOSS : 0.05309408800676465 [SCORE] : 0.5755908330281575\n",
      "[757/1000]\n",
      "- [VAL] LOSS : 0.027670903131365776 [SCORE] : 1.0\n",
      "[758/1000]\n",
      "- [TRAIN] LOSS : 0.053091809153556824 [SCORE] : 0.5755908330281575\n",
      "[758/1000]\n",
      "- [VAL] LOSS : 0.027670064941048622 [SCORE] : 1.0\n",
      "[759/1000]\n",
      "- [TRAIN] LOSS : 0.0530875051394105 [SCORE] : 0.5755908330281575\n",
      "[759/1000]\n",
      "- [VAL] LOSS : 0.027669286355376244 [SCORE] : 1.0\n",
      "[760/1000]\n",
      "- [TRAIN] LOSS : 0.05308418879285455 [SCORE] : 0.5755908330281575\n",
      "[760/1000]\n",
      "- [VAL] LOSS : 0.027668621391057968 [SCORE] : 1.0\n",
      "[761/1000]\n",
      "- [TRAIN] LOSS : 0.05308154805873831 [SCORE] : 0.5755908330281575\n",
      "[761/1000]\n",
      "- [VAL] LOSS : 0.027667850255966187 [SCORE] : 1.0\n",
      "[762/1000]\n",
      "- [TRAIN] LOSS : 0.05307878217039009 [SCORE] : 0.5755908330281575\n",
      "[762/1000]\n",
      "- [VAL] LOSS : 0.027667129412293434 [SCORE] : 1.0\n",
      "[763/1000]\n",
      "- [TRAIN] LOSS : 0.05307650407776236 [SCORE] : 0.5755908330281575\n",
      "[763/1000]\n",
      "- [VAL] LOSS : 0.02766631543636322 [SCORE] : 1.0\n",
      "[764/1000]\n",
      "- [TRAIN] LOSS : 0.053072245046496394 [SCORE] : 0.5755908330281575\n",
      "[764/1000]\n",
      "- [VAL] LOSS : 0.027665654197335243 [SCORE] : 1.0\n",
      "[765/1000]\n",
      "- [TRAIN] LOSS : 0.05306897655439873 [SCORE] : 0.5755908330281575\n",
      "[765/1000]\n",
      "- [VAL] LOSS : 0.027664976194500923 [SCORE] : 1.0\n",
      "[766/1000]\n",
      "- [TRAIN] LOSS : 0.053066296766822535 [SCORE] : 0.5755908330281575\n",
      "[766/1000]\n",
      "- [VAL] LOSS : 0.02766432799398899 [SCORE] : 1.0\n",
      "[767/1000]\n",
      "- [TRAIN] LOSS : 0.05306359718864163 [SCORE] : 0.5755908330281575\n",
      "[767/1000]\n",
      "- [VAL] LOSS : 0.02766365185379982 [SCORE] : 1.0\n",
      "[768/1000]\n",
      "- [TRAIN] LOSS : 0.05306132746239503 [SCORE] : 0.5755908330281575\n",
      "[768/1000]\n",
      "- [VAL] LOSS : 0.027662908658385277 [SCORE] : 1.0\n",
      "[769/1000]\n",
      "- [TRAIN] LOSS : 0.05305707414324085 [SCORE] : 0.5755908330281575\n",
      "[769/1000]\n",
      "- [VAL] LOSS : 0.027662286534905434 [SCORE] : 1.0\n",
      "[770/1000]\n",
      "- [TRAIN] LOSS : 0.05305389603599906 [SCORE] : 0.5755908330281575\n",
      "[770/1000]\n",
      "- [VAL] LOSS : 0.027661681175231934 [SCORE] : 1.0\n",
      "[771/1000]\n",
      "- [TRAIN] LOSS : 0.05305122978364428 [SCORE] : 0.5755908330281575\n",
      "[771/1000]\n",
      "- [VAL] LOSS : 0.027661113068461418 [SCORE] : 1.0\n",
      "[772/1000]\n",
      "- [TRAIN] LOSS : 0.053048543818295005 [SCORE] : 0.5755908330281575\n",
      "[772/1000]\n",
      "- [VAL] LOSS : 0.02766047790646553 [SCORE] : 1.0\n",
      "[773/1000]\n",
      "- [TRAIN] LOSS : 0.05304635443414251 [SCORE] : 0.5755908330281575\n",
      "[773/1000]\n",
      "- [VAL] LOSS : 0.027659794315695763 [SCORE] : 1.0\n",
      "[774/1000]\n",
      "- [TRAIN] LOSS : 0.05304209988874694 [SCORE] : 0.5755908330281575\n",
      "[774/1000]\n",
      "- [VAL] LOSS : 0.027659235522150993 [SCORE] : 1.0\n",
      "[775/1000]\n",
      "- [TRAIN] LOSS : 0.05303889773786068 [SCORE] : 0.5755908330281575\n",
      "[775/1000]\n",
      "- [VAL] LOSS : 0.027658728882670403 [SCORE] : 1.0\n",
      "[776/1000]\n",
      "- [TRAIN] LOSS : 0.05303627513349056 [SCORE] : 0.5755908330281575\n",
      "[776/1000]\n",
      "- [VAL] LOSS : 0.02765819802880287 [SCORE] : 1.0\n",
      "[777/1000]\n",
      "- [TRAIN] LOSS : 0.05303363588949044 [SCORE] : 0.5755908330281575\n",
      "[777/1000]\n",
      "- [VAL] LOSS : 0.027657655999064445 [SCORE] : 1.0\n",
      "[778/1000]\n",
      "- [TRAIN] LOSS : 0.05303068802071114 [SCORE] : 0.5755908330281575\n",
      "[778/1000]\n",
      "- [VAL] LOSS : 0.02765706367790699 [SCORE] : 1.0\n",
      "[779/1000]\n",
      "- [TRAIN] LOSS : 0.053028367025156814 [SCORE] : 0.5755908330281575\n",
      "[779/1000]\n",
      "- [VAL] LOSS : 0.027656439691781998 [SCORE] : 1.0\n",
      "[780/1000]\n",
      "- [TRAIN] LOSS : 0.053024190462504825 [SCORE] : 0.5755908330281575\n",
      "[780/1000]\n",
      "- [VAL] LOSS : 0.027656003832817078 [SCORE] : 1.0\n",
      "[781/1000]\n",
      "- [TRAIN] LOSS : 0.053021119860932234 [SCORE] : 0.5755908330281575\n",
      "[781/1000]\n",
      "- [VAL] LOSS : 0.0276555884629488 [SCORE] : 1.0\n",
      "[782/1000]\n",
      "- [TRAIN] LOSS : 0.053018562076613304 [SCORE] : 0.5755908330281575\n",
      "[782/1000]\n",
      "- [VAL] LOSS : 0.027655085548758507 [SCORE] : 1.0\n",
      "[783/1000]\n",
      "- [TRAIN] LOSS : 0.05301589619678756 [SCORE] : 0.5755908330281575\n",
      "[783/1000]\n",
      "- [VAL] LOSS : 0.027654606848955154 [SCORE] : 1.0\n",
      "[784/1000]\n",
      "- [TRAIN] LOSS : 0.05301375339428584 [SCORE] : 0.5755908330281575\n",
      "[784/1000]\n",
      "- [VAL] LOSS : 0.027654075995087624 [SCORE] : 1.0\n",
      "[785/1000]\n",
      "- [TRAIN] LOSS : 0.053009557025507095 [SCORE] : 0.5755908330281575\n",
      "[785/1000]\n",
      "- [VAL] LOSS : 0.02765364944934845 [SCORE] : 1.0\n",
      "[786/1000]\n",
      "- [TRAIN] LOSS : 0.05300638801418245 [SCORE] : 0.5755908330281575\n",
      "[786/1000]\n",
      "- [VAL] LOSS : 0.027653226628899574 [SCORE] : 1.0\n",
      "[787/1000]\n",
      "- [TRAIN] LOSS : 0.05300383275995652 [SCORE] : 0.5755908330281575\n",
      "[787/1000]\n",
      "- [VAL] LOSS : 0.027652788907289505 [SCORE] : 1.0\n",
      "[788/1000]\n",
      "- [TRAIN] LOSS : 0.05300129503011704 [SCORE] : 0.5755908330281575\n",
      "[788/1000]\n",
      "- [VAL] LOSS : 0.02765236236155033 [SCORE] : 1.0\n",
      "[789/1000]\n",
      "- [TRAIN] LOSS : 0.05299912413271765 [SCORE] : 0.5755908330281575\n",
      "[789/1000]\n",
      "- [VAL] LOSS : 0.027651889249682426 [SCORE] : 1.0\n",
      "[790/1000]\n",
      "- [TRAIN] LOSS : 0.05299494833064576 [SCORE] : 0.5755908330281575\n",
      "[790/1000]\n",
      "- [VAL] LOSS : 0.027651473879814148 [SCORE] : 1.0\n",
      "[791/1000]\n",
      "- [TRAIN] LOSS : 0.0529918273134778 [SCORE] : 0.5755908330281575\n",
      "[791/1000]\n",
      "- [VAL] LOSS : 0.027651149779558182 [SCORE] : 1.0\n",
      "[792/1000]\n",
      "- [TRAIN] LOSS : 0.05298930546268821 [SCORE] : 0.5755908330281575\n",
      "[792/1000]\n",
      "- [VAL] LOSS : 0.027650771662592888 [SCORE] : 1.0\n",
      "[793/1000]\n",
      "- [TRAIN] LOSS : 0.05298673814783494 [SCORE] : 0.5755908330281575\n",
      "[793/1000]\n",
      "- [VAL] LOSS : 0.02765043079853058 [SCORE] : 1.0\n",
      "[794/1000]\n",
      "- [TRAIN] LOSS : 0.052983857908596596 [SCORE] : 0.5755908330281575\n",
      "[794/1000]\n",
      "- [VAL] LOSS : 0.027650030329823494 [SCORE] : 1.0\n",
      "[795/1000]\n",
      "- [TRAIN] LOSS : 0.0529816201577584 [SCORE] : 0.5755908330281575\n",
      "[795/1000]\n",
      "- [VAL] LOSS : 0.027649613097310066 [SCORE] : 1.0\n",
      "[796/1000]\n",
      "- [TRAIN] LOSS : 0.05297754107353588 [SCORE] : 0.5755908330281575\n",
      "[796/1000]\n",
      "- [VAL] LOSS : 0.027649344876408577 [SCORE] : 1.0\n",
      "[797/1000]\n",
      "- [TRAIN] LOSS : 0.052974495974679785 [SCORE] : 0.5755908330281575\n",
      "[797/1000]\n",
      "- [VAL] LOSS : 0.0276490431278944 [SCORE] : 1.0\n",
      "[798/1000]\n",
      "- [TRAIN] LOSS : 0.052971998291711016 [SCORE] : 0.5755908330281575\n",
      "[798/1000]\n",
      "- [VAL] LOSS : 0.027648763731122017 [SCORE] : 1.0\n",
      "[799/1000]\n",
      "- [TRAIN] LOSS : 0.052969455160200594 [SCORE] : 0.5755908330281575\n",
      "[799/1000]\n",
      "- [VAL] LOSS : 0.02764846198260784 [SCORE] : 1.0\n",
      "[800/1000]\n",
      "- [TRAIN] LOSS : 0.05296661970205605 [SCORE] : 0.5755908330281575\n",
      "[800/1000]\n",
      "- [VAL] LOSS : 0.02764813043177128 [SCORE] : 1.0\n",
      "[801/1000]\n",
      "- [TRAIN] LOSS : 0.05296442398491005 [SCORE] : 0.5755908330281575\n",
      "[801/1000]\n",
      "- [VAL] LOSS : 0.02764781191945076 [SCORE] : 1.0\n",
      "[802/1000]\n",
      "- [TRAIN] LOSS : 0.05296035724071165 [SCORE] : 0.5755908330281575\n",
      "[802/1000]\n",
      "- [VAL] LOSS : 0.02764754928648472 [SCORE] : 1.0\n",
      "[803/1000]\n",
      "- [TRAIN] LOSS : 0.05295735149023433 [SCORE] : 0.5755908330281575\n",
      "[803/1000]\n",
      "- [VAL] LOSS : 0.027647342532873154 [SCORE] : 1.0\n",
      "[804/1000]\n",
      "- [TRAIN] LOSS : 0.05295488092427452 [SCORE] : 0.5755908330281575\n",
      "[804/1000]\n",
      "- [VAL] LOSS : 0.02764713391661644 [SCORE] : 1.0\n",
      "[805/1000]\n",
      "- [TRAIN] LOSS : 0.05295233380359908 [SCORE] : 0.5755908330281575\n",
      "[805/1000]\n",
      "- [VAL] LOSS : 0.027646884322166443 [SCORE] : 1.0\n",
      "[806/1000]\n",
      "- [TRAIN] LOSS : 0.052950280951336026 [SCORE] : 0.5755908330281575\n",
      "[806/1000]\n",
      "- [VAL] LOSS : 0.027646556496620178 [SCORE] : 1.0\n",
      "[807/1000]\n",
      "- [TRAIN] LOSS : 0.05294619694662591 [SCORE] : 0.5755908330281575\n",
      "[807/1000]\n",
      "- [VAL] LOSS : 0.027646368369460106 [SCORE] : 1.0\n",
      "[808/1000]\n",
      "- [TRAIN] LOSS : 0.052943167338768644 [SCORE] : 0.5755908330281575\n",
      "[808/1000]\n",
      "- [VAL] LOSS : 0.02764621190726757 [SCORE] : 1.0\n",
      "[809/1000]\n",
      "- [TRAIN] LOSS : 0.05294068010213474 [SCORE] : 0.5755908330281575\n",
      "[809/1000]\n",
      "- [VAL] LOSS : 0.027646023780107498 [SCORE] : 1.0\n",
      "[810/1000]\n",
      "- [TRAIN] LOSS : 0.05293819488336642 [SCORE] : 0.5755908330281575\n",
      "[810/1000]\n",
      "- [VAL] LOSS : 0.027645796537399292 [SCORE] : 1.0\n",
      "[811/1000]\n",
      "- [TRAIN] LOSS : 0.05293539126093189 [SCORE] : 0.5755908330281575\n",
      "[811/1000]\n",
      "- [VAL] LOSS : 0.02764558605849743 [SCORE] : 1.0\n",
      "[812/1000]\n",
      "- [TRAIN] LOSS : 0.052933258252839246 [SCORE] : 0.5755908330281575\n",
      "[812/1000]\n",
      "- [VAL] LOSS : 0.027645375579595566 [SCORE] : 1.0\n",
      "[813/1000]\n",
      "- [TRAIN] LOSS : 0.052929225657135245 [SCORE] : 0.5755908330281575\n",
      "[813/1000]\n",
      "- [VAL] LOSS : 0.027645250782370567 [SCORE] : 1.0\n",
      "[814/1000]\n",
      "- [TRAIN] LOSS : 0.05292630499849717 [SCORE] : 0.5755908330281575\n",
      "[814/1000]\n",
      "- [VAL] LOSS : 0.027645135298371315 [SCORE] : 1.0\n",
      "[815/1000]\n",
      "- [TRAIN] LOSS : 0.05292390092896918 [SCORE] : 0.5755908330281575\n",
      "[815/1000]\n",
      "- [VAL] LOSS : 0.027645034715533257 [SCORE] : 1.0\n",
      "[816/1000]\n",
      "- [TRAIN] LOSS : 0.052921399225791296 [SCORE] : 0.5755908330281575\n",
      "[816/1000]\n",
      "- [VAL] LOSS : 0.02764485776424408 [SCORE] : 1.0\n",
      "[817/1000]\n",
      "- [TRAIN] LOSS : 0.05291862185113132 [SCORE] : 0.5755908330281575\n",
      "[817/1000]\n",
      "- [VAL] LOSS : 0.02764471061527729 [SCORE] : 1.0\n",
      "[818/1000]\n",
      "- [TRAIN] LOSS : 0.05291646751575172 [SCORE] : 0.5755908330281575\n",
      "[818/1000]\n",
      "- [VAL] LOSS : 0.027644550427794456 [SCORE] : 1.0\n",
      "[819/1000]\n",
      "- [TRAIN] LOSS : 0.05291248944898446 [SCORE] : 0.5755908330281575\n",
      "[819/1000]\n",
      "- [VAL] LOSS : 0.02764449082314968 [SCORE] : 1.0\n",
      "[820/1000]\n",
      "- [TRAIN] LOSS : 0.05290955056746801 [SCORE] : 0.5755908330281575\n",
      "[820/1000]\n",
      "- [VAL] LOSS : 0.027644425630569458 [SCORE] : 1.0\n",
      "[821/1000]\n",
      "- [TRAIN] LOSS : 0.052907192365576824 [SCORE] : 0.5755908330281575\n",
      "[821/1000]\n",
      "- [VAL] LOSS : 0.027644352987408638 [SCORE] : 1.0\n",
      "[822/1000]\n",
      "- [TRAIN] LOSS : 0.05290473148537179 [SCORE] : 0.5755908330281575\n",
      "[822/1000]\n",
      "- [VAL] LOSS : 0.027644287794828415 [SCORE] : 1.0\n",
      "[823/1000]\n",
      "- [TRAIN] LOSS : 0.052901971091826754 [SCORE] : 0.5755908330281575\n",
      "[823/1000]\n",
      "- [VAL] LOSS : 0.027644170448184013 [SCORE] : 1.0\n",
      "[824/1000]\n",
      "- [TRAIN] LOSS : 0.052899873520558076 [SCORE] : 0.5755908330281575\n",
      "[824/1000]\n",
      "- [VAL] LOSS : 0.0276440791785717 [SCORE] : 1.0\n",
      "[825/1000]\n",
      "- [TRAIN] LOSS : 0.0528958892604957 [SCORE] : 0.5755908330281575\n",
      "[825/1000]\n",
      "- [VAL] LOSS : 0.027644062414765358 [SCORE] : 1.0\n",
      "[826/1000]\n",
      "- [TRAIN] LOSS : 0.05289298786471287 [SCORE] : 0.5755908330281575\n",
      "[826/1000]\n",
      "- [VAL] LOSS : 0.027644062414765358 [SCORE] : 1.0\n",
      "[827/1000]\n",
      "- [TRAIN] LOSS : 0.0528906457281361 [SCORE] : 0.5755908330281575\n",
      "[827/1000]\n",
      "- [VAL] LOSS : 0.027644027024507523 [SCORE] : 1.0\n",
      "[828/1000]\n",
      "- [TRAIN] LOSS : 0.05288820803786318 [SCORE] : 0.5755908330281575\n",
      "[828/1000]\n",
      "- [VAL] LOSS : 0.027643995359539986 [SCORE] : 1.0\n",
      "[829/1000]\n",
      "- [TRAIN] LOSS : 0.052885492363323766 [SCORE] : 0.5755908330281575\n",
      "[829/1000]\n",
      "- [VAL] LOSS : 0.02764396369457245 [SCORE] : 1.0\n",
      "[830/1000]\n",
      "- [TRAIN] LOSS : 0.05288339531980455 [SCORE] : 0.5755908330281575\n",
      "[830/1000]\n",
      "- [VAL] LOSS : 0.027643902227282524 [SCORE] : 1.0\n",
      "[831/1000]\n",
      "- [TRAIN] LOSS : 0.05287944624821345 [SCORE] : 0.5755908330281575\n",
      "[831/1000]\n",
      "- [VAL] LOSS : 0.027643958106637 [SCORE] : 1.0\n",
      "[832/1000]\n",
      "- [TRAIN] LOSS : 0.052876563960065445 [SCORE] : 0.5755908330281575\n",
      "[832/1000]\n",
      "- [VAL] LOSS : 0.02764398418366909 [SCORE] : 1.0\n",
      "[833/1000]\n",
      "- [TRAIN] LOSS : 0.05287424431492885 [SCORE] : 0.5755908330281575\n",
      "[833/1000]\n",
      "- [VAL] LOSS : 0.027644023299217224 [SCORE] : 1.0\n",
      "[834/1000]\n",
      "- [TRAIN] LOSS : 0.05287181769187252 [SCORE] : 0.5755908330281575\n",
      "[834/1000]\n",
      "- [VAL] LOSS : 0.02764408104121685 [SCORE] : 1.0\n",
      "[835/1000]\n",
      "- [TRAIN] LOSS : 0.05286915081863602 [SCORE] : 0.5755908330281575\n",
      "[835/1000]\n",
      "- [VAL] LOSS : 0.027644077315926552 [SCORE] : 1.0\n",
      "[836/1000]\n",
      "- [TRAIN] LOSS : 0.05286629797580342 [SCORE] : 0.5755908330281575\n",
      "[836/1000]\n",
      "- [VAL] LOSS : 0.027644125744700432 [SCORE] : 1.0\n",
      "[837/1000]\n",
      "- [TRAIN] LOSS : 0.05286428779363632 [SCORE] : 0.5755908330281575\n",
      "[837/1000]\n",
      "- [VAL] LOSS : 0.027644101530313492 [SCORE] : 1.0\n",
      "[838/1000]\n",
      "- [TRAIN] LOSS : 0.05286044324748218 [SCORE] : 0.5755908330281575\n",
      "[838/1000]\n",
      "- [VAL] LOSS : 0.027644256129860878 [SCORE] : 1.0\n",
      "[839/1000]\n",
      "- [TRAIN] LOSS : 0.05285761409128706 [SCORE] : 0.5755908330281575\n",
      "[839/1000]\n",
      "- [VAL] LOSS : 0.02764434926211834 [SCORE] : 1.0\n",
      "[840/1000]\n",
      "- [TRAIN] LOSS : 0.0528553134140869 [SCORE] : 0.5755908330281575\n",
      "[840/1000]\n",
      "- [VAL] LOSS : 0.027644416317343712 [SCORE] : 1.0\n",
      "[841/1000]\n",
      "- [TRAIN] LOSS : 0.05285288542509079 [SCORE] : 0.5755908330281575\n",
      "[841/1000]\n",
      "- [VAL] LOSS : 0.027644531801342964 [SCORE] : 1.0\n",
      "[842/1000]\n",
      "- [TRAIN] LOSS : 0.0528502121878167 [SCORE] : 0.5755908330281575\n",
      "[842/1000]\n",
      "- [VAL] LOSS : 0.027644604444503784 [SCORE] : 1.0\n",
      "성능 및 손실 개선이 없어서 학습\n"
     ]
    }
   ],
   "source": [
    "# 학습의 효과 확인 손실값과 성능평가값 저장 필요\n",
    "LOSS_HISTORY, SCORE_HISTROY=[[],[]], [[],[]]\n",
    "\n",
    "f1_score_metric = MulticlassF1Score(num_classes=3)\n",
    "# 중단 기준\n",
    "BREAK_CNT =0\n",
    "for epoch in range(1,EPOCH+1):\n",
    "    # 학습 모드로 모델 설정\n",
    "    model.train()\n",
    "\n",
    "    # 배치크기 만큼 데이터 로딩해서 학습 진행\n",
    "    loss_total, score_total=0, 0\n",
    "    for featureTS, targetTS in trainDL:\n",
    "\n",
    "        # 학습 진행\n",
    "        pre_y=model(featureTS)\n",
    "\n",
    "        # 손실 계산 : nn.CrossEntropyLoss 요구사항 : 정답/타겟은 0D 또는 1D, 타입은 long\n",
    "        loss=crossLoss(pre_y, targetTS.reshape(-1).long())\n",
    "        loss_total += loss.item()\n",
    "\n",
    "        # 성능평가 계산\n",
    "        score=f1_score_metric(pre_y, targetTS.reshape(-1))\n",
    "        score_total += score.item()\n",
    "\n",
    "        # 최적화 진행\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 에포크 당 검증기능\n",
    "    # 모델 검증 모드 설정\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 검증 데이터셋\n",
    "        val_featureTS=torch.FloatTensor(valDS.featureDF.values)\n",
    "        val_targetTS=torch.FloatTensor(valDS.targetDF.values)\n",
    "\n",
    "        # 추론/평가\n",
    "        pre_val=model(val_featureTS)\n",
    "\n",
    "        # 손실\n",
    "        loss_val=crossLoss(pre_val, val_targetTS.reshape(-1).long())\n",
    "\n",
    "        # 성능평가\n",
    "        score_val=MulticlassF1Score(num_classes=3)(pre_val, val_targetTS.reshape(-1))\n",
    "    \n",
    "    # 에포크 당 손실값과 성능평가값 저장\n",
    "    LOSS_HISTORY[0].append(loss_total/BATCH_CNT)\n",
    "    SCORE_HISTROY[0].append(score_total/BATCH_CNT)\n",
    "\n",
    "    LOSS_HISTORY[1].append(loss_val)\n",
    "    SCORE_HISTROY[1].append(score_val)\n",
    "\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- [TRAIN] LOSS : {LOSS_HISTORY[0][-1]} [SCORE] : {SCORE_HISTROY[0][-1]}')\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- [VAL] LOSS : {LOSS_HISTORY[1][-1]} [SCORE] : {SCORE_HISTROY[1][-1]}')\n",
    "\n",
    "\n",
    "    # 학습이 잘안되면 중단\n",
    "    # LOSS를 기준으로 중단\n",
    "\n",
    "    if len(LOSS_HISTORY[1]) >=2:\n",
    "        if LOSS_HISTORY[1][-1] >= LOSS_HISTORY[1][-2]:\n",
    "            BREAK_CNT +=1\n",
    "\n",
    "    if len(LOSS_HISTORY[1]) ==1:\n",
    "        # 첫번째라서 무조건 저장\n",
    "        torch.save(model.state_dict(), SAVE_FILE)\n",
    "        # 모델 저장\n",
    "        torch.save(model, SAVE_MODEL)\n",
    "    else:\n",
    "        if LOSS_HISTORY[1][-1] < min(LOSS_HISTORY[1]):      # 현재 에포크를 포함하여 전체 검증 손실에서 최소값 찾음음\n",
    "            torch.save(model.state_dict(), SAVE_FILE)\n",
    "            torch.save(model, SAVE_MODEL)\n",
    "            \n",
    "    # 학습 중단 여부 설정\n",
    "    if BREAK_CNT >10 :\n",
    "        print(\"성능 및 손실 개선이 없어서 학습\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 검증 모드 설정\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 검증 데이터셋\n",
    "    test_featureTS=torch.FloatTensor(testDS.featureDF.values)\n",
    "    test_targetTS=torch.FloatTensor(testDS.targetDF.values)\n",
    "\n",
    "    # 추론/평가\n",
    "    pre_val=model(test_featureTS)\n",
    "    #print(pre_val)\n",
    "\n",
    "    # 손실\n",
    "    loss_test=crossLoss(pre_val, test_targetTS.reshape(-1).long())\n",
    "\n",
    "    # 성능평가\n",
    "    score_test=MulticlassF1Score(num_classes=3)(pre_val, test_targetTS.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function_list import predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [float(x) for x in input(\"SL, SW, PL , PW: \").split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5, 1.2, 1.4, 1.8]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_val, idx =predict_data(model, data, multi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Setosa', 'Versicolor', 'Virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virginica\n"
     ]
    }
   ],
   "source": [
    "predict = class_names[idx]\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function_list import plot_loss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_score(LOSS_HISTORY, SCORE_HISTROY, 'score',100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
