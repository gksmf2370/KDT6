{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN 기반 다중분류 모델 구현\n",
    "- 데이터셋 : iris.csv\n",
    "- Feature : 4개 Sepal_Length, Sepal_Width, Petal_Length, Petal_Width\n",
    "- Target : 1개 Variety\n",
    "- 학습-방법 : 지도학습 > 분류 > 다중분류 (클래스 3개)\n",
    "- 알고리즘 : 인공신경망(ANN) => MLP, DNN : 은닉층이 많은 구성\n",
    "- 프레임워크 : Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 및 데이터 준비 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 관련 모듈 로딩\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torchinfo import summary\n",
    "\n",
    "# Data 관련 모듈 로딩\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch V. 2.4.1\n",
      "Pandas V. 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# 활용 패키지 버전 체크 => 사용자 정의 함수로 구현하기\n",
    "print(f'Pytorch V. {torch.__version__}')\n",
    "print(f'Pandas V. {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width variety\n",
       "0           5.1          3.5           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로딩\n",
    "DATA_FILE='../data/iris.csv'\n",
    "\n",
    "# CSV => DataFrame\n",
    "irisDF=pd.read_csv(DATA_FILE)\n",
    "\n",
    "# 데이터 확인\n",
    "irisDF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Virginica'], dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타겟 변경 => 정수화, 클래스 3개\n",
    "irisDF['variety'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels => {'Setosa': 0, 'Versicolor': 1, 'Virginica': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0           5.1          3.5           1.4          0.2        0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=dict(zip(irisDF['variety'].unique().tolist(),range(3)))\n",
    "print(f'labels => {labels}')\n",
    "\n",
    "irisDF['variety']=irisDF['variety'].replace(labels)\n",
    "irisDF.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 모델 클래스 설계 및 정의<hr>\n",
    "- 클래스목적 : iris 데이터를 학습 및 추론 목적 \n",
    "- 클래스이름 : IrisMCFModel\n",
    "- 부모클래스 : nn.Module\n",
    "- 매개변수 : 층별 입출력 개수 고정하기때문에 필요 없음\n",
    "- 속성필드 : \n",
    "- 기능역할 : __init__() : 모델 구조, forward() : 순방향 학습 <= 오버라이딩\n",
    "- 클래스구조\n",
    "    * 입력층 : 입력  4개(피처)  출력 10개(퍼셉트론/뉴런 10개 존재)\n",
    "    * 은닉층 : 입력 10개        출력 5개(퍼셉트론/뉴런 5개 존재)\n",
    "    * 출력층 : 입력  5개        출력 1개(퍼셉트론/뉴런 1개 존재 : 2진분류)\n",
    "\n",
    "- 활성화함수\n",
    "    * 클래스형태 => nn.MESLoss, nn.ReLU => __init__() 메서드\n",
    "    * 함수형태 => torch.nn.fuctional 아래에 => forward() 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisMCFModel(nn.Module):\n",
    "\n",
    "    # 모델 구조 구성 및 인스턴스 생성 메서드\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer=nn.Linear(4,10)\n",
    "        self.hd_layer=nn.Linear(10,5)\n",
    "        self.out_layer=nn.Linear(5,3) # 다중분류 'Setosa', 'Versicolor', 'Virginica' \n",
    "\n",
    "    # 순방향 학습 진행 메서드\n",
    "    def forward(self, x):\n",
    "        y=F.relu(self.in_layer(x))\n",
    "        y=F.relu(self.hd_layer(y))\n",
    "        return self.out_layer(y) # 5개의 숫자 값 => 다중분류 : 손실함수 CrossEntrpyLoss가 내부에서 softmax 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisMCFModel(\n",
      "  (in_layer): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (hd_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=5, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스 생성\n",
    "model=IrisMCFModel()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "IrisMCFModel                             [1000, 3]                 --\n",
       "├─Linear: 1-1                            [1000, 10]                50\n",
       "├─Linear: 1-2                            [1000, 5]                 55\n",
       "├─Linear: 1-3                            [1000, 3]                 18\n",
       "==========================================================================================\n",
       "Total params: 123\n",
       "Trainable params: 123\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.12\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.14\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 사용 메모리 정보 확인\n",
    "summary(model, input_size=(1000,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 데이터셋 클래스 설계 및 정의<hr>\n",
    "- 데이터셋 : iris.csv\n",
    "- 피쳐개수 : 4개\n",
    "- 타겟개수 : 1개\n",
    "- 클래스이름 : IrisDataset\n",
    "- 부모클래스 : utils.data.Dataset\n",
    "- 속성필드 : featureDF, targetDF, n_rows, n_features\n",
    "- 필수메서드\n",
    "    * __init__(self) : 데이터셋 저장 및 전처리, 개발자가 필요한 속성 설정\n",
    "    * __len__(self) : 데이터의 개수 반환\n",
    "    * __getitem__(self, index) : 특정 인덱스의 피쳐와 타겟 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, featureDF, targetDF):\n",
    "        self.featureDF=featureDF\n",
    "        self.targetDF=targetDF\n",
    "        self.n_rows=featureDF.shape[0]\n",
    "        self.n_features=featureDF.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 넘파이를 텐서로\n",
    "        featureTS=torch.FloatTensor(self.featureDF.iloc[index].values)\n",
    "        targetTS=torch.FloatTensor(self.targetDF.iloc[index].values)        \n",
    "        # 피쳐와 타겟 반환\n",
    "        return featureTS, targetTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3-1] 데이터셋 인스턴스 생성 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureDF => (150, 4), targetDF => (150, 1)\n"
     ]
    }
   ],
   "source": [
    "# 피쳐, 타겟 추출\n",
    "featureDF, targetDF=irisDF[irisDF.columns[:-1]], irisDF[irisDF.columns[-1:]]\n",
    "print(f'featureDF => {featureDF.shape}, targetDF => {targetDF.shape}')\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "irisDS=IrisDataset(featureDF, targetDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 학습 준비\n",
    "- 학습 횟수 : EPOCH <= 처음부터 끝까지 학습하는 단위\n",
    "- 배치 크기 : BATCH_SIZE <= 한번에 학습할 데이터셋 양\n",
    "- 위치 지정 : DEVICE <= 텐서 저장 및 실행 위치 (GPU/CPU)\n",
    "- 학습률 : LR 가중치와 절편 업데이트 시 경사하강법으로 업데이트 간격 설정 0.001~0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 진행 관련 설정\n",
    "EPOCH=1000\n",
    "BATCH_SIZE=10\n",
    "BATCH_CNT=irisDF.shape[0]/BATCH_SIZE\n",
    "DEVICE= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인스턴스/객체 : 모델, 데이터셋, 최적화 (+ 손실함수, 성능지표)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 4) (38, 4) (28, 4)\n",
      "(84, 1) (38, 1) (28, 1)\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스\n",
    "model=IrisMCFModel()\n",
    "\n",
    "# 데이터셋 인스턴스\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터 분리\n",
    "X_train, X_test, y_train, y_test=train_test_split(featureDF, targetDF, random_state=1)\n",
    "X_train, X_val, y_train, y_val=train_test_split(X_train, y_train, random_state=1)\n",
    "print(f'{X_train.shape} {X_test.shape} {X_val.shape}')\n",
    "print(f'{y_train.shape} {y_test.shape} {y_val.shape}')\n",
    "\n",
    "trainDS=IrisDataset(X_train, y_train)\n",
    "valDS=IrisDataset(X_val, y_val)\n",
    "testDS=IrisDataset(X_test, y_test)\n",
    "\n",
    "# 데이터로드 인스턴스\n",
    "trainDL=DataLoader(trainDS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화, 손실함수 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 인스턴스 => W, b 텐서 즉, model.parameters() 전달\n",
    "optimizer=optim.Adam(model.parameters(),lr=LR)\n",
    "\n",
    "# 손실함수 인스턴스 => 분류 => 다중분류 CrossEntropyLoss\n",
    "#                            예측값은 선형식 결과값으로 전달 => AF 처리 X\n",
    "crossLoss=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 파일 관련\n",
    "### models 폴더 아래 프로젝트 폴더 아래 모델 파일 저장\n",
    "import os\n",
    "\n",
    "# 저장 경로\n",
    "SAVE_PATH = '../models/iris/MCF/'\n",
    "# 저장 파일명\n",
    "SAVE_FILE=SAVE_PATH+'model_train_wbs.pth'\n",
    "\n",
    "# 모델 구조 및 파라미터 모두 저장 파일명명\n",
    "SAVE_MODEL=SAVE_PATH+'model_all.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로상 폴더 존재 여부 체크\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)   # 폴더 / 폴더/.. 하위폴더까지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000]\n",
      "- [TRAIN] LOSS : 0.7057619015375773 [SCORE] : 0.09865283071994782\n",
      "[1/1000]\n",
      "- [VAL] LOSS : 1.3174775838851929 [SCORE] : 0.10101010650396347\n",
      "[2/1000]\n",
      "- [TRAIN] LOSS : 0.6935278892517089 [SCORE] : 0.09865283071994782\n",
      "[2/1000]\n",
      "- [VAL] LOSS : 1.2845618724822998 [SCORE] : 0.10101010650396347\n",
      "[3/1000]\n",
      "- [TRAIN] LOSS : 0.6839449683825175 [SCORE] : 0.09865283071994782\n",
      "[3/1000]\n",
      "- [VAL] LOSS : 1.2570594549179077 [SCORE] : 0.10101010650396347\n",
      "[4/1000]\n",
      "- [TRAIN] LOSS : 0.6767165025075277 [SCORE] : 0.09865283071994782\n",
      "[4/1000]\n",
      "- [VAL] LOSS : 1.2346842288970947 [SCORE] : 0.10101010650396347\n",
      "[5/1000]\n",
      "- [TRAIN] LOSS : 0.6712969700495403 [SCORE] : 0.09865283071994782\n",
      "[5/1000]\n",
      "- [VAL] LOSS : 1.2164554595947266 [SCORE] : 0.10101010650396347\n",
      "[6/1000]\n",
      "- [TRAIN] LOSS : 0.6671454509099325 [SCORE] : 0.09865283071994782\n",
      "[6/1000]\n",
      "- [VAL] LOSS : 1.201358437538147 [SCORE] : 0.10101010650396347\n",
      "[7/1000]\n",
      "- [TRAIN] LOSS : 0.6638283650080363 [SCORE] : 0.09999963243802389\n",
      "[7/1000]\n",
      "- [VAL] LOSS : 1.1885319948196411 [SCORE] : 0.10101010650396347\n",
      "[8/1000]\n",
      "- [TRAIN] LOSS : 0.6610282778739929 [SCORE] : 0.11420234342416127\n",
      "[8/1000]\n",
      "- [VAL] LOSS : 1.1772997379302979 [SCORE] : 0.12121212482452393\n",
      "[9/1000]\n",
      "- [TRAIN] LOSS : 0.6585377335548401 [SCORE] : 0.11837606926759084\n",
      "[9/1000]\n",
      "- [VAL] LOSS : 1.1673517227172852 [SCORE] : 0.1367521435022354\n",
      "[10/1000]\n",
      "- [TRAIN] LOSS : 0.6563802003860474 [SCORE] : 0.08272086133559545\n",
      "[10/1000]\n",
      "- [VAL] LOSS : 1.1594043970108032 [SCORE] : 0.13730356097221375\n",
      "[11/1000]\n",
      "- [TRAIN] LOSS : 0.6547880530357361 [SCORE] : 0.06912112782398859\n",
      "[11/1000]\n",
      "- [VAL] LOSS : 1.154250979423523 [SCORE] : 0.11764705926179886\n",
      "[12/1000]\n",
      "- [TRAIN] LOSS : 0.652961798508962 [SCORE] : 0.058223259449005124\n",
      "[12/1000]\n",
      "- [VAL] LOSS : 1.1499717235565186 [SCORE] : 0.14814814925193787\n",
      "[13/1000]\n",
      "- [TRAIN] LOSS : 0.6508057435353597 [SCORE] : 0.07065527240435282\n",
      "[13/1000]\n",
      "- [VAL] LOSS : 1.144538164138794 [SCORE] : 0.17543859779834747\n",
      "[14/1000]\n",
      "- [TRAIN] LOSS : 0.648481031258901 [SCORE] : 0.09532134681940078\n",
      "[14/1000]\n",
      "- [VAL] LOSS : 1.1388299465179443 [SCORE] : 0.18803419172763824\n",
      "[15/1000]\n",
      "- [TRAIN] LOSS : 0.6460732380549113 [SCORE] : 0.10153735329707464\n",
      "[15/1000]\n",
      "- [VAL] LOSS : 1.1331332921981812 [SCORE] : 0.18803419172763824\n",
      "[16/1000]\n",
      "- [TRAIN] LOSS : 0.6433498740196228 [SCORE] : 0.1039793555935224\n",
      "[16/1000]\n",
      "- [VAL] LOSS : 1.1268119812011719 [SCORE] : 0.18803419172763824\n",
      "[17/1000]\n",
      "- [TRAIN] LOSS : 0.6405961076418559 [SCORE] : 0.1039793555935224\n",
      "[17/1000]\n",
      "- [VAL] LOSS : 1.1203309297561646 [SCORE] : 0.18803419172763824\n",
      "[18/1000]\n",
      "- [TRAIN] LOSS : 0.6374640146891276 [SCORE] : 0.10581085731585821\n",
      "[18/1000]\n",
      "- [VAL] LOSS : 1.1132652759552002 [SCORE] : 0.18803419172763824\n",
      "[19/1000]\n",
      "- [TRAIN] LOSS : 0.6339298446973165 [SCORE] : 0.10318015466133754\n",
      "[19/1000]\n",
      "- [VAL] LOSS : 1.105364203453064 [SCORE] : 0.2222222238779068\n",
      "[20/1000]\n",
      "- [TRAIN] LOSS : 0.6300475160280864 [SCORE] : 0.1078891505797704\n",
      "[20/1000]\n",
      "- [VAL] LOSS : 1.096681833267212 [SCORE] : 0.28612926602363586\n",
      "[21/1000]\n",
      "- [TRAIN] LOSS : 0.6258192658424377 [SCORE] : 0.14868884533643723\n",
      "[21/1000]\n",
      "- [VAL] LOSS : 1.0873485803604126 [SCORE] : 0.3017544150352478\n",
      "[22/1000]\n",
      "- [TRAIN] LOSS : 0.6209853053092956 [SCORE] : 0.18641210893789928\n",
      "[22/1000]\n",
      "- [VAL] LOSS : 1.0762430429458618 [SCORE] : 0.28333333134651184\n",
      "[23/1000]\n",
      "- [TRAIN] LOSS : 0.6158234000205993 [SCORE] : 0.18253203928470613\n",
      "[23/1000]\n",
      "- [VAL] LOSS : 1.064531683921814 [SCORE] : 0.28333333134651184\n",
      "[24/1000]\n",
      "- [TRAIN] LOSS : 0.6101532498995463 [SCORE] : 0.19923755725224812\n",
      "[24/1000]\n",
      "- [VAL] LOSS : 1.0520933866500854 [SCORE] : 0.3636363744735718\n",
      "[25/1000]\n",
      "- [TRAIN] LOSS : 0.6037004748980205 [SCORE] : 0.19775607585906982\n",
      "[25/1000]\n",
      "- [VAL] LOSS : 1.037246584892273 [SCORE] : 0.3636363744735718\n",
      "[26/1000]\n",
      "- [TRAIN] LOSS : 0.5968515356381734 [SCORE] : 0.20762139658133189\n",
      "[26/1000]\n",
      "- [VAL] LOSS : 1.0213335752487183 [SCORE] : 0.4016563296318054\n",
      "[27/1000]\n",
      "- [TRAIN] LOSS : 0.5893947839736938 [SCORE] : 0.235111806790034\n",
      "[27/1000]\n",
      "- [VAL] LOSS : 1.0049251317977905 [SCORE] : 0.4388888478279114\n",
      "[28/1000]\n",
      "- [TRAIN] LOSS : 0.5811378161112467 [SCORE] : 0.23751039505004884\n",
      "[28/1000]\n",
      "- [VAL] LOSS : 0.9862625002861023 [SCORE] : 0.4388888478279114\n",
      "[29/1000]\n",
      "- [TRAIN] LOSS : 0.5722984433174133 [SCORE] : 0.24357741475105285\n",
      "[29/1000]\n",
      "- [VAL] LOSS : 0.9660900235176086 [SCORE] : 0.4623878598213196\n",
      "[30/1000]\n",
      "- [TRAIN] LOSS : 0.5628663976987203 [SCORE] : 0.24357741475105285\n",
      "[30/1000]\n",
      "- [VAL] LOSS : 0.9450435638427734 [SCORE] : 0.4623878598213196\n",
      "[31/1000]\n",
      "- [TRAIN] LOSS : 0.5526093085606892 [SCORE] : 0.2672566990057627\n",
      "[31/1000]\n",
      "- [VAL] LOSS : 0.9215196371078491 [SCORE] : 0.6595655679702759\n",
      "[32/1000]\n",
      "- [TRAIN] LOSS : 0.5418773571650187 [SCORE] : 0.40046016573905946\n",
      "[32/1000]\n",
      "- [VAL] LOSS : 0.8976060748100281 [SCORE] : 0.8754578828811646\n",
      "[33/1000]\n",
      "- [TRAIN] LOSS : 0.530456006526947 [SCORE] : 0.4721244255701701\n",
      "[33/1000]\n",
      "- [VAL] LOSS : 0.8718372583389282 [SCORE] : 1.0\n",
      "[34/1000]\n",
      "- [TRAIN] LOSS : 0.518490747610728 [SCORE] : 0.5348308563232422\n",
      "[34/1000]\n",
      "- [VAL] LOSS : 0.8447754979133606 [SCORE] : 1.0\n",
      "[35/1000]\n",
      "- [TRAIN] LOSS : 0.5061379591623942 [SCORE] : 0.5424499034881591\n",
      "[35/1000]\n",
      "- [VAL] LOSS : 0.8172478079795837 [SCORE] : 1.0\n",
      "[36/1000]\n",
      "- [TRAIN] LOSS : 0.4932447115580241 [SCORE] : 0.5661632219950358\n",
      "[36/1000]\n",
      "- [VAL] LOSS : 0.7881283760070801 [SCORE] : 1.0\n",
      "[37/1000]\n",
      "- [TRAIN] LOSS : 0.479450802008311 [SCORE] : 0.5600093762079875\n",
      "[37/1000]\n",
      "- [VAL] LOSS : 0.7526363134384155 [SCORE] : 0.9484702348709106\n",
      "[38/1000]\n",
      "- [TRAIN] LOSS : 0.4562394698460897 [SCORE] : 0.5501328349113465\n",
      "[38/1000]\n",
      "- [VAL] LOSS : 0.6957239508628845 [SCORE] : 0.8171428442001343\n",
      "[39/1000]\n",
      "- [TRAIN] LOSS : 0.43493794202804564 [SCORE] : 0.4334920724232992\n",
      "[39/1000]\n",
      "- [VAL] LOSS : 0.649796187877655 [SCORE] : 0.7264957427978516\n",
      "[40/1000]\n",
      "- [TRAIN] LOSS : 0.41825137535731 [SCORE] : 0.37182337045669556\n",
      "[40/1000]\n",
      "- [VAL] LOSS : 0.6112100481987 [SCORE] : 0.7264957427978516\n",
      "[41/1000]\n",
      "- [TRAIN] LOSS : 0.4026628772417704 [SCORE] : 0.39849003553390505\n",
      "[41/1000]\n",
      "- [VAL] LOSS : 0.5774294137954712 [SCORE] : 0.7264957427978516\n",
      "[42/1000]\n",
      "- [TRAIN] LOSS : 0.3879092415173849 [SCORE] : 0.4075376550356547\n",
      "[42/1000]\n",
      "- [VAL] LOSS : 0.5469151735305786 [SCORE] : 0.7264957427978516\n",
      "[43/1000]\n",
      "- [TRAIN] LOSS : 0.3738955577214559 [SCORE] : 0.42272247076034547\n",
      "[43/1000]\n",
      "- [VAL] LOSS : 0.5191226601600647 [SCORE] : 0.8171428442001343\n",
      "[44/1000]\n",
      "- [TRAIN] LOSS : 0.3606293757756551 [SCORE] : 0.4283068855603536\n",
      "[44/1000]\n",
      "- [VAL] LOSS : 0.4936005175113678 [SCORE] : 0.8171428442001343\n",
      "[45/1000]\n",
      "- [TRAIN] LOSS : 0.3481553355852763 [SCORE] : 0.44509700934092206\n",
      "[45/1000]\n",
      "- [VAL] LOSS : 0.47017452120780945 [SCORE] : 0.888888955116272\n",
      "[46/1000]\n",
      "- [TRAIN] LOSS : 0.33636260231335957 [SCORE] : 0.4767019510269165\n",
      "[46/1000]\n",
      "- [VAL] LOSS : 0.4488408863544464 [SCORE] : 0.888888955116272\n",
      "[47/1000]\n",
      "- [TRAIN] LOSS : 0.32524315516153973 [SCORE] : 0.4767019510269165\n",
      "[47/1000]\n",
      "- [VAL] LOSS : 0.42932382225990295 [SCORE] : 0.888888955116272\n",
      "[48/1000]\n",
      "- [TRAIN] LOSS : 0.31465024352073667 [SCORE] : 0.5120987733205159\n",
      "[48/1000]\n",
      "- [VAL] LOSS : 0.4111127257347107 [SCORE] : 0.888888955116272\n",
      "[49/1000]\n",
      "- [TRAIN] LOSS : 0.30453149080276487 [SCORE] : 0.5234920740127563\n",
      "[49/1000]\n",
      "- [VAL] LOSS : 0.39396315813064575 [SCORE] : 0.9484702348709106\n",
      "[50/1000]\n",
      "- [TRAIN] LOSS : 0.2949019114176432 [SCORE] : 0.535873023668925\n",
      "[50/1000]\n",
      "- [VAL] LOSS : 0.3780175745487213 [SCORE] : 0.9484702348709106\n",
      "[51/1000]\n",
      "- [TRAIN] LOSS : 0.28568979700406394 [SCORE] : 0.535873023668925\n",
      "[51/1000]\n",
      "- [VAL] LOSS : 0.3631839156150818 [SCORE] : 0.9484702348709106\n",
      "[52/1000]\n",
      "- [TRAIN] LOSS : 0.2768265426158905 [SCORE] : 0.5501328349113465\n",
      "[52/1000]\n",
      "- [VAL] LOSS : 0.34913191199302673 [SCORE] : 0.9484702348709106\n",
      "[53/1000]\n",
      "- [TRAIN] LOSS : 0.2683642903963725 [SCORE] : 0.5501328349113465\n",
      "[53/1000]\n",
      "- [VAL] LOSS : 0.3359301686286926 [SCORE] : 0.9484702348709106\n",
      "[54/1000]\n",
      "- [TRAIN] LOSS : 0.26019625465075175 [SCORE] : 0.5600093762079875\n",
      "[54/1000]\n",
      "- [VAL] LOSS : 0.32337474822998047 [SCORE] : 0.9484702348709106\n",
      "[55/1000]\n",
      "- [TRAIN] LOSS : 0.25230202277501423 [SCORE] : 0.5600093762079875\n",
      "[55/1000]\n",
      "- [VAL] LOSS : 0.31137070059776306 [SCORE] : 0.9484702348709106\n",
      "[56/1000]\n",
      "- [TRAIN] LOSS : 0.2446821451187134 [SCORE] : 0.5600093762079875\n",
      "[56/1000]\n",
      "- [VAL] LOSS : 0.2999303340911865 [SCORE] : 0.9484702348709106\n",
      "[57/1000]\n",
      "- [TRAIN] LOSS : 0.2373213728268941 [SCORE] : 0.5600093762079875\n",
      "[57/1000]\n",
      "- [VAL] LOSS : 0.2889387607574463 [SCORE] : 0.9484702348709106\n",
      "[58/1000]\n",
      "- [TRAIN] LOSS : 0.23026342789332072 [SCORE] : 0.5600093762079875\n",
      "[58/1000]\n",
      "- [VAL] LOSS : 0.27860280871391296 [SCORE] : 0.9484702348709106\n",
      "[59/1000]\n",
      "- [TRAIN] LOSS : 0.22340518633524578 [SCORE] : 0.5600093762079875\n",
      "[59/1000]\n",
      "- [VAL] LOSS : 0.2687080502510071 [SCORE] : 0.9484702348709106\n",
      "[60/1000]\n",
      "- [TRAIN] LOSS : 0.21676009496053059 [SCORE] : 0.5600093762079875\n",
      "[60/1000]\n",
      "- [VAL] LOSS : 0.2591380774974823 [SCORE] : 0.9484702348709106\n",
      "[61/1000]\n",
      "- [TRAIN] LOSS : 0.21038453380266825 [SCORE] : 0.5661632219950358\n",
      "[61/1000]\n",
      "- [VAL] LOSS : 0.24989323318004608 [SCORE] : 0.9484702348709106\n",
      "[62/1000]\n",
      "- [TRAIN] LOSS : 0.20431888699531556 [SCORE] : 0.5661632219950358\n",
      "[62/1000]\n",
      "- [VAL] LOSS : 0.24099351465702057 [SCORE] : 0.9484702348709106\n",
      "[63/1000]\n",
      "- [TRAIN] LOSS : 0.19859937230745953 [SCORE] : 0.5661632219950358\n",
      "[63/1000]\n",
      "- [VAL] LOSS : 0.2328798919916153 [SCORE] : 0.9484702348709106\n",
      "[64/1000]\n",
      "- [TRAIN] LOSS : 0.1930015484491984 [SCORE] : 0.5661632219950358\n",
      "[64/1000]\n",
      "- [VAL] LOSS : 0.22489027678966522 [SCORE] : 0.9484702348709106\n",
      "[65/1000]\n",
      "- [TRAIN] LOSS : 0.18763283491134644 [SCORE] : 0.5661632219950358\n",
      "[65/1000]\n",
      "- [VAL] LOSS : 0.21716122329235077 [SCORE] : 0.9484702348709106\n",
      "[66/1000]\n",
      "- [TRAIN] LOSS : 0.18257468243439992 [SCORE] : 0.5755908330281575\n",
      "[66/1000]\n",
      "- [VAL] LOSS : 0.20994912087917328 [SCORE] : 0.9484702348709106\n",
      "[67/1000]\n",
      "- [TRAIN] LOSS : 0.17761434217294056 [SCORE] : 0.5755908330281575\n",
      "[67/1000]\n",
      "- [VAL] LOSS : 0.2028277963399887 [SCORE] : 0.9484702348709106\n",
      "[68/1000]\n",
      "- [TRAIN] LOSS : 0.17295082310835522 [SCORE] : 0.5755908330281575\n",
      "[68/1000]\n",
      "- [VAL] LOSS : 0.1961885541677475 [SCORE] : 1.0\n",
      "[69/1000]\n",
      "- [TRAIN] LOSS : 0.16850405633449556 [SCORE] : 0.5755908330281575\n",
      "[69/1000]\n",
      "- [VAL] LOSS : 0.19008027017116547 [SCORE] : 1.0\n",
      "[70/1000]\n",
      "- [TRAIN] LOSS : 0.16419817507266998 [SCORE] : 0.5755908330281575\n",
      "[70/1000]\n",
      "- [VAL] LOSS : 0.18421342968940735 [SCORE] : 1.0\n",
      "[71/1000]\n",
      "- [TRAIN] LOSS : 0.1600514809290568 [SCORE] : 0.5755908330281575\n",
      "[71/1000]\n",
      "- [VAL] LOSS : 0.17852093279361725 [SCORE] : 1.0\n",
      "[72/1000]\n",
      "- [TRAIN] LOSS : 0.15614606936772665 [SCORE] : 0.5755908330281575\n",
      "[72/1000]\n",
      "- [VAL] LOSS : 0.17325608432292938 [SCORE] : 1.0\n",
      "[73/1000]\n",
      "- [TRAIN] LOSS : 0.15238483051458995 [SCORE] : 0.5755908330281575\n",
      "[73/1000]\n",
      "- [VAL] LOSS : 0.16826292872428894 [SCORE] : 1.0\n",
      "[74/1000]\n",
      "- [TRAIN] LOSS : 0.14875440200169882 [SCORE] : 0.5755908330281575\n",
      "[74/1000]\n",
      "- [VAL] LOSS : 0.16339197754859924 [SCORE] : 1.0\n",
      "[75/1000]\n",
      "- [TRAIN] LOSS : 0.1453107217947642 [SCORE] : 0.5755908330281575\n",
      "[75/1000]\n",
      "- [VAL] LOSS : 0.1587846726179123 [SCORE] : 1.0\n",
      "[76/1000]\n",
      "- [TRAIN] LOSS : 0.14197278618812562 [SCORE] : 0.5755908330281575\n",
      "[76/1000]\n",
      "- [VAL] LOSS : 0.15421779453754425 [SCORE] : 1.0\n",
      "[77/1000]\n",
      "- [TRAIN] LOSS : 0.13882100383440654 [SCORE] : 0.5755908330281575\n",
      "[77/1000]\n",
      "- [VAL] LOSS : 0.14986911416053772 [SCORE] : 1.0\n",
      "[78/1000]\n",
      "- [TRAIN] LOSS : 0.1358201911052068 [SCORE] : 0.5755908330281575\n",
      "[78/1000]\n",
      "- [VAL] LOSS : 0.14579008519649506 [SCORE] : 1.0\n",
      "[79/1000]\n",
      "- [TRAIN] LOSS : 0.13293068011601766 [SCORE] : 0.5755908330281575\n",
      "[79/1000]\n",
      "- [VAL] LOSS : 0.14188313484191895 [SCORE] : 1.0\n",
      "[80/1000]\n",
      "- [TRAIN] LOSS : 0.13015999694665273 [SCORE] : 0.5755908330281575\n",
      "[80/1000]\n",
      "- [VAL] LOSS : 0.13811154663562775 [SCORE] : 1.0\n",
      "[81/1000]\n",
      "- [TRAIN] LOSS : 0.12750374972820283 [SCORE] : 0.5755908330281575\n",
      "[81/1000]\n",
      "- [VAL] LOSS : 0.13445205986499786 [SCORE] : 1.0\n",
      "[82/1000]\n",
      "- [TRAIN] LOSS : 0.12500042418638865 [SCORE] : 0.5755908330281575\n",
      "[82/1000]\n",
      "- [VAL] LOSS : 0.13105125725269318 [SCORE] : 1.0\n",
      "[83/1000]\n",
      "- [TRAIN] LOSS : 0.12254476795593898 [SCORE] : 0.5755908330281575\n",
      "[83/1000]\n",
      "- [VAL] LOSS : 0.12764619290828705 [SCORE] : 1.0\n",
      "[84/1000]\n",
      "- [TRAIN] LOSS : 0.12023978332678477 [SCORE] : 0.5755908330281575\n",
      "[84/1000]\n",
      "- [VAL] LOSS : 0.12440336495637894 [SCORE] : 1.0\n",
      "[85/1000]\n",
      "- [TRAIN] LOSS : 0.11806564629077912 [SCORE] : 0.5755908330281575\n",
      "[85/1000]\n",
      "- [VAL] LOSS : 0.1214270144701004 [SCORE] : 1.0\n",
      "[86/1000]\n",
      "- [TRAIN] LOSS : 0.11595024019479752 [SCORE] : 0.5755908330281575\n",
      "[86/1000]\n",
      "- [VAL] LOSS : 0.11855820566415787 [SCORE] : 1.0\n",
      "[87/1000]\n",
      "- [TRAIN] LOSS : 0.1139196107784907 [SCORE] : 0.5755908330281575\n",
      "[87/1000]\n",
      "- [VAL] LOSS : 0.11577145010232925 [SCORE] : 1.0\n",
      "[88/1000]\n",
      "- [TRAIN] LOSS : 0.11198986719051997 [SCORE] : 0.5755908330281575\n",
      "[88/1000]\n",
      "- [VAL] LOSS : 0.11311729997396469 [SCORE] : 1.0\n",
      "[89/1000]\n",
      "- [TRAIN] LOSS : 0.11013763944307962 [SCORE] : 0.5755908330281575\n",
      "[89/1000]\n",
      "- [VAL] LOSS : 0.11057181656360626 [SCORE] : 1.0\n",
      "[90/1000]\n",
      "- [TRAIN] LOSS : 0.10835711558659872 [SCORE] : 0.5755908330281575\n",
      "[90/1000]\n",
      "- [VAL] LOSS : 0.10810812562704086 [SCORE] : 1.0\n",
      "[91/1000]\n",
      "- [TRAIN] LOSS : 0.1066658561428388 [SCORE] : 0.5755908330281575\n",
      "[91/1000]\n",
      "- [VAL] LOSS : 0.10577089339494705 [SCORE] : 1.0\n",
      "[92/1000]\n",
      "- [TRAIN] LOSS : 0.1050423910220464 [SCORE] : 0.5755908330281575\n",
      "[92/1000]\n",
      "- [VAL] LOSS : 0.10354384034872055 [SCORE] : 1.0\n",
      "[93/1000]\n",
      "- [TRAIN] LOSS : 0.1034715712070465 [SCORE] : 0.5755908330281575\n",
      "[93/1000]\n",
      "- [VAL] LOSS : 0.10137508064508438 [SCORE] : 1.0\n",
      "[94/1000]\n",
      "- [TRAIN] LOSS : 0.10197593520085017 [SCORE] : 0.5755908330281575\n",
      "[94/1000]\n",
      "- [VAL] LOSS : 0.09929507225751877 [SCORE] : 1.0\n",
      "[95/1000]\n",
      "- [TRAIN] LOSS : 0.10054384569327036 [SCORE] : 0.5755908330281575\n",
      "[95/1000]\n",
      "- [VAL] LOSS : 0.0973043441772461 [SCORE] : 1.0\n",
      "[96/1000]\n",
      "- [TRAIN] LOSS : 0.09916337331136067 [SCORE] : 0.5755908330281575\n",
      "[96/1000]\n",
      "- [VAL] LOSS : 0.09538376331329346 [SCORE] : 1.0\n",
      "[97/1000]\n",
      "- [TRAIN] LOSS : 0.09783996144930522 [SCORE] : 0.5755908330281575\n",
      "[97/1000]\n",
      "- [VAL] LOSS : 0.09353282302618027 [SCORE] : 1.0\n",
      "[98/1000]\n",
      "- [TRAIN] LOSS : 0.09657203406095505 [SCORE] : 0.5755908330281575\n",
      "[98/1000]\n",
      "- [VAL] LOSS : 0.09175727516412735 [SCORE] : 1.0\n",
      "[99/1000]\n",
      "- [TRAIN] LOSS : 0.09536169370015463 [SCORE] : 0.5755908330281575\n",
      "[99/1000]\n",
      "- [VAL] LOSS : 0.09007040411233902 [SCORE] : 1.0\n",
      "[100/1000]\n",
      "- [TRAIN] LOSS : 0.09418037633101145 [SCORE] : 0.5755908330281575\n",
      "[100/1000]\n",
      "- [VAL] LOSS : 0.08841975033283234 [SCORE] : 1.0\n",
      "[101/1000]\n",
      "- [TRAIN] LOSS : 0.09305389076471329 [SCORE] : 0.5755908330281575\n",
      "[101/1000]\n",
      "- [VAL] LOSS : 0.08682500571012497 [SCORE] : 1.0\n",
      "[102/1000]\n",
      "- [TRAIN] LOSS : 0.0919702967007955 [SCORE] : 0.5755908330281575\n",
      "[102/1000]\n",
      "- [VAL] LOSS : 0.08528737723827362 [SCORE] : 1.0\n",
      "[103/1000]\n",
      "- [TRAIN] LOSS : 0.09093503604332606 [SCORE] : 0.5755908330281575\n",
      "[103/1000]\n",
      "- [VAL] LOSS : 0.08382079750299454 [SCORE] : 1.0\n",
      "[104/1000]\n",
      "- [TRAIN] LOSS : 0.089937491218249 [SCORE] : 0.5755908330281575\n",
      "[104/1000]\n",
      "- [VAL] LOSS : 0.08241917937994003 [SCORE] : 1.0\n",
      "[105/1000]\n",
      "- [TRAIN] LOSS : 0.08896709928909938 [SCORE] : 0.5755908330281575\n",
      "[105/1000]\n",
      "- [VAL] LOSS : 0.08104759454727173 [SCORE] : 1.0\n",
      "[106/1000]\n",
      "- [TRAIN] LOSS : 0.08803807894388835 [SCORE] : 0.5755908330281575\n",
      "[106/1000]\n",
      "- [VAL] LOSS : 0.07972059398889542 [SCORE] : 1.0\n",
      "[107/1000]\n",
      "- [TRAIN] LOSS : 0.08714646423856418 [SCORE] : 0.5755908330281575\n",
      "[107/1000]\n",
      "- [VAL] LOSS : 0.07844602316617966 [SCORE] : 1.0\n",
      "[108/1000]\n",
      "- [TRAIN] LOSS : 0.08628677949309349 [SCORE] : 0.5755908330281575\n",
      "[108/1000]\n",
      "- [VAL] LOSS : 0.0772194042801857 [SCORE] : 1.0\n",
      "[109/1000]\n",
      "- [TRAIN] LOSS : 0.08545657495657603 [SCORE] : 0.5755908330281575\n",
      "[109/1000]\n",
      "- [VAL] LOSS : 0.07603365182876587 [SCORE] : 1.0\n",
      "[110/1000]\n",
      "- [TRAIN] LOSS : 0.08465604782104492 [SCORE] : 0.5755908330281575\n",
      "[110/1000]\n",
      "- [VAL] LOSS : 0.07488660514354706 [SCORE] : 1.0\n",
      "[111/1000]\n",
      "- [TRAIN] LOSS : 0.08388455957174301 [SCORE] : 0.5755908330281575\n",
      "[111/1000]\n",
      "- [VAL] LOSS : 0.07377831637859344 [SCORE] : 1.0\n",
      "[112/1000]\n",
      "- [TRAIN] LOSS : 0.08314044798413912 [SCORE] : 0.5755908330281575\n",
      "[112/1000]\n",
      "- [VAL] LOSS : 0.0727076306939125 [SCORE] : 1.0\n",
      "[113/1000]\n",
      "- [TRAIN] LOSS : 0.08242209975918134 [SCORE] : 0.5755908330281575\n",
      "[113/1000]\n",
      "- [VAL] LOSS : 0.07167249917984009 [SCORE] : 1.0\n",
      "[114/1000]\n",
      "- [TRAIN] LOSS : 0.08172848795851072 [SCORE] : 0.5755908330281575\n",
      "[114/1000]\n",
      "- [VAL] LOSS : 0.07067163288593292 [SCORE] : 1.0\n",
      "[115/1000]\n",
      "- [TRAIN] LOSS : 0.08105866511662801 [SCORE] : 0.5755908330281575\n",
      "[115/1000]\n",
      "- [VAL] LOSS : 0.06971119344234467 [SCORE] : 1.0\n",
      "[116/1000]\n",
      "- [TRAIN] LOSS : 0.08041163235902786 [SCORE] : 0.5755908330281575\n",
      "[116/1000]\n",
      "- [VAL] LOSS : 0.06878156214952469 [SCORE] : 1.0\n",
      "[117/1000]\n",
      "- [TRAIN] LOSS : 0.07978636672099432 [SCORE] : 0.5755908330281575\n",
      "[117/1000]\n",
      "- [VAL] LOSS : 0.06788135319948196 [SCORE] : 1.0\n",
      "[118/1000]\n",
      "- [TRAIN] LOSS : 0.079181869328022 [SCORE] : 0.5755908330281575\n",
      "[118/1000]\n",
      "- [VAL] LOSS : 0.06700943410396576 [SCORE] : 1.0\n",
      "[119/1000]\n",
      "- [TRAIN] LOSS : 0.07859736780325571 [SCORE] : 0.5755908330281575\n",
      "[119/1000]\n",
      "- [VAL] LOSS : 0.06616447865962982 [SCORE] : 1.0\n",
      "[120/1000]\n",
      "- [TRAIN] LOSS : 0.07803193032741547 [SCORE] : 0.5755908330281575\n",
      "[120/1000]\n",
      "- [VAL] LOSS : 0.06534547358751297 [SCORE] : 1.0\n",
      "[121/1000]\n",
      "- [TRAIN] LOSS : 0.0774848553041617 [SCORE] : 0.5755908330281575\n",
      "[121/1000]\n",
      "- [VAL] LOSS : 0.06455139070749283 [SCORE] : 1.0\n",
      "[122/1000]\n",
      "- [TRAIN] LOSS : 0.07696076532204946 [SCORE] : 0.5755908330281575\n",
      "[122/1000]\n",
      "- [VAL] LOSS : 0.06379689276218414 [SCORE] : 1.0\n",
      "[123/1000]\n",
      "- [TRAIN] LOSS : 0.07644468769431115 [SCORE] : 0.5755908330281575\n",
      "[123/1000]\n",
      "- [VAL] LOSS : 0.06305587291717529 [SCORE] : 1.0\n",
      "[124/1000]\n",
      "- [TRAIN] LOSS : 0.07594560186068217 [SCORE] : 0.5755908330281575\n",
      "[124/1000]\n",
      "- [VAL] LOSS : 0.06232837587594986 [SCORE] : 1.0\n",
      "[125/1000]\n",
      "- [TRAIN] LOSS : 0.07546656851967176 [SCORE] : 0.5755908330281575\n",
      "[125/1000]\n",
      "- [VAL] LOSS : 0.061625633388757706 [SCORE] : 1.0\n",
      "[126/1000]\n",
      "- [TRAIN] LOSS : 0.07499745190143585 [SCORE] : 0.5755908330281575\n",
      "[126/1000]\n",
      "- [VAL] LOSS : 0.06093299761414528 [SCORE] : 1.0\n",
      "[127/1000]\n",
      "- [TRAIN] LOSS : 0.07455388853947321 [SCORE] : 0.5755908330281575\n",
      "[127/1000]\n",
      "- [VAL] LOSS : 0.060280341655015945 [SCORE] : 1.0\n",
      "[128/1000]\n",
      "- [TRAIN] LOSS : 0.07411526317397753 [SCORE] : 0.5755908330281575\n",
      "[128/1000]\n",
      "- [VAL] LOSS : 0.05964449793100357 [SCORE] : 1.0\n",
      "[129/1000]\n",
      "- [TRAIN] LOSS : 0.0736871620019277 [SCORE] : 0.5755908330281575\n",
      "[129/1000]\n",
      "- [VAL] LOSS : 0.05901682376861572 [SCORE] : 1.0\n",
      "[130/1000]\n",
      "- [TRAIN] LOSS : 0.07327475895484288 [SCORE] : 0.5755908330281575\n",
      "[130/1000]\n",
      "- [VAL] LOSS : 0.05840414762496948 [SCORE] : 1.0\n",
      "[131/1000]\n",
      "- [TRAIN] LOSS : 0.07287649661302567 [SCORE] : 0.5755908330281575\n",
      "[131/1000]\n",
      "- [VAL] LOSS : 0.057811032980680466 [SCORE] : 1.0\n",
      "[132/1000]\n",
      "- [TRAIN] LOSS : 0.07248940095305442 [SCORE] : 0.5755908330281575\n",
      "[132/1000]\n",
      "- [VAL] LOSS : 0.0572357103228569 [SCORE] : 1.0\n",
      "[133/1000]\n",
      "- [TRAIN] LOSS : 0.07211270754535994 [SCORE] : 0.5755908330281575\n",
      "[133/1000]\n",
      "- [VAL] LOSS : 0.05667530745267868 [SCORE] : 1.0\n",
      "[134/1000]\n",
      "- [TRAIN] LOSS : 0.07174247751633327 [SCORE] : 0.5755908330281575\n",
      "[134/1000]\n",
      "- [VAL] LOSS : 0.056117571890354156 [SCORE] : 1.0\n",
      "[135/1000]\n",
      "- [TRAIN] LOSS : 0.07139475668470065 [SCORE] : 0.5755908330281575\n",
      "[135/1000]\n",
      "- [VAL] LOSS : 0.055592138320207596 [SCORE] : 1.0\n",
      "[136/1000]\n",
      "- [TRAIN] LOSS : 0.0710498129328092 [SCORE] : 0.5755908330281575\n",
      "[136/1000]\n",
      "- [VAL] LOSS : 0.05508173257112503 [SCORE] : 1.0\n",
      "[137/1000]\n",
      "- [TRAIN] LOSS : 0.07071129257480303 [SCORE] : 0.5755908330281575\n",
      "[137/1000]\n",
      "- [VAL] LOSS : 0.05457722768187523 [SCORE] : 1.0\n",
      "[138/1000]\n",
      "- [TRAIN] LOSS : 0.07037783910830815 [SCORE] : 0.5755908330281575\n",
      "[138/1000]\n",
      "- [VAL] LOSS : 0.05406438559293747 [SCORE] : 1.0\n",
      "[139/1000]\n",
      "- [TRAIN] LOSS : 0.07006865777075291 [SCORE] : 0.5755908330281575\n",
      "[139/1000]\n",
      "- [VAL] LOSS : 0.05357951298356056 [SCORE] : 1.0\n",
      "[140/1000]\n",
      "- [TRAIN] LOSS : 0.06976834858457247 [SCORE] : 0.5755908330281575\n",
      "[140/1000]\n",
      "- [VAL] LOSS : 0.05312372371554375 [SCORE] : 1.0\n",
      "[141/1000]\n",
      "- [TRAIN] LOSS : 0.06946586792667707 [SCORE] : 0.5755908330281575\n",
      "[141/1000]\n",
      "- [VAL] LOSS : 0.05267275497317314 [SCORE] : 1.0\n",
      "[142/1000]\n",
      "- [TRAIN] LOSS : 0.06917389519512654 [SCORE] : 0.5755908330281575\n",
      "[142/1000]\n",
      "- [VAL] LOSS : 0.05222995951771736 [SCORE] : 1.0\n",
      "[143/1000]\n",
      "- [TRAIN] LOSS : 0.06889052242040634 [SCORE] : 0.5755908330281575\n",
      "[143/1000]\n",
      "- [VAL] LOSS : 0.051797445863485336 [SCORE] : 1.0\n",
      "[144/1000]\n",
      "- [TRAIN] LOSS : 0.06861488421758016 [SCORE] : 0.5755908330281575\n",
      "[144/1000]\n",
      "- [VAL] LOSS : 0.05137551203370094 [SCORE] : 1.0\n",
      "[145/1000]\n",
      "- [TRAIN] LOSS : 0.06834649990002314 [SCORE] : 0.5755908330281575\n",
      "[145/1000]\n",
      "- [VAL] LOSS : 0.05096397548913956 [SCORE] : 1.0\n",
      "[146/1000]\n",
      "- [TRAIN] LOSS : 0.06808505232135455 [SCORE] : 0.5755908330281575\n",
      "[146/1000]\n",
      "- [VAL] LOSS : 0.05056226998567581 [SCORE] : 1.0\n",
      "[147/1000]\n",
      "- [TRAIN] LOSS : 0.06783026667932669 [SCORE] : 0.5755908330281575\n",
      "[147/1000]\n",
      "- [VAL] LOSS : 0.050170283764600754 [SCORE] : 1.0\n",
      "[148/1000]\n",
      "- [TRAIN] LOSS : 0.06758515760302544 [SCORE] : 0.5755908330281575\n",
      "[148/1000]\n",
      "- [VAL] LOSS : 0.0497937835752964 [SCORE] : 1.0\n",
      "[149/1000]\n",
      "- [TRAIN] LOSS : 0.06733886674046516 [SCORE] : 0.5755908330281575\n",
      "[149/1000]\n",
      "- [VAL] LOSS : 0.04941661283373833 [SCORE] : 1.0\n",
      "[150/1000]\n",
      "- [TRAIN] LOSS : 0.06710725662608942 [SCORE] : 0.5755908330281575\n",
      "[150/1000]\n",
      "- [VAL] LOSS : 0.04905598983168602 [SCORE] : 1.0\n",
      "[151/1000]\n",
      "- [TRAIN] LOSS : 0.06687679712971052 [SCORE] : 0.5755908330281575\n",
      "[151/1000]\n",
      "- [VAL] LOSS : 0.04870258644223213 [SCORE] : 1.0\n",
      "[152/1000]\n",
      "- [TRAIN] LOSS : 0.06665125377476215 [SCORE] : 0.5755908330281575\n",
      "[152/1000]\n",
      "- [VAL] LOSS : 0.048353902995586395 [SCORE] : 1.0\n",
      "[153/1000]\n",
      "- [TRAIN] LOSS : 0.06643258295953273 [SCORE] : 0.5755908330281575\n",
      "[153/1000]\n",
      "- [VAL] LOSS : 0.04801235720515251 [SCORE] : 1.0\n",
      "[154/1000]\n",
      "- [TRAIN] LOSS : 0.06622005912164847 [SCORE] : 0.5755908330281575\n",
      "[154/1000]\n",
      "- [VAL] LOSS : 0.04767964407801628 [SCORE] : 1.0\n",
      "[155/1000]\n",
      "- [TRAIN] LOSS : 0.06601160379747549 [SCORE] : 0.5755908330281575\n",
      "[155/1000]\n",
      "- [VAL] LOSS : 0.047353215515613556 [SCORE] : 1.0\n",
      "[156/1000]\n",
      "- [TRAIN] LOSS : 0.06580870288113753 [SCORE] : 0.5755908330281575\n",
      "[156/1000]\n",
      "- [VAL] LOSS : 0.04703393206000328 [SCORE] : 1.0\n",
      "[157/1000]\n",
      "- [TRAIN] LOSS : 0.06561087196071942 [SCORE] : 0.5755908330281575\n",
      "[157/1000]\n",
      "- [VAL] LOSS : 0.04672209173440933 [SCORE] : 1.0\n",
      "[158/1000]\n",
      "- [TRAIN] LOSS : 0.06541754441956679 [SCORE] : 0.5755908330281575\n",
      "[158/1000]\n",
      "- [VAL] LOSS : 0.046417154371738434 [SCORE] : 1.0\n",
      "[159/1000]\n",
      "- [TRAIN] LOSS : 0.06522857944170633 [SCORE] : 0.5755908330281575\n",
      "[159/1000]\n",
      "- [VAL] LOSS : 0.04611852020025253 [SCORE] : 1.0\n",
      "[160/1000]\n",
      "- [TRAIN] LOSS : 0.0650439859678348 [SCORE] : 0.5755908330281575\n",
      "[160/1000]\n",
      "- [VAL] LOSS : 0.04582597687840462 [SCORE] : 1.0\n",
      "[161/1000]\n",
      "- [TRAIN] LOSS : 0.06486373866597811 [SCORE] : 0.5755908330281575\n",
      "[161/1000]\n",
      "- [VAL] LOSS : 0.04553940147161484 [SCORE] : 1.0\n",
      "[162/1000]\n",
      "- [TRAIN] LOSS : 0.06468768902122975 [SCORE] : 0.5755908330281575\n",
      "[162/1000]\n",
      "- [VAL] LOSS : 0.045258697122335434 [SCORE] : 1.0\n",
      "[163/1000]\n",
      "- [TRAIN] LOSS : 0.06451564530531566 [SCORE] : 0.5755908330281575\n",
      "[163/1000]\n",
      "- [VAL] LOSS : 0.04498383030295372 [SCORE] : 1.0\n",
      "[164/1000]\n",
      "- [TRAIN] LOSS : 0.06434744348128636 [SCORE] : 0.5755908330281575\n",
      "[164/1000]\n",
      "- [VAL] LOSS : 0.04471445456147194 [SCORE] : 1.0\n",
      "[165/1000]\n",
      "- [TRAIN] LOSS : 0.06418305188417435 [SCORE] : 0.5755908330281575\n",
      "[165/1000]\n",
      "- [VAL] LOSS : 0.044450484216213226 [SCORE] : 1.0\n",
      "[166/1000]\n",
      "- [TRAIN] LOSS : 0.06402230436603228 [SCORE] : 0.5755908330281575\n",
      "[166/1000]\n",
      "- [VAL] LOSS : 0.044191740453243256 [SCORE] : 1.0\n",
      "[167/1000]\n",
      "- [TRAIN] LOSS : 0.06386517894764741 [SCORE] : 0.5755908330281575\n",
      "[167/1000]\n",
      "- [VAL] LOSS : 0.04393818601965904 [SCORE] : 1.0\n",
      "[168/1000]\n",
      "- [TRAIN] LOSS : 0.06371148154139519 [SCORE] : 0.5755908330281575\n",
      "[168/1000]\n",
      "- [VAL] LOSS : 0.04368964210152626 [SCORE] : 1.0\n",
      "[169/1000]\n",
      "- [TRAIN] LOSS : 0.06356111541390419 [SCORE] : 0.5755908330281575\n",
      "[169/1000]\n",
      "- [VAL] LOSS : 0.043445877730846405 [SCORE] : 1.0\n",
      "[170/1000]\n",
      "- [TRAIN] LOSS : 0.06341402406493822 [SCORE] : 0.5755908330281575\n",
      "[170/1000]\n",
      "- [VAL] LOSS : 0.04320687800645828 [SCORE] : 1.0\n",
      "[171/1000]\n",
      "- [TRAIN] LOSS : 0.06327011436223984 [SCORE] : 0.5755908330281575\n",
      "[171/1000]\n",
      "- [VAL] LOSS : 0.04297247156500816 [SCORE] : 1.0\n",
      "[172/1000]\n",
      "- [TRAIN] LOSS : 0.06312928572297097 [SCORE] : 0.5755908330281575\n",
      "[172/1000]\n",
      "- [VAL] LOSS : 0.0427425280213356 [SCORE] : 1.0\n",
      "[173/1000]\n",
      "- [TRAIN] LOSS : 0.06299139397839705 [SCORE] : 0.5755908330281575\n",
      "[173/1000]\n",
      "- [VAL] LOSS : 0.04251695051789284 [SCORE] : 1.0\n",
      "[174/1000]\n",
      "- [TRAIN] LOSS : 0.06285646880666415 [SCORE] : 0.5755908330281575\n",
      "[174/1000]\n",
      "- [VAL] LOSS : 0.042295608669519424 [SCORE] : 1.0\n",
      "[175/1000]\n",
      "- [TRAIN] LOSS : 0.06272435523569583 [SCORE] : 0.5755908330281575\n",
      "[175/1000]\n",
      "- [VAL] LOSS : 0.04207845777273178 [SCORE] : 1.0\n",
      "[176/1000]\n",
      "- [TRAIN] LOSS : 0.06259498136738936 [SCORE] : 0.5755908330281575\n",
      "[176/1000]\n",
      "- [VAL] LOSS : 0.04186532273888588 [SCORE] : 1.0\n",
      "[177/1000]\n",
      "- [TRAIN] LOSS : 0.06246825158596039 [SCORE] : 0.5755908330281575\n",
      "[177/1000]\n",
      "- [VAL] LOSS : 0.04165611416101456 [SCORE] : 1.0\n",
      "[178/1000]\n",
      "- [TRAIN] LOSS : 0.062344181289275484 [SCORE] : 0.5755908330281575\n",
      "[178/1000]\n",
      "- [VAL] LOSS : 0.04145071655511856 [SCORE] : 1.0\n",
      "[179/1000]\n",
      "- [TRAIN] LOSS : 0.0622225987414519 [SCORE] : 0.5755908330281575\n",
      "[179/1000]\n",
      "- [VAL] LOSS : 0.041249025613069534 [SCORE] : 1.0\n",
      "[180/1000]\n",
      "- [TRAIN] LOSS : 0.06210348842044671 [SCORE] : 0.5755908330281575\n",
      "[180/1000]\n",
      "- [VAL] LOSS : 0.041051022708415985 [SCORE] : 1.0\n",
      "[181/1000]\n",
      "- [TRAIN] LOSS : 0.06198676551381747 [SCORE] : 0.5755908330281575\n",
      "[181/1000]\n",
      "- [VAL] LOSS : 0.04085659235715866 [SCORE] : 1.0\n",
      "[182/1000]\n",
      "- [TRAIN] LOSS : 0.061872401957710585 [SCORE] : 0.5755908330281575\n",
      "[182/1000]\n",
      "- [VAL] LOSS : 0.04066554829478264 [SCORE] : 1.0\n",
      "[183/1000]\n",
      "- [TRAIN] LOSS : 0.06176025519768397 [SCORE] : 0.5755908330281575\n",
      "[183/1000]\n",
      "- [VAL] LOSS : 0.04047796502709389 [SCORE] : 1.0\n",
      "[184/1000]\n",
      "- [TRAIN] LOSS : 0.06165039415160815 [SCORE] : 0.5755908330281575\n",
      "[184/1000]\n",
      "- [VAL] LOSS : 0.040293604135513306 [SCORE] : 1.0\n",
      "[185/1000]\n",
      "- [TRAIN] LOSS : 0.061542632927497225 [SCORE] : 0.5755908330281575\n",
      "[185/1000]\n",
      "- [VAL] LOSS : 0.040112514048814774 [SCORE] : 1.0\n",
      "[186/1000]\n",
      "- [TRAIN] LOSS : 0.06143698903421561 [SCORE] : 0.5755908330281575\n",
      "[186/1000]\n",
      "- [VAL] LOSS : 0.03993454575538635 [SCORE] : 1.0\n",
      "[187/1000]\n",
      "- [TRAIN] LOSS : 0.061333364869157476 [SCORE] : 0.5755908330281575\n",
      "[187/1000]\n",
      "- [VAL] LOSS : 0.03975965827703476 [SCORE] : 1.0\n",
      "[188/1000]\n",
      "- [TRAIN] LOSS : 0.061231771111488344 [SCORE] : 0.5755908330281575\n",
      "[188/1000]\n",
      "- [VAL] LOSS : 0.03958774358034134 [SCORE] : 1.0\n",
      "[189/1000]\n",
      "- [TRAIN] LOSS : 0.06113207563757896 [SCORE] : 0.5755908330281575\n",
      "[189/1000]\n",
      "- [VAL] LOSS : 0.03941873461008072 [SCORE] : 1.0\n",
      "[190/1000]\n",
      "- [TRAIN] LOSS : 0.06103428180019061 [SCORE] : 0.5755908330281575\n",
      "[190/1000]\n",
      "- [VAL] LOSS : 0.03925259783864021 [SCORE] : 1.0\n",
      "[191/1000]\n",
      "- [TRAIN] LOSS : 0.06093834886948268 [SCORE] : 0.5755908330281575\n",
      "[191/1000]\n",
      "- [VAL] LOSS : 0.039089225232601166 [SCORE] : 1.0\n",
      "[192/1000]\n",
      "- [TRAIN] LOSS : 0.06084420569241047 [SCORE] : 0.5755908330281575\n",
      "[192/1000]\n",
      "- [VAL] LOSS : 0.03892857953906059 [SCORE] : 1.0\n",
      "[193/1000]\n",
      "- [TRAIN] LOSS : 0.060751800611615184 [SCORE] : 0.5755908330281575\n",
      "[193/1000]\n",
      "- [VAL] LOSS : 0.03877061977982521 [SCORE] : 1.0\n",
      "[194/1000]\n",
      "- [TRAIN] LOSS : 0.06066112630069256 [SCORE] : 0.5755908330281575\n",
      "[194/1000]\n",
      "- [VAL] LOSS : 0.03861520439386368 [SCORE] : 1.0\n",
      "[195/1000]\n",
      "- [TRAIN] LOSS : 0.060572109060982864 [SCORE] : 0.5755908330281575\n",
      "[195/1000]\n",
      "- [VAL] LOSS : 0.03846235200762749 [SCORE] : 1.0\n",
      "[196/1000]\n",
      "- [TRAIN] LOSS : 0.060484751438101135 [SCORE] : 0.5755908330281575\n",
      "[196/1000]\n",
      "- [VAL] LOSS : 0.038311995565891266 [SCORE] : 1.0\n",
      "[197/1000]\n",
      "- [TRAIN] LOSS : 0.06039893695463737 [SCORE] : 0.5755908330281575\n",
      "[197/1000]\n",
      "- [VAL] LOSS : 0.03816402703523636 [SCORE] : 1.0\n",
      "[198/1000]\n",
      "- [TRAIN] LOSS : 0.0603146996969978 [SCORE] : 0.5755908330281575\n",
      "[198/1000]\n",
      "- [VAL] LOSS : 0.03801850602030754 [SCORE] : 1.0\n",
      "[199/1000]\n",
      "- [TRAIN] LOSS : 0.06023195410768191 [SCORE] : 0.5755908330281575\n",
      "[199/1000]\n",
      "- [VAL] LOSS : 0.0378752164542675 [SCORE] : 1.0\n",
      "[200/1000]\n",
      "- [TRAIN] LOSS : 0.06015070968617996 [SCORE] : 0.5755908330281575\n",
      "[200/1000]\n",
      "- [VAL] LOSS : 0.03773421049118042 [SCORE] : 1.0\n",
      "[201/1000]\n",
      "- [TRAIN] LOSS : 0.06007089385141929 [SCORE] : 0.5755908330281575\n",
      "[201/1000]\n",
      "- [VAL] LOSS : 0.03759543225169182 [SCORE] : 1.0\n",
      "[202/1000]\n",
      "- [TRAIN] LOSS : 0.059992466246088345 [SCORE] : 0.5755908330281575\n",
      "[202/1000]\n",
      "- [VAL] LOSS : 0.03745882958173752 [SCORE] : 1.0\n",
      "[203/1000]\n",
      "- [TRAIN] LOSS : 0.05991546182582776 [SCORE] : 0.5755908330281575\n",
      "[203/1000]\n",
      "- [VAL] LOSS : 0.03732434660196304 [SCORE] : 1.0\n",
      "[204/1000]\n",
      "- [TRAIN] LOSS : 0.059839801552395024 [SCORE] : 0.5755908330281575\n",
      "[204/1000]\n",
      "- [VAL] LOSS : 0.03719336912035942 [SCORE] : 1.0\n",
      "[205/1000]\n",
      "- [TRAIN] LOSS : 0.059767647025485836 [SCORE] : 0.5755908330281575\n",
      "[205/1000]\n",
      "- [VAL] LOSS : 0.037068139761686325 [SCORE] : 1.0\n",
      "[206/1000]\n",
      "- [TRAIN] LOSS : 0.05968971618761619 [SCORE] : 0.5755908330281575\n",
      "[206/1000]\n",
      "- [VAL] LOSS : 0.03693575784564018 [SCORE] : 1.0\n",
      "[207/1000]\n",
      "- [TRAIN] LOSS : 0.05961841146151225 [SCORE] : 0.5755908330281575\n",
      "[207/1000]\n",
      "- [VAL] LOSS : 0.03680550307035446 [SCORE] : 1.0\n",
      "[208/1000]\n",
      "- [TRAIN] LOSS : 0.05955003363390764 [SCORE] : 0.5755908330281575\n",
      "[208/1000]\n",
      "- [VAL] LOSS : 0.03668200597167015 [SCORE] : 1.0\n",
      "[209/1000]\n",
      "- [TRAIN] LOSS : 0.05948350007335345 [SCORE] : 0.5755908330281575\n",
      "[209/1000]\n",
      "- [VAL] LOSS : 0.036565590649843216 [SCORE] : 1.0\n",
      "[210/1000]\n",
      "- [TRAIN] LOSS : 0.05941019834329685 [SCORE] : 0.5755908330281575\n",
      "[210/1000]\n",
      "- [VAL] LOSS : 0.03644179180264473 [SCORE] : 1.0\n",
      "[211/1000]\n",
      "- [TRAIN] LOSS : 0.05934304700543483 [SCORE] : 0.5755908330281575\n",
      "[211/1000]\n",
      "- [VAL] LOSS : 0.03632020205259323 [SCORE] : 1.0\n",
      "[212/1000]\n",
      "- [TRAIN] LOSS : 0.05928110765914122 [SCORE] : 0.5755908330281575\n",
      "[212/1000]\n",
      "- [VAL] LOSS : 0.03620696812868118 [SCORE] : 1.0\n",
      "[213/1000]\n",
      "- [TRAIN] LOSS : 0.059212676559885345 [SCORE] : 0.5755908330281575\n",
      "[213/1000]\n",
      "- [VAL] LOSS : 0.03608895093202591 [SCORE] : 1.0\n",
      "[214/1000]\n",
      "- [TRAIN] LOSS : 0.05914903761198123 [SCORE] : 0.5755908330281575\n",
      "[214/1000]\n",
      "- [VAL] LOSS : 0.03597234562039375 [SCORE] : 1.0\n",
      "[215/1000]\n",
      "- [TRAIN] LOSS : 0.059087770308057466 [SCORE] : 0.5755908330281575\n",
      "[215/1000]\n",
      "- [VAL] LOSS : 0.035860948264598846 [SCORE] : 1.0\n",
      "[216/1000]\n",
      "- [TRAIN] LOSS : 0.05902860779315233 [SCORE] : 0.5755908330281575\n",
      "[216/1000]\n",
      "- [VAL] LOSS : 0.03575575724244118 [SCORE] : 1.0\n",
      "[217/1000]\n",
      "- [TRAIN] LOSS : 0.05896329761793216 [SCORE] : 0.5755908330281575\n",
      "[217/1000]\n",
      "- [VAL] LOSS : 0.03564390540122986 [SCORE] : 1.0\n",
      "[218/1000]\n",
      "- [TRAIN] LOSS : 0.05890358636776606 [SCORE] : 0.5755908330281575\n",
      "[218/1000]\n",
      "- [VAL] LOSS : 0.03553418070077896 [SCORE] : 1.0\n",
      "[219/1000]\n",
      "- [TRAIN] LOSS : 0.05884850062429905 [SCORE] : 0.5755908330281575\n",
      "[219/1000]\n",
      "- [VAL] LOSS : 0.03543202951550484 [SCORE] : 1.0\n",
      "[220/1000]\n",
      "- [TRAIN] LOSS : 0.05878729149699211 [SCORE] : 0.5755908330281575\n",
      "[220/1000]\n",
      "- [VAL] LOSS : 0.0353253073990345 [SCORE] : 1.0\n",
      "[221/1000]\n",
      "- [TRAIN] LOSS : 0.05873043065269788 [SCORE] : 0.5755908330281575\n",
      "[221/1000]\n",
      "- [VAL] LOSS : 0.035219740122556686 [SCORE] : 1.0\n",
      "[222/1000]\n",
      "- [TRAIN] LOSS : 0.05867582019418478 [SCORE] : 0.5755908330281575\n",
      "[222/1000]\n",
      "- [VAL] LOSS : 0.03511892259120941 [SCORE] : 1.0\n",
      "[223/1000]\n",
      "- [TRAIN] LOSS : 0.058623065054416654 [SCORE] : 0.5755908330281575\n",
      "[223/1000]\n",
      "- [VAL] LOSS : 0.03502384200692177 [SCORE] : 1.0\n",
      "[224/1000]\n",
      "- [TRAIN] LOSS : 0.058564484243591626 [SCORE] : 0.5755908330281575\n",
      "[224/1000]\n",
      "- [VAL] LOSS : 0.03492255136370659 [SCORE] : 1.0\n",
      "[225/1000]\n",
      "- [TRAIN] LOSS : 0.05850735573718945 [SCORE] : 0.5755908330281575\n",
      "[225/1000]\n",
      "- [VAL] LOSS : 0.03481551259756088 [SCORE] : 1.0\n",
      "[226/1000]\n",
      "- [TRAIN] LOSS : 0.058461297179261844 [SCORE] : 0.5755908330281575\n",
      "[226/1000]\n",
      "- [VAL] LOSS : 0.03472072631120682 [SCORE] : 1.0\n",
      "[227/1000]\n",
      "- [TRAIN] LOSS : 0.05841134948035081 [SCORE] : 0.5755908330281575\n",
      "[227/1000]\n",
      "- [VAL] LOSS : 0.03463001176714897 [SCORE] : 1.0\n",
      "[228/1000]\n",
      "- [TRAIN] LOSS : 0.058357188540200396 [SCORE] : 0.5755908330281575\n",
      "[228/1000]\n",
      "- [VAL] LOSS : 0.03453436866402626 [SCORE] : 1.0\n",
      "[229/1000]\n",
      "- [TRAIN] LOSS : 0.0583077813188235 [SCORE] : 0.5755908330281575\n",
      "[229/1000]\n",
      "- [VAL] LOSS : 0.03444132208824158 [SCORE] : 1.0\n",
      "[230/1000]\n",
      "- [TRAIN] LOSS : 0.058261712454259394 [SCORE] : 0.5755908330281575\n",
      "[230/1000]\n",
      "- [VAL] LOSS : 0.03435611352324486 [SCORE] : 1.0\n",
      "[231/1000]\n",
      "- [TRAIN] LOSS : 0.058211722100774445 [SCORE] : 0.5755908330281575\n",
      "[231/1000]\n",
      "- [VAL] LOSS : 0.03426934778690338 [SCORE] : 1.0\n",
      "[232/1000]\n",
      "- [TRAIN] LOSS : 0.058160133908192316 [SCORE] : 0.5755908330281575\n",
      "[232/1000]\n",
      "- [VAL] LOSS : 0.03417649120092392 [SCORE] : 1.0\n",
      "[233/1000]\n",
      "- [TRAIN] LOSS : 0.0581145732353131 [SCORE] : 0.5755908330281575\n",
      "[233/1000]\n",
      "- [VAL] LOSS : 0.03408751264214516 [SCORE] : 1.0\n",
      "[234/1000]\n",
      "- [TRAIN] LOSS : 0.05807190854102373 [SCORE] : 0.5755908330281575\n",
      "[234/1000]\n",
      "- [VAL] LOSS : 0.03400612995028496 [SCORE] : 1.0\n",
      "[235/1000]\n",
      "- [TRAIN] LOSS : 0.058023163800438246 [SCORE] : 0.5755908330281575\n",
      "[235/1000]\n",
      "- [VAL] LOSS : 0.03392154350876808 [SCORE] : 1.0\n",
      "[236/1000]\n",
      "- [TRAIN] LOSS : 0.05797935649752617 [SCORE] : 0.5755908330281575\n",
      "[236/1000]\n",
      "- [VAL] LOSS : 0.0338398814201355 [SCORE] : 1.0\n",
      "[237/1000]\n",
      "- [TRAIN] LOSS : 0.05793244335800409 [SCORE] : 0.5755908330281575\n",
      "[237/1000]\n",
      "- [VAL] LOSS : 0.033755432814359665 [SCORE] : 1.0\n",
      "[238/1000]\n",
      "- [TRAIN] LOSS : 0.05789112330724796 [SCORE] : 0.5755908330281575\n",
      "[238/1000]\n",
      "- [VAL] LOSS : 0.03367570787668228 [SCORE] : 1.0\n",
      "[239/1000]\n",
      "- [TRAIN] LOSS : 0.057845946339269476 [SCORE] : 0.5755908330281575\n",
      "[239/1000]\n",
      "- [VAL] LOSS : 0.03359407186508179 [SCORE] : 1.0\n",
      "[240/1000]\n",
      "- [TRAIN] LOSS : 0.057805583191414676 [SCORE] : 0.5755908330281575\n",
      "[240/1000]\n",
      "- [VAL] LOSS : 0.03351685777306557 [SCORE] : 1.0\n",
      "[241/1000]\n",
      "- [TRAIN] LOSS : 0.05776135548949242 [SCORE] : 0.5755908330281575\n",
      "[241/1000]\n",
      "- [VAL] LOSS : 0.033437203615903854 [SCORE] : 1.0\n",
      "[242/1000]\n",
      "- [TRAIN] LOSS : 0.05772212228427331 [SCORE] : 0.5755908330281575\n",
      "[242/1000]\n",
      "- [VAL] LOSS : 0.033361680805683136 [SCORE] : 1.0\n",
      "[243/1000]\n",
      "- [TRAIN] LOSS : 0.05767919321854909 [SCORE] : 0.5755908330281575\n",
      "[243/1000]\n",
      "- [VAL] LOSS : 0.033283963799476624 [SCORE] : 1.0\n",
      "[244/1000]\n",
      "- [TRAIN] LOSS : 0.057641126401722434 [SCORE] : 0.5755908330281575\n",
      "[244/1000]\n",
      "- [VAL] LOSS : 0.03321037441492081 [SCORE] : 1.0\n",
      "[245/1000]\n",
      "- [TRAIN] LOSS : 0.05759933857868115 [SCORE] : 0.5755908330281575\n",
      "[245/1000]\n",
      "- [VAL] LOSS : 0.033134642988443375 [SCORE] : 1.0\n",
      "[246/1000]\n",
      "- [TRAIN] LOSS : 0.057562337070703504 [SCORE] : 0.5755908330281575\n",
      "[246/1000]\n",
      "- [VAL] LOSS : 0.03306294605135918 [SCORE] : 1.0\n",
      "[247/1000]\n",
      "- [TRAIN] LOSS : 0.057521682046353814 [SCORE] : 0.5755908330281575\n",
      "[247/1000]\n",
      "- [VAL] LOSS : 0.03298903629183769 [SCORE] : 1.0\n",
      "[248/1000]\n",
      "- [TRAIN] LOSS : 0.057485718342165155 [SCORE] : 0.5755908330281575\n",
      "[248/1000]\n",
      "- [VAL] LOSS : 0.03291910141706467 [SCORE] : 1.0\n",
      "[249/1000]\n",
      "- [TRAIN] LOSS : 0.05744668369491895 [SCORE] : 0.5755908330281575\n",
      "[249/1000]\n",
      "- [VAL] LOSS : 0.032847560942173004 [SCORE] : 1.0\n",
      "[250/1000]\n",
      "- [TRAIN] LOSS : 0.05741176300992568 [SCORE] : 0.5755908330281575\n",
      "[250/1000]\n",
      "- [VAL] LOSS : 0.03278006985783577 [SCORE] : 1.0\n",
      "[251/1000]\n",
      "- [TRAIN] LOSS : 0.05737290618320306 [SCORE] : 0.5755908330281575\n",
      "[251/1000]\n",
      "- [VAL] LOSS : 0.03270981088280678 [SCORE] : 1.0\n",
      "[252/1000]\n",
      "- [TRAIN] LOSS : 0.05733874626457691 [SCORE] : 0.5755908330281575\n",
      "[252/1000]\n",
      "- [VAL] LOSS : 0.03264300897717476 [SCORE] : 1.0\n",
      "[253/1000]\n",
      "- [TRAIN] LOSS : 0.05730129697670539 [SCORE] : 0.5755908330281575\n",
      "[253/1000]\n",
      "- [VAL] LOSS : 0.032574065029621124 [SCORE] : 1.0\n",
      "[254/1000]\n",
      "- [TRAIN] LOSS : 0.05726834150652091 [SCORE] : 0.5755908330281575\n",
      "[254/1000]\n",
      "- [VAL] LOSS : 0.03250900283455849 [SCORE] : 1.0\n",
      "[255/1000]\n",
      "- [TRAIN] LOSS : 0.05723188786456982 [SCORE] : 0.5755908330281575\n",
      "[255/1000]\n",
      "- [VAL] LOSS : 0.03244173154234886 [SCORE] : 1.0\n",
      "[256/1000]\n",
      "- [TRAIN] LOSS : 0.05719991705069939 [SCORE] : 0.5755908330281575\n",
      "[256/1000]\n",
      "- [VAL] LOSS : 0.032379310578107834 [SCORE] : 1.0\n",
      "[257/1000]\n",
      "- [TRAIN] LOSS : 0.05716581729551156 [SCORE] : 0.5755908330281575\n",
      "[257/1000]\n",
      "- [VAL] LOSS : 0.03231656923890114 [SCORE] : 1.0\n",
      "[258/1000]\n",
      "- [TRAIN] LOSS : 0.05712999297926823 [SCORE] : 0.5755908330281575\n",
      "[258/1000]\n",
      "- [VAL] LOSS : 0.032249365001916885 [SCORE] : 1.0\n",
      "[259/1000]\n",
      "- [TRAIN] LOSS : 0.05709830305228631 [SCORE] : 0.5755908330281575\n",
      "[259/1000]\n",
      "- [VAL] LOSS : 0.032184891402721405 [SCORE] : 1.0\n",
      "[260/1000]\n",
      "- [TRAIN] LOSS : 0.057069098701079686 [SCORE] : 0.5755908330281575\n",
      "[260/1000]\n",
      "- [VAL] LOSS : 0.03212716430425644 [SCORE] : 1.0\n",
      "[261/1000]\n",
      "- [TRAIN] LOSS : 0.05703596323728562 [SCORE] : 0.5755908330281575\n",
      "[261/1000]\n",
      "- [VAL] LOSS : 0.032068125903606415 [SCORE] : 1.0\n",
      "[262/1000]\n",
      "- [TRAIN] LOSS : 0.05700137484818697 [SCORE] : 0.5755908330281575\n",
      "[262/1000]\n",
      "- [VAL] LOSS : 0.03200460597872734 [SCORE] : 1.0\n",
      "[263/1000]\n",
      "- [TRAIN] LOSS : 0.056972656833628815 [SCORE] : 0.5755908330281575\n",
      "[263/1000]\n",
      "- [VAL] LOSS : 0.03194606304168701 [SCORE] : 1.0\n",
      "[264/1000]\n",
      "- [TRAIN] LOSS : 0.056942027186354 [SCORE] : 0.5755908330281575\n",
      "[264/1000]\n",
      "- [VAL] LOSS : 0.03188789635896683 [SCORE] : 1.0\n",
      "[265/1000]\n",
      "- [TRAIN] LOSS : 0.056909569166600706 [SCORE] : 0.5755908330281575\n",
      "[265/1000]\n",
      "- [VAL] LOSS : 0.031826216727495193 [SCORE] : 1.0\n",
      "[266/1000]\n",
      "- [TRAIN] LOSS : 0.05688074280818303 [SCORE] : 0.5755908330281575\n",
      "[266/1000]\n",
      "- [VAL] LOSS : 0.03176731616258621 [SCORE] : 1.0\n",
      "[267/1000]\n",
      "- [TRAIN] LOSS : 0.05685369695226351 [SCORE] : 0.5755908330281575\n",
      "[267/1000]\n",
      "- [VAL] LOSS : 0.03171418234705925 [SCORE] : 1.0\n",
      "[268/1000]\n",
      "- [TRAIN] LOSS : 0.056823222214976946 [SCORE] : 0.5755908330281575\n",
      "[268/1000]\n",
      "- [VAL] LOSS : 0.03165942057967186 [SCORE] : 1.0\n",
      "[269/1000]\n",
      "- [TRAIN] LOSS : 0.05679049113144477 [SCORE] : 0.5755908330281575\n",
      "[269/1000]\n",
      "- [VAL] LOSS : 0.0315985307097435 [SCORE] : 1.0\n",
      "[270/1000]\n",
      "- [TRAIN] LOSS : 0.056766065582633016 [SCORE] : 0.5755908330281575\n",
      "[270/1000]\n",
      "- [VAL] LOSS : 0.03154512122273445 [SCORE] : 1.0\n",
      "[271/1000]\n",
      "- [TRAIN] LOSS : 0.056738332255433004 [SCORE] : 0.5755908330281575\n",
      "[271/1000]\n",
      "- [VAL] LOSS : 0.03149370476603508 [SCORE] : 1.0\n",
      "[272/1000]\n",
      "- [TRAIN] LOSS : 0.05670815696939826 [SCORE] : 0.5755908330281575\n",
      "[272/1000]\n",
      "- [VAL] LOSS : 0.03143836185336113 [SCORE] : 1.0\n",
      "[273/1000]\n",
      "- [TRAIN] LOSS : 0.05667896919573347 [SCORE] : 0.5755908330281575\n",
      "[273/1000]\n",
      "- [VAL] LOSS : 0.03138015419244766 [SCORE] : 1.0\n",
      "[274/1000]\n",
      "- [TRAIN] LOSS : 0.05665637189522386 [SCORE] : 0.5755908330281575\n",
      "[274/1000]\n",
      "- [VAL] LOSS : 0.03133045509457588 [SCORE] : 1.0\n",
      "[275/1000]\n",
      "- [TRAIN] LOSS : 0.05662844901283582 [SCORE] : 0.5755908330281575\n",
      "[275/1000]\n",
      "- [VAL] LOSS : 0.03128023073077202 [SCORE] : 1.0\n",
      "[276/1000]\n",
      "- [TRAIN] LOSS : 0.05660060880084832 [SCORE] : 0.5755908330281575\n",
      "[276/1000]\n",
      "- [VAL] LOSS : 0.03122822940349579 [SCORE] : 1.0\n",
      "[277/1000]\n",
      "- [TRAIN] LOSS : 0.05657425206154585 [SCORE] : 0.5755908330281575\n",
      "[277/1000]\n",
      "- [VAL] LOSS : 0.031176218762993813 [SCORE] : 1.0\n",
      "[278/1000]\n",
      "- [TRAIN] LOSS : 0.05654886740570267 [SCORE] : 0.5755908330281575\n",
      "[278/1000]\n",
      "- [VAL] LOSS : 0.031125353649258614 [SCORE] : 1.0\n",
      "[279/1000]\n",
      "- [TRAIN] LOSS : 0.05652367019404968 [SCORE] : 0.5755908330281575\n",
      "[279/1000]\n",
      "- [VAL] LOSS : 0.03107561357319355 [SCORE] : 1.0\n",
      "[280/1000]\n",
      "- [TRAIN] LOSS : 0.056498467177152636 [SCORE] : 0.5755908330281575\n",
      "[280/1000]\n",
      "- [VAL] LOSS : 0.031026514247059822 [SCORE] : 1.0\n",
      "[281/1000]\n",
      "- [TRAIN] LOSS : 0.05647338544949889 [SCORE] : 0.5755908330281575\n",
      "[281/1000]\n",
      "- [VAL] LOSS : 0.030977817252278328 [SCORE] : 1.0\n",
      "[282/1000]\n",
      "- [TRAIN] LOSS : 0.056448667061825594 [SCORE] : 0.5755908330281575\n",
      "[282/1000]\n",
      "- [VAL] LOSS : 0.030929524451494217 [SCORE] : 1.0\n",
      "[283/1000]\n",
      "- [TRAIN] LOSS : 0.056424174830317496 [SCORE] : 0.5755908330281575\n",
      "[283/1000]\n",
      "- [VAL] LOSS : 0.030881505459547043 [SCORE] : 1.0\n",
      "[284/1000]\n",
      "- [TRAIN] LOSS : 0.05640014996752143 [SCORE] : 0.5755908330281575\n",
      "[284/1000]\n",
      "- [VAL] LOSS : 0.030834035947918892 [SCORE] : 1.0\n",
      "[285/1000]\n",
      "- [TRAIN] LOSS : 0.05637646798665325 [SCORE] : 0.5755908330281575\n",
      "[285/1000]\n",
      "- [VAL] LOSS : 0.03078720159828663 [SCORE] : 1.0\n",
      "[286/1000]\n",
      "- [TRAIN] LOSS : 0.05635293237864971 [SCORE] : 0.5755908330281575\n",
      "[286/1000]\n",
      "- [VAL] LOSS : 0.03074093721807003 [SCORE] : 1.0\n",
      "[287/1000]\n",
      "- [TRAIN] LOSS : 0.056329677781711024 [SCORE] : 0.5755908330281575\n",
      "[287/1000]\n",
      "- [VAL] LOSS : 0.030695179477334023 [SCORE] : 1.0\n",
      "[288/1000]\n",
      "- [TRAIN] LOSS : 0.05630666424209873 [SCORE] : 0.5755908330281575\n",
      "[288/1000]\n",
      "- [VAL] LOSS : 0.030649852007627487 [SCORE] : 1.0\n",
      "[289/1000]\n",
      "- [TRAIN] LOSS : 0.056283903463433184 [SCORE] : 0.5755908330281575\n",
      "[289/1000]\n",
      "- [VAL] LOSS : 0.030604949221014977 [SCORE] : 1.0\n",
      "[290/1000]\n",
      "- [TRAIN] LOSS : 0.05626142676919699 [SCORE] : 0.5755908330281575\n",
      "[290/1000]\n",
      "- [VAL] LOSS : 0.030560551211237907 [SCORE] : 1.0\n",
      "[291/1000]\n",
      "- [TRAIN] LOSS : 0.0562391718228658 [SCORE] : 0.5755908330281575\n",
      "[291/1000]\n",
      "- [VAL] LOSS : 0.0305166132748127 [SCORE] : 1.0\n",
      "[292/1000]\n",
      "- [TRAIN] LOSS : 0.05621722616876165 [SCORE] : 0.5755908330281575\n",
      "[292/1000]\n",
      "- [VAL] LOSS : 0.030473139137029648 [SCORE] : 1.0\n",
      "[293/1000]\n",
      "- [TRAIN] LOSS : 0.05619571205849449 [SCORE] : 0.5755908330281575\n",
      "[293/1000]\n",
      "- [VAL] LOSS : 0.030430329963564873 [SCORE] : 1.0\n",
      "[294/1000]\n",
      "- [TRAIN] LOSS : 0.05617434407273928 [SCORE] : 0.5755908330281575\n",
      "[294/1000]\n",
      "- [VAL] LOSS : 0.030388349667191505 [SCORE] : 1.0\n",
      "[295/1000]\n",
      "- [TRAIN] LOSS : 0.05615284033119679 [SCORE] : 0.5755908330281575\n",
      "[295/1000]\n",
      "- [VAL] LOSS : 0.03034643828868866 [SCORE] : 1.0\n",
      "[296/1000]\n",
      "- [TRAIN] LOSS : 0.056131600153942904 [SCORE] : 0.5755908330281575\n",
      "[296/1000]\n",
      "- [VAL] LOSS : 0.030304601415991783 [SCORE] : 1.0\n",
      "[297/1000]\n",
      "- [TRAIN] LOSS : 0.056110792234539984 [SCORE] : 0.5755908330281575\n",
      "[297/1000]\n",
      "- [VAL] LOSS : 0.030263101682066917 [SCORE] : 1.0\n",
      "[298/1000]\n",
      "- [TRAIN] LOSS : 0.056090302548060814 [SCORE] : 0.5755908330281575\n",
      "[298/1000]\n",
      "- [VAL] LOSS : 0.030222173780202866 [SCORE] : 1.0\n",
      "[299/1000]\n",
      "- [TRAIN] LOSS : 0.05606998596340418 [SCORE] : 0.5755908330281575\n",
      "[299/1000]\n",
      "- [VAL] LOSS : 0.030181679874658585 [SCORE] : 1.0\n",
      "[300/1000]\n",
      "- [TRAIN] LOSS : 0.05604986402516564 [SCORE] : 0.5755908330281575\n",
      "[300/1000]\n",
      "- [VAL] LOSS : 0.03014160320162773 [SCORE] : 1.0\n",
      "[301/1000]\n",
      "- [TRAIN] LOSS : 0.05602995635320743 [SCORE] : 0.5755908330281575\n",
      "[301/1000]\n",
      "- [VAL] LOSS : 0.03010196052491665 [SCORE] : 1.0\n",
      "[302/1000]\n",
      "- [TRAIN] LOSS : 0.056010250747203824 [SCORE] : 0.5755908330281575\n",
      "[302/1000]\n",
      "- [VAL] LOSS : 0.030062681064009666 [SCORE] : 1.0\n",
      "[303/1000]\n",
      "- [TRAIN] LOSS : 0.055990756334116064 [SCORE] : 0.5755908330281575\n",
      "[303/1000]\n",
      "- [VAL] LOSS : 0.030023757368326187 [SCORE] : 1.0\n",
      "[304/1000]\n",
      "- [TRAIN] LOSS : 0.0559714670603474 [SCORE] : 0.5755908330281575\n",
      "[304/1000]\n",
      "- [VAL] LOSS : 0.029985232278704643 [SCORE] : 1.0\n",
      "[305/1000]\n",
      "- [TRAIN] LOSS : 0.05595238568882147 [SCORE] : 0.5755908330281575\n",
      "[305/1000]\n",
      "- [VAL] LOSS : 0.029947131872177124 [SCORE] : 1.0\n",
      "[306/1000]\n",
      "- [TRAIN] LOSS : 0.055933506693691015 [SCORE] : 0.5755908330281575\n",
      "[306/1000]\n",
      "- [VAL] LOSS : 0.029909390956163406 [SCORE] : 1.0\n",
      "[307/1000]\n",
      "- [TRAIN] LOSS : 0.05591482473537326 [SCORE] : 0.5755908330281575\n",
      "[307/1000]\n",
      "- [VAL] LOSS : 0.029871994629502296 [SCORE] : 1.0\n",
      "[308/1000]\n",
      "- [TRAIN] LOSS : 0.05589634850621224 [SCORE] : 0.5755908330281575\n",
      "[308/1000]\n",
      "- [VAL] LOSS : 0.029834995046257973 [SCORE] : 1.0\n",
      "[309/1000]\n",
      "- [TRAIN] LOSS : 0.0558779940319558 [SCORE] : 0.5755908330281575\n",
      "[309/1000]\n",
      "- [VAL] LOSS : 0.0297983568161726 [SCORE] : 1.0\n",
      "[310/1000]\n",
      "- [TRAIN] LOSS : 0.05585991013795137 [SCORE] : 0.5755908330281575\n",
      "[310/1000]\n",
      "- [VAL] LOSS : 0.029762068763375282 [SCORE] : 1.0\n",
      "[311/1000]\n",
      "- [TRAIN] LOSS : 0.055841965787112716 [SCORE] : 0.5755908330281575\n",
      "[311/1000]\n",
      "- [VAL] LOSS : 0.029726127162575722 [SCORE] : 1.0\n",
      "[312/1000]\n",
      "- [TRAIN] LOSS : 0.05582423194622 [SCORE] : 0.5755908330281575\n",
      "[312/1000]\n",
      "- [VAL] LOSS : 0.0296905767172575 [SCORE] : 1.0\n",
      "[313/1000]\n",
      "- [TRAIN] LOSS : 0.05580668048933148 [SCORE] : 0.5755908330281575\n",
      "[313/1000]\n",
      "- [VAL] LOSS : 0.029655352234840393 [SCORE] : 1.0\n",
      "[314/1000]\n",
      "- [TRAIN] LOSS : 0.05578927689542373 [SCORE] : 0.5755908330281575\n",
      "[314/1000]\n",
      "- [VAL] LOSS : 0.029620468616485596 [SCORE] : 1.0\n",
      "[315/1000]\n",
      "- [TRAIN] LOSS : 0.05577209076533715 [SCORE] : 0.5755908330281575\n",
      "[315/1000]\n",
      "- [VAL] LOSS : 0.029585883021354675 [SCORE] : 1.0\n",
      "[316/1000]\n",
      "- [TRAIN] LOSS : 0.055755037690202396 [SCORE] : 0.5755908330281575\n",
      "[316/1000]\n",
      "- [VAL] LOSS : 0.02955167554318905 [SCORE] : 1.0\n",
      "[317/1000]\n",
      "- [TRAIN] LOSS : 0.05573815588528911 [SCORE] : 0.5755908330281575\n",
      "[317/1000]\n",
      "- [VAL] LOSS : 0.02951776422560215 [SCORE] : 1.0\n",
      "[318/1000]\n",
      "- [TRAIN] LOSS : 0.055721466057002546 [SCORE] : 0.5755908330281575\n",
      "[318/1000]\n",
      "- [VAL] LOSS : 0.029484186321496964 [SCORE] : 1.0\n",
      "[319/1000]\n",
      "- [TRAIN] LOSS : 0.05570490701744954 [SCORE] : 0.5755908330281575\n",
      "[319/1000]\n",
      "- [VAL] LOSS : 0.02945094369351864 [SCORE] : 1.0\n",
      "[320/1000]\n",
      "- [TRAIN] LOSS : 0.055688542469094195 [SCORE] : 0.5755908330281575\n",
      "[320/1000]\n",
      "- [VAL] LOSS : 0.02941804751753807 [SCORE] : 1.0\n",
      "[321/1000]\n",
      "- [TRAIN] LOSS : 0.05567229784404238 [SCORE] : 0.5755908330281575\n",
      "[321/1000]\n",
      "- [VAL] LOSS : 0.029385384172201157 [SCORE] : 1.0\n",
      "[322/1000]\n",
      "- [TRAIN] LOSS : 0.0556562486415108 [SCORE] : 0.5755908330281575\n",
      "[322/1000]\n",
      "- [VAL] LOSS : 0.029353126883506775 [SCORE] : 1.0\n",
      "[323/1000]\n",
      "- [TRAIN] LOSS : 0.05564034177611272 [SCORE] : 0.5755908330281575\n",
      "[323/1000]\n",
      "- [VAL] LOSS : 0.02932111918926239 [SCORE] : 1.0\n",
      "[324/1000]\n",
      "- [TRAIN] LOSS : 0.05562460341801246 [SCORE] : 0.5755908330281575\n",
      "[324/1000]\n",
      "- [VAL] LOSS : 0.029289400205016136 [SCORE] : 1.0\n",
      "[325/1000]\n",
      "- [TRAIN] LOSS : 0.055608982065071665 [SCORE] : 0.5755908330281575\n",
      "[325/1000]\n",
      "- [VAL] LOSS : 0.029258012771606445 [SCORE] : 1.0\n",
      "[326/1000]\n",
      "- [TRAIN] LOSS : 0.05559353002657493 [SCORE] : 0.5755908330281575\n",
      "[326/1000]\n",
      "- [VAL] LOSS : 0.029226887971162796 [SCORE] : 1.0\n",
      "[327/1000]\n",
      "- [TRAIN] LOSS : 0.05557825348029534 [SCORE] : 0.5755908330281575\n",
      "[327/1000]\n",
      "- [VAL] LOSS : 0.029196059331297874 [SCORE] : 1.0\n",
      "[328/1000]\n",
      "- [TRAIN] LOSS : 0.05556310514609019 [SCORE] : 0.5755908330281575\n",
      "[328/1000]\n",
      "- [VAL] LOSS : 0.029165562242269516 [SCORE] : 1.0\n",
      "[329/1000]\n",
      "- [TRAIN] LOSS : 0.055548061678806944 [SCORE] : 0.5755908330281575\n",
      "[329/1000]\n",
      "- [VAL] LOSS : 0.02913529798388481 [SCORE] : 1.0\n",
      "[330/1000]\n",
      "- [TRAIN] LOSS : 0.055533192089448374 [SCORE] : 0.5755908330281575\n",
      "[330/1000]\n",
      "- [VAL] LOSS : 0.02910531871020794 [SCORE] : 1.0\n",
      "[331/1000]\n",
      "- [TRAIN] LOSS : 0.055518472691377004 [SCORE] : 0.5755908330281575\n",
      "[331/1000]\n",
      "- [VAL] LOSS : 0.02907564677298069 [SCORE] : 1.0\n",
      "[332/1000]\n",
      "- [TRAIN] LOSS : 0.05550389563043912 [SCORE] : 0.5755908330281575\n",
      "[332/1000]\n",
      "- [VAL] LOSS : 0.029046256095170975 [SCORE] : 1.0\n",
      "[333/1000]\n",
      "- [TRAIN] LOSS : 0.055489419400691985 [SCORE] : 0.5755908330281575\n",
      "[333/1000]\n",
      "- [VAL] LOSS : 0.029017135500907898 [SCORE] : 1.0\n",
      "[334/1000]\n",
      "- [TRAIN] LOSS : 0.055475090102603035 [SCORE] : 0.5755908330281575\n",
      "[334/1000]\n",
      "- [VAL] LOSS : 0.02898826077580452 [SCORE] : 1.0\n",
      "[335/1000]\n",
      "- [TRAIN] LOSS : 0.0554609092262884 [SCORE] : 0.5755908330281575\n",
      "[335/1000]\n",
      "- [VAL] LOSS : 0.028959637507796288 [SCORE] : 1.0\n",
      "[336/1000]\n",
      "- [TRAIN] LOSS : 0.05544682921220859 [SCORE] : 0.5755908330281575\n",
      "[336/1000]\n",
      "- [VAL] LOSS : 0.028931304812431335 [SCORE] : 1.0\n",
      "[337/1000]\n",
      "- [TRAIN] LOSS : 0.05543287862092257 [SCORE] : 0.5755908330281575\n",
      "[337/1000]\n",
      "- [VAL] LOSS : 0.028903227299451828 [SCORE] : 1.0\n",
      "[338/1000]\n",
      "- [TRAIN] LOSS : 0.05541906620686253 [SCORE] : 0.5755908330281575\n",
      "[338/1000]\n",
      "- [VAL] LOSS : 0.028875352814793587 [SCORE] : 1.0\n",
      "[339/1000]\n",
      "- [TRAIN] LOSS : 0.05540538157025973 [SCORE] : 0.5755908330281575\n",
      "[339/1000]\n",
      "- [VAL] LOSS : 0.028847794979810715 [SCORE] : 1.0\n",
      "[340/1000]\n",
      "- [TRAIN] LOSS : 0.05539183417956035 [SCORE] : 0.5755908330281575\n",
      "[340/1000]\n",
      "- [VAL] LOSS : 0.028820496052503586 [SCORE] : 1.0\n",
      "[341/1000]\n",
      "- [TRAIN] LOSS : 0.0553783867508173 [SCORE] : 0.5755908330281575\n",
      "[341/1000]\n",
      "- [VAL] LOSS : 0.028793366625905037 [SCORE] : 1.0\n",
      "[342/1000]\n",
      "- [TRAIN] LOSS : 0.055365058096746605 [SCORE] : 0.5755908330281575\n",
      "[342/1000]\n",
      "- [VAL] LOSS : 0.028766566887497902 [SCORE] : 1.0\n",
      "[343/1000]\n",
      "- [TRAIN] LOSS : 0.05535186100751162 [SCORE] : 0.5755908330281575\n",
      "[343/1000]\n",
      "- [VAL] LOSS : 0.02873997762799263 [SCORE] : 1.0\n",
      "[344/1000]\n",
      "- [TRAIN] LOSS : 0.05533875878900289 [SCORE] : 0.5755908330281575\n",
      "[344/1000]\n",
      "- [VAL] LOSS : 0.028713611885905266 [SCORE] : 1.0\n",
      "[345/1000]\n",
      "- [TRAIN] LOSS : 0.05532581247389316 [SCORE] : 0.5755908330281575\n",
      "[345/1000]\n",
      "- [VAL] LOSS : 0.028687449172139168 [SCORE] : 1.0\n",
      "[346/1000]\n",
      "- [TRAIN] LOSS : 0.05531296003609896 [SCORE] : 0.5755908330281575\n",
      "[346/1000]\n",
      "- [VAL] LOSS : 0.028661619871854782 [SCORE] : 1.0\n",
      "[347/1000]\n",
      "- [TRAIN] LOSS : 0.05530019781241814 [SCORE] : 0.5755908330281575\n",
      "[347/1000]\n",
      "- [VAL] LOSS : 0.02863594889640808 [SCORE] : 1.0\n",
      "[348/1000]\n",
      "- [TRAIN] LOSS : 0.05528758013000091 [SCORE] : 0.5755908330281575\n",
      "[348/1000]\n",
      "- [VAL] LOSS : 0.028610525652766228 [SCORE] : 1.0\n",
      "[349/1000]\n",
      "- [TRAIN] LOSS : 0.05527506644527117 [SCORE] : 0.5755908330281575\n",
      "[349/1000]\n",
      "- [VAL] LOSS : 0.028585325926542282 [SCORE] : 1.0\n",
      "[350/1000]\n",
      "- [TRAIN] LOSS : 0.05526264446477095 [SCORE] : 0.5755908330281575\n",
      "[350/1000]\n",
      "- [VAL] LOSS : 0.02856033481657505 [SCORE] : 1.0\n",
      "[351/1000]\n",
      "- [TRAIN] LOSS : 0.05525032067671418 [SCORE] : 0.5755908330281575\n",
      "[351/1000]\n",
      "- [VAL] LOSS : 0.028535587713122368 [SCORE] : 1.0\n",
      "[352/1000]\n",
      "- [TRAIN] LOSS : 0.05523811752597491 [SCORE] : 0.5755908330281575\n",
      "[352/1000]\n",
      "- [VAL] LOSS : 0.0285110492259264 [SCORE] : 1.0\n",
      "[353/1000]\n",
      "- [TRAIN] LOSS : 0.055226016758630675 [SCORE] : 0.5755908330281575\n",
      "[353/1000]\n",
      "- [VAL] LOSS : 0.028486711904406548 [SCORE] : 1.0\n",
      "[354/1000]\n",
      "- [TRAIN] LOSS : 0.05521402449036638 [SCORE] : 0.5755908330281575\n",
      "[354/1000]\n",
      "- [VAL] LOSS : 0.028462573885917664 [SCORE] : 1.0\n",
      "[355/1000]\n",
      "- [TRAIN] LOSS : 0.05520210570345322 [SCORE] : 0.5755908330281575\n",
      "[355/1000]\n",
      "- [VAL] LOSS : 0.028438696637749672 [SCORE] : 1.0\n",
      "[356/1000]\n",
      "- [TRAIN] LOSS : 0.05519033229599397 [SCORE] : 0.5755908330281575\n",
      "[356/1000]\n",
      "- [VAL] LOSS : 0.02841503545641899 [SCORE] : 1.0\n",
      "[357/1000]\n",
      "- [TRAIN] LOSS : 0.055178676079958675 [SCORE] : 0.5755908330281575\n",
      "[357/1000]\n",
      "- [VAL] LOSS : 0.02839149907231331 [SCORE] : 1.0\n",
      "[358/1000]\n",
      "- [TRAIN] LOSS : 0.05516702458262444 [SCORE] : 0.5755908330281575\n",
      "[358/1000]\n",
      "- [VAL] LOSS : 0.02836822345852852 [SCORE] : 1.0\n",
      "[359/1000]\n",
      "- [TRAIN] LOSS : 0.05515550849959254 [SCORE] : 0.5755908330281575\n",
      "[359/1000]\n",
      "- [VAL] LOSS : 0.028345158323645592 [SCORE] : 1.0\n",
      "[360/1000]\n",
      "- [TRAIN] LOSS : 0.05514408542464177 [SCORE] : 0.5755908330281575\n",
      "[360/1000]\n",
      "- [VAL] LOSS : 0.028322292491793633 [SCORE] : 1.0\n",
      "[361/1000]\n",
      "- [TRAIN] LOSS : 0.055132796863714854 [SCORE] : 0.5755908330281575\n",
      "[361/1000]\n",
      "- [VAL] LOSS : 0.028299605473876 [SCORE] : 1.0\n",
      "[362/1000]\n",
      "- [TRAIN] LOSS : 0.055121560922513406 [SCORE] : 0.5755908330281575\n",
      "[362/1000]\n",
      "- [VAL] LOSS : 0.028277112171053886 [SCORE] : 1.0\n",
      "[363/1000]\n",
      "- [TRAIN] LOSS : 0.05511043552930157 [SCORE] : 0.5755908330281575\n",
      "[363/1000]\n",
      "- [VAL] LOSS : 0.02825482375919819 [SCORE] : 1.0\n",
      "[364/1000]\n",
      "- [TRAIN] LOSS : 0.0550993815685312 [SCORE] : 0.5755908330281575\n",
      "[364/1000]\n",
      "- [VAL] LOSS : 0.028232719749212265 [SCORE] : 1.0\n",
      "[365/1000]\n",
      "- [TRAIN] LOSS : 0.055088446599741776 [SCORE] : 0.5755908330281575\n",
      "[365/1000]\n",
      "- [VAL] LOSS : 0.028210820630192757 [SCORE] : 1.0\n",
      "[366/1000]\n",
      "- [TRAIN] LOSS : 0.05507754950473706 [SCORE] : 0.5755908330281575\n",
      "[366/1000]\n",
      "- [VAL] LOSS : 0.028189104050397873 [SCORE] : 1.0\n",
      "[367/1000]\n",
      "- [TRAIN] LOSS : 0.055066777703662716 [SCORE] : 0.5755908330281575\n",
      "[367/1000]\n",
      "- [VAL] LOSS : 0.02816753275692463 [SCORE] : 1.0\n",
      "[368/1000]\n",
      "- [TRAIN] LOSS : 0.05505610611289739 [SCORE] : 0.5755908330281575\n",
      "[368/1000]\n",
      "- [VAL] LOSS : 0.028146207332611084 [SCORE] : 1.0\n",
      "[369/1000]\n",
      "- [TRAIN] LOSS : 0.055045480156938234 [SCORE] : 0.5755908330281575\n",
      "[369/1000]\n",
      "- [VAL] LOSS : 0.028125016018748283 [SCORE] : 1.0\n",
      "[370/1000]\n",
      "- [TRAIN] LOSS : 0.05503496155142784 [SCORE] : 0.5755908330281575\n",
      "[370/1000]\n",
      "- [VAL] LOSS : 0.0281040258705616 [SCORE] : 1.0\n",
      "[371/1000]\n",
      "- [TRAIN] LOSS : 0.05502452397098144 [SCORE] : 0.5755908330281575\n",
      "[371/1000]\n",
      "- [VAL] LOSS : 0.0280831940472126 [SCORE] : 1.0\n",
      "[372/1000]\n",
      "- [TRAIN] LOSS : 0.0550141650562485 [SCORE] : 0.5755908330281575\n",
      "[372/1000]\n",
      "- [VAL] LOSS : 0.02806256338953972 [SCORE] : 1.0\n",
      "[373/1000]\n",
      "- [TRAIN] LOSS : 0.055003898218274117 [SCORE] : 0.5755908330281575\n",
      "[373/1000]\n",
      "- [VAL] LOSS : 0.028042074292898178 [SCORE] : 1.0\n",
      "[374/1000]\n",
      "- [TRAIN] LOSS : 0.05499369899431864 [SCORE] : 0.5755908330281575\n",
      "[374/1000]\n",
      "- [VAL] LOSS : 0.028021827340126038 [SCORE] : 1.0\n",
      "[375/1000]\n",
      "- [TRAIN] LOSS : 0.05498359076057871 [SCORE] : 0.5755908330281575\n",
      "[375/1000]\n",
      "- [VAL] LOSS : 0.028001680970191956 [SCORE] : 1.0\n",
      "[376/1000]\n",
      "- [TRAIN] LOSS : 0.05497352285310626 [SCORE] : 0.5755908330281575\n",
      "[376/1000]\n",
      "- [VAL] LOSS : 0.027981752529740334 [SCORE] : 1.0\n",
      "[377/1000]\n",
      "- [TRAIN] LOSS : 0.05496355928480625 [SCORE] : 0.5755908330281575\n",
      "[377/1000]\n",
      "- [VAL] LOSS : 0.02796190418303013 [SCORE] : 1.0\n",
      "[378/1000]\n",
      "- [TRAIN] LOSS : 0.05495366354783376 [SCORE] : 0.5755908330281575\n",
      "[378/1000]\n",
      "- [VAL] LOSS : 0.027942312881350517 [SCORE] : 1.0\n",
      "[379/1000]\n",
      "- [TRAIN] LOSS : 0.05494385104005536 [SCORE] : 0.5755908330281575\n",
      "[379/1000]\n",
      "- [VAL] LOSS : 0.027922850102186203 [SCORE] : 1.0\n",
      "[380/1000]\n",
      "- [TRAIN] LOSS : 0.05493408969293038 [SCORE] : 0.5755908330281575\n",
      "[380/1000]\n",
      "- [VAL] LOSS : 0.027903571724891663 [SCORE] : 1.0\n",
      "[381/1000]\n",
      "- [TRAIN] LOSS : 0.05492445013175408 [SCORE] : 0.5755908330281575\n",
      "[381/1000]\n",
      "- [VAL] LOSS : 0.027884414419531822 [SCORE] : 1.0\n",
      "[382/1000]\n",
      "- [TRAIN] LOSS : 0.054914850958933434 [SCORE] : 0.5755908330281575\n",
      "[382/1000]\n",
      "- [VAL] LOSS : 0.027865475043654442 [SCORE] : 1.0\n",
      "[383/1000]\n",
      "- [TRAIN] LOSS : 0.054905301642914614 [SCORE] : 0.5755908330281575\n",
      "[383/1000]\n",
      "- [VAL] LOSS : 0.027846667915582657 [SCORE] : 1.0\n",
      "[384/1000]\n",
      "- [TRAIN] LOSS : 0.05489587926616271 [SCORE] : 0.5755908330281575\n",
      "[384/1000]\n",
      "- [VAL] LOSS : 0.027827978134155273 [SCORE] : 1.0\n",
      "[385/1000]\n",
      "- [TRAIN] LOSS : 0.054886483835677305 [SCORE] : 0.5755908330281575\n",
      "[385/1000]\n",
      "- [VAL] LOSS : 0.02780948206782341 [SCORE] : 1.0\n",
      "[386/1000]\n",
      "- [TRAIN] LOSS : 0.0548771639354527 [SCORE] : 0.5755908330281575\n",
      "[386/1000]\n",
      "- [VAL] LOSS : 0.027791129425168037 [SCORE] : 1.0\n",
      "[387/1000]\n",
      "- [TRAIN] LOSS : 0.05486790258437395 [SCORE] : 0.5755908330281575\n",
      "[387/1000]\n",
      "- [VAL] LOSS : 0.02777290530502796 [SCORE] : 1.0\n",
      "[388/1000]\n",
      "- [TRAIN] LOSS : 0.054858751874417064 [SCORE] : 0.5755908330281575\n",
      "[388/1000]\n",
      "- [VAL] LOSS : 0.02775484509766102 [SCORE] : 1.0\n",
      "[389/1000]\n",
      "- [TRAIN] LOSS : 0.05484966405977806 [SCORE] : 0.5755908330281575\n",
      "[389/1000]\n",
      "- [VAL] LOSS : 0.027736946940422058 [SCORE] : 1.0\n",
      "[390/1000]\n",
      "- [TRAIN] LOSS : 0.05484057444458206 [SCORE] : 0.5755908330281575\n",
      "[390/1000]\n",
      "- [VAL] LOSS : 0.02771919034421444 [SCORE] : 1.0\n",
      "[391/1000]\n",
      "- [TRAIN] LOSS : 0.0548316347412765 [SCORE] : 0.5755908330281575\n",
      "[391/1000]\n",
      "- [VAL] LOSS : 0.02770158275961876 [SCORE] : 1.0\n",
      "[392/1000]\n",
      "- [TRAIN] LOSS : 0.05482266265898943 [SCORE] : 0.5755908330281575\n",
      "[392/1000]\n",
      "- [VAL] LOSS : 0.02768406644463539 [SCORE] : 1.0\n",
      "[393/1000]\n",
      "- [TRAIN] LOSS : 0.05481383558362722 [SCORE] : 0.5755908330281575\n",
      "[393/1000]\n",
      "- [VAL] LOSS : 0.027666738256812096 [SCORE] : 1.0\n",
      "[394/1000]\n",
      "- [TRAIN] LOSS : 0.05480503011494875 [SCORE] : 0.5755908330281575\n",
      "[394/1000]\n",
      "- [VAL] LOSS : 0.02764957584440708 [SCORE] : 1.0\n",
      "[395/1000]\n",
      "- [TRAIN] LOSS : 0.05479633640497923 [SCORE] : 0.5755908330281575\n",
      "[395/1000]\n",
      "- [VAL] LOSS : 0.027632543817162514 [SCORE] : 1.0\n",
      "[396/1000]\n",
      "- [TRAIN] LOSS : 0.05478769112378359 [SCORE] : 0.5755908330281575\n",
      "[396/1000]\n",
      "- [VAL] LOSS : 0.027615630999207497 [SCORE] : 1.0\n",
      "[397/1000]\n",
      "- [TRAIN] LOSS : 0.054779052672286825 [SCORE] : 0.5755908330281575\n",
      "[397/1000]\n",
      "- [VAL] LOSS : 0.02759886346757412 [SCORE] : 1.0\n",
      "[398/1000]\n",
      "- [TRAIN] LOSS : 0.05477049158265193 [SCORE] : 0.5755908330281575\n",
      "[398/1000]\n",
      "- [VAL] LOSS : 0.027582203969359398 [SCORE] : 1.0\n",
      "[399/1000]\n",
      "- [TRAIN] LOSS : 0.054762002701560654 [SCORE] : 0.5755908330281575\n",
      "[399/1000]\n",
      "- [VAL] LOSS : 0.027565686032176018 [SCORE] : 1.0\n",
      "[400/1000]\n",
      "- [TRAIN] LOSS : 0.054753598477691415 [SCORE] : 0.5755908330281575\n",
      "[400/1000]\n",
      "- [VAL] LOSS : 0.027549292892217636 [SCORE] : 1.0\n",
      "[401/1000]\n",
      "- [TRAIN] LOSS : 0.05474524277572831 [SCORE] : 0.5755908330281575\n",
      "[401/1000]\n",
      "- [VAL] LOSS : 0.027533087879419327 [SCORE] : 1.0\n",
      "[402/1000]\n",
      "- [TRAIN] LOSS : 0.054736918024718764 [SCORE] : 0.5755908330281575\n",
      "[402/1000]\n",
      "- [VAL] LOSS : 0.027516955509781837 [SCORE] : 1.0\n",
      "[403/1000]\n",
      "- [TRAIN] LOSS : 0.05472864179561535 [SCORE] : 0.5755908330281575\n",
      "[403/1000]\n",
      "- [VAL] LOSS : 0.027500974014401436 [SCORE] : 1.0\n",
      "[404/1000]\n",
      "- [TRAIN] LOSS : 0.05472045782953501 [SCORE] : 0.5755908330281575\n",
      "[404/1000]\n",
      "- [VAL] LOSS : 0.027485083788633347 [SCORE] : 1.0\n",
      "[405/1000]\n",
      "- [TRAIN] LOSS : 0.054712270759046076 [SCORE] : 0.5755908330281575\n",
      "[405/1000]\n",
      "- [VAL] LOSS : 0.027469370514154434 [SCORE] : 1.0\n",
      "[406/1000]\n",
      "- [TRAIN] LOSS : 0.054704218668242294 [SCORE] : 0.5755908330281575\n",
      "[406/1000]\n",
      "- [VAL] LOSS : 0.027453767135739326 [SCORE] : 1.0\n",
      "[407/1000]\n",
      "- [TRAIN] LOSS : 0.054696157077948254 [SCORE] : 0.5755908330281575\n",
      "[407/1000]\n",
      "- [VAL] LOSS : 0.027438314631581306 [SCORE] : 1.0\n",
      "[408/1000]\n",
      "- [TRAIN] LOSS : 0.05468818737814824 [SCORE] : 0.5755908330281575\n",
      "[408/1000]\n",
      "- [VAL] LOSS : 0.027422944083809853 [SCORE] : 1.0\n",
      "[409/1000]\n",
      "- [TRAIN] LOSS : 0.054680246952921154 [SCORE] : 0.5755908330281575\n",
      "[409/1000]\n",
      "- [VAL] LOSS : 0.027407685294747353 [SCORE] : 1.0\n",
      "[410/1000]\n",
      "- [TRAIN] LOSS : 0.054672380164265635 [SCORE] : 0.5755908330281575\n",
      "[410/1000]\n",
      "- [VAL] LOSS : 0.02739260531961918 [SCORE] : 1.0\n",
      "[411/1000]\n",
      "- [TRAIN] LOSS : 0.05466456081097325 [SCORE] : 0.5755908330281575\n",
      "[411/1000]\n",
      "- [VAL] LOSS : 0.02737767994403839 [SCORE] : 1.0\n",
      "[412/1000]\n",
      "- [TRAIN] LOSS : 0.054656793177127835 [SCORE] : 0.5755908330281575\n",
      "[412/1000]\n",
      "- [VAL] LOSS : 0.027362847700715065 [SCORE] : 1.0\n",
      "[413/1000]\n",
      "- [TRAIN] LOSS : 0.05464901753390829 [SCORE] : 0.5755908330281575\n",
      "[413/1000]\n",
      "- [VAL] LOSS : 0.02734810672700405 [SCORE] : 1.0\n",
      "[414/1000]\n",
      "- [TRAIN] LOSS : 0.05464137836049 [SCORE] : 0.5755908330281575\n",
      "[414/1000]\n",
      "- [VAL] LOSS : 0.02733348309993744 [SCORE] : 1.0\n",
      "[415/1000]\n",
      "- [TRAIN] LOSS : 0.054633751542617875 [SCORE] : 0.5755908330281575\n",
      "[415/1000]\n",
      "- [VAL] LOSS : 0.02731902524828911 [SCORE] : 1.0\n",
      "[416/1000]\n",
      "- [TRAIN] LOSS : 0.05462617178757986 [SCORE] : 0.5755908330281575\n",
      "[416/1000]\n",
      "- [VAL] LOSS : 0.027304632589221 [SCORE] : 1.0\n",
      "[417/1000]\n",
      "- [TRAIN] LOSS : 0.054618648067116735 [SCORE] : 0.5755908330281575\n",
      "[417/1000]\n",
      "- [VAL] LOSS : 0.02729039080440998 [SCORE] : 1.0\n",
      "[418/1000]\n",
      "- [TRAIN] LOSS : 0.05461114638795455 [SCORE] : 0.5755908330281575\n",
      "[418/1000]\n",
      "- [VAL] LOSS : 0.027276257053017616 [SCORE] : 1.0\n",
      "[419/1000]\n",
      "- [TRAIN] LOSS : 0.05460374603668849 [SCORE] : 0.5755908330281575\n",
      "[419/1000]\n",
      "- [VAL] LOSS : 0.027262184768915176 [SCORE] : 1.0\n",
      "[420/1000]\n",
      "- [TRAIN] LOSS : 0.054596318894376356 [SCORE] : 0.5755908330281575\n",
      "[420/1000]\n",
      "- [VAL] LOSS : 0.02724825218319893 [SCORE] : 1.0\n",
      "[421/1000]\n",
      "- [TRAIN] LOSS : 0.05458905327444275 [SCORE] : 0.5755908330281575\n",
      "[421/1000]\n",
      "- [VAL] LOSS : 0.027234453707933426 [SCORE] : 1.0\n",
      "[422/1000]\n",
      "- [TRAIN] LOSS : 0.05458171510448059 [SCORE] : 0.5755908330281575\n",
      "[422/1000]\n",
      "- [VAL] LOSS : 0.02722075581550598 [SCORE] : 1.0\n",
      "[423/1000]\n",
      "- [TRAIN] LOSS : 0.054574491021533805 [SCORE] : 0.5755908330281575\n",
      "[423/1000]\n",
      "- [VAL] LOSS : 0.027207182720303535 [SCORE] : 1.0\n",
      "[424/1000]\n",
      "- [TRAIN] LOSS : 0.05456726939106981 [SCORE] : 0.5755908330281575\n",
      "[424/1000]\n",
      "- [VAL] LOSS : 0.02719373069703579 [SCORE] : 1.0\n",
      "[425/1000]\n",
      "- [TRAIN] LOSS : 0.054560111494114 [SCORE] : 0.5755908330281575\n",
      "[425/1000]\n",
      "- [VAL] LOSS : 0.02718036249279976 [SCORE] : 1.0\n",
      "[426/1000]\n",
      "- [TRAIN] LOSS : 0.05455299898361166 [SCORE] : 0.5755908330281575\n",
      "[426/1000]\n",
      "- [VAL] LOSS : 0.02716706693172455 [SCORE] : 1.0\n",
      "[427/1000]\n",
      "- [TRAIN] LOSS : 0.054545929189771414 [SCORE] : 0.5755908330281575\n",
      "[427/1000]\n",
      "- [VAL] LOSS : 0.027153903618454933 [SCORE] : 1.0\n",
      "[428/1000]\n",
      "- [TRAIN] LOSS : 0.05453891518215338 [SCORE] : 0.5755908330281575\n",
      "[428/1000]\n",
      "- [VAL] LOSS : 0.027140839025378227 [SCORE] : 1.0\n",
      "[429/1000]\n",
      "- [TRAIN] LOSS : 0.05453194091096521 [SCORE] : 0.5755908330281575\n",
      "[429/1000]\n",
      "- [VAL] LOSS : 0.02712789736688137 [SCORE] : 1.0\n",
      "[430/1000]\n",
      "- [TRAIN] LOSS : 0.05452497508376837 [SCORE] : 0.5755908330281575\n",
      "[430/1000]\n",
      "- [VAL] LOSS : 0.02711503393948078 [SCORE] : 1.0\n",
      "[431/1000]\n",
      "- [TRAIN] LOSS : 0.05451829324786862 [SCORE] : 0.5755908330281575\n",
      "[431/1000]\n",
      "- [VAL] LOSS : 0.027102435007691383 [SCORE] : 1.0\n",
      "[432/1000]\n",
      "- [TRAIN] LOSS : 0.05451144444135328 [SCORE] : 0.5755908330281575\n",
      "[432/1000]\n",
      "- [VAL] LOSS : 0.027089973911643028 [SCORE] : 1.0\n",
      "[433/1000]\n",
      "- [TRAIN] LOSS : 0.05450450538968046 [SCORE] : 0.5755908330281575\n",
      "[433/1000]\n",
      "- [VAL] LOSS : 0.027077535167336464 [SCORE] : 1.0\n",
      "[434/1000]\n",
      "- [TRAIN] LOSS : 0.0544976689076672 [SCORE] : 0.5755908330281575\n",
      "[434/1000]\n",
      "- [VAL] LOSS : 0.027065027505159378 [SCORE] : 1.0\n",
      "[435/1000]\n",
      "- [TRAIN] LOSS : 0.054490918898954986 [SCORE] : 0.5755908330281575\n",
      "[435/1000]\n",
      "- [VAL] LOSS : 0.027052581310272217 [SCORE] : 1.0\n",
      "[436/1000]\n",
      "- [TRAIN] LOSS : 0.05448424277516703 [SCORE] : 0.5755908330281575\n",
      "[436/1000]\n",
      "- [VAL] LOSS : 0.027040302753448486 [SCORE] : 1.0\n",
      "[437/1000]\n",
      "- [TRAIN] LOSS : 0.05447762729600072 [SCORE] : 0.5755908330281575\n",
      "[437/1000]\n",
      "- [VAL] LOSS : 0.02702808938920498 [SCORE] : 1.0\n",
      "[438/1000]\n",
      "- [TRAIN] LOSS : 0.05447097788564861 [SCORE] : 0.5755908330281575\n",
      "[438/1000]\n",
      "- [VAL] LOSS : 0.027016017585992813 [SCORE] : 1.0\n",
      "[439/1000]\n",
      "- [TRAIN] LOSS : 0.0544643876918902 [SCORE] : 0.5755908330281575\n",
      "[439/1000]\n",
      "- [VAL] LOSS : 0.027004031464457512 [SCORE] : 1.0\n",
      "[440/1000]\n",
      "- [TRAIN] LOSS : 0.054457855178043246 [SCORE] : 0.5755908330281575\n",
      "[440/1000]\n",
      "- [VAL] LOSS : 0.02699212171137333 [SCORE] : 1.0\n",
      "[441/1000]\n",
      "- [TRAIN] LOSS : 0.054451345140114425 [SCORE] : 0.5755908330281575\n",
      "[441/1000]\n",
      "- [VAL] LOSS : 0.026980312541127205 [SCORE] : 1.0\n",
      "[442/1000]\n",
      "- [TRAIN] LOSS : 0.05444487165659666 [SCORE] : 0.5755908330281575\n",
      "[442/1000]\n",
      "- [VAL] LOSS : 0.026968613266944885 [SCORE] : 1.0\n",
      "[443/1000]\n",
      "- [TRAIN] LOSS : 0.05443846178241074 [SCORE] : 0.5755908330281575\n",
      "[443/1000]\n",
      "- [VAL] LOSS : 0.026956966146826744 [SCORE] : 1.0\n",
      "[444/1000]\n",
      "- [TRAIN] LOSS : 0.0544320347563674 [SCORE] : 0.5755908330281575\n",
      "[444/1000]\n",
      "- [VAL] LOSS : 0.026945417746901512 [SCORE] : 1.0\n",
      "[445/1000]\n",
      "- [TRAIN] LOSS : 0.05442567719146609 [SCORE] : 0.5755908330281575\n",
      "[445/1000]\n",
      "- [VAL] LOSS : 0.026933971792459488 [SCORE] : 1.0\n",
      "[446/1000]\n",
      "- [TRAIN] LOSS : 0.05441937915359934 [SCORE] : 0.5755908330281575\n",
      "[446/1000]\n",
      "- [VAL] LOSS : 0.02692256309092045 [SCORE] : 1.0\n",
      "[447/1000]\n",
      "- [TRAIN] LOSS : 0.05441308422014117 [SCORE] : 0.5755908330281575\n",
      "[447/1000]\n",
      "- [VAL] LOSS : 0.02691126987338066 [SCORE] : 1.0\n",
      "[448/1000]\n",
      "- [TRAIN] LOSS : 0.054406827160467706 [SCORE] : 0.5755908330281575\n",
      "[448/1000]\n",
      "- [VAL] LOSS : 0.026900116354227066 [SCORE] : 1.0\n",
      "[449/1000]\n",
      "- [TRAIN] LOSS : 0.054400577877337736 [SCORE] : 0.5755908330281575\n",
      "[449/1000]\n",
      "- [VAL] LOSS : 0.026888979598879814 [SCORE] : 1.0\n",
      "[450/1000]\n",
      "- [TRAIN] LOSS : 0.0543944022928675 [SCORE] : 0.5755908330281575\n",
      "[450/1000]\n",
      "- [VAL] LOSS : 0.02687792107462883 [SCORE] : 1.0\n",
      "[451/1000]\n",
      "- [TRAIN] LOSS : 0.054388250689953566 [SCORE] : 0.5755908330281575\n",
      "[451/1000]\n",
      "- [VAL] LOSS : 0.02686699852347374 [SCORE] : 1.0\n",
      "[452/1000]\n",
      "- [TRAIN] LOSS : 0.05438216552138329 [SCORE] : 0.5755908330281575\n",
      "[452/1000]\n",
      "- [VAL] LOSS : 0.026856137439608574 [SCORE] : 1.0\n",
      "[453/1000]\n",
      "- [TRAIN] LOSS : 0.05437606402362386 [SCORE] : 0.5755908330281575\n",
      "[453/1000]\n",
      "- [VAL] LOSS : 0.02684532105922699 [SCORE] : 1.0\n",
      "[454/1000]\n",
      "- [TRAIN] LOSS : 0.05436998726800084 [SCORE] : 0.5755908330281575\n",
      "[454/1000]\n",
      "- [VAL] LOSS : 0.02683459408581257 [SCORE] : 1.0\n",
      "[455/1000]\n",
      "- [TRAIN] LOSS : 0.054363972460851076 [SCORE] : 0.5755908330281575\n",
      "[455/1000]\n",
      "- [VAL] LOSS : 0.026823971420526505 [SCORE] : 1.0\n",
      "[456/1000]\n",
      "- [TRAIN] LOSS : 0.05435799243859947 [SCORE] : 0.5755908330281575\n",
      "[456/1000]\n",
      "- [VAL] LOSS : 0.026813408359885216 [SCORE] : 1.0\n",
      "[457/1000]\n",
      "- [TRAIN] LOSS : 0.05435203776384393 [SCORE] : 0.5755908330281575\n",
      "[457/1000]\n",
      "- [VAL] LOSS : 0.026802908629179 [SCORE] : 1.0\n",
      "[458/1000]\n",
      "- [TRAIN] LOSS : 0.05434613513449828 [SCORE] : 0.5755908330281575\n",
      "[458/1000]\n",
      "- [VAL] LOSS : 0.026792556047439575 [SCORE] : 1.0\n",
      "[459/1000]\n",
      "- [TRAIN] LOSS : 0.05434022475965321 [SCORE] : 0.5755908330281575\n",
      "[459/1000]\n",
      "- [VAL] LOSS : 0.026782238855957985 [SCORE] : 1.0\n",
      "[460/1000]\n",
      "- [TRAIN] LOSS : 0.05433433667446176 [SCORE] : 0.5755908330281575\n",
      "[460/1000]\n",
      "- [VAL] LOSS : 0.026771971955895424 [SCORE] : 1.0\n",
      "[461/1000]\n",
      "- [TRAIN] LOSS : 0.0543285451674213 [SCORE] : 0.5755908330281575\n",
      "[461/1000]\n",
      "- [VAL] LOSS : 0.026761796325445175 [SCORE] : 1.0\n",
      "[462/1000]\n",
      "- [TRAIN] LOSS : 0.054322704672813416 [SCORE] : 0.5755908330281575\n",
      "[462/1000]\n",
      "- [VAL] LOSS : 0.02675168216228485 [SCORE] : 1.0\n",
      "[463/1000]\n",
      "- [TRAIN] LOSS : 0.0543169681293269 [SCORE] : 0.5755908330281575\n",
      "[463/1000]\n",
      "- [VAL] LOSS : 0.026741689071059227 [SCORE] : 1.0\n",
      "[464/1000]\n",
      "- [TRAIN] LOSS : 0.05431119981221855 [SCORE] : 0.5755908330281575\n",
      "[464/1000]\n",
      "- [VAL] LOSS : 0.02673174813389778 [SCORE] : 1.0\n",
      "[465/1000]\n",
      "- [TRAIN] LOSS : 0.05430548524794479 [SCORE] : 0.5755908330281575\n",
      "[465/1000]\n",
      "- [VAL] LOSS : 0.026721874251961708 [SCORE] : 1.0\n",
      "[466/1000]\n",
      "- [TRAIN] LOSS : 0.05429981763785084 [SCORE] : 0.5755908330281575\n",
      "[466/1000]\n",
      "- [VAL] LOSS : 0.026712102815508842 [SCORE] : 1.0\n",
      "[467/1000]\n",
      "- [TRAIN] LOSS : 0.05429417196040352 [SCORE] : 0.5755908330281575\n",
      "[467/1000]\n",
      "- [VAL] LOSS : 0.02670239843428135 [SCORE] : 1.0\n",
      "[468/1000]\n",
      "- [TRAIN] LOSS : 0.05428851150597135 [SCORE] : 0.5755908330281575\n",
      "[468/1000]\n",
      "- [VAL] LOSS : 0.026692712679505348 [SCORE] : 1.0\n",
      "[469/1000]\n",
      "- [TRAIN] LOSS : 0.05428294038089613 [SCORE] : 0.5755908330281575\n",
      "[469/1000]\n",
      "- [VAL] LOSS : 0.02668311633169651 [SCORE] : 1.0\n",
      "[470/1000]\n",
      "- [TRAIN] LOSS : 0.054277345957234505 [SCORE] : 0.5755908330281575\n",
      "[470/1000]\n",
      "- [VAL] LOSS : 0.026673603802919388 [SCORE] : 1.0\n",
      "[471/1000]\n",
      "- [TRAIN] LOSS : 0.05427177539095283 [SCORE] : 0.5755908330281575\n",
      "[471/1000]\n",
      "- [VAL] LOSS : 0.026664171367883682 [SCORE] : 1.0\n",
      "[472/1000]\n",
      "- [TRAIN] LOSS : 0.05426628946637114 [SCORE] : 0.5755908330281575\n",
      "[472/1000]\n",
      "- [VAL] LOSS : 0.026654785498976707 [SCORE] : 1.0\n",
      "[473/1000]\n",
      "- [TRAIN] LOSS : 0.05426080029768248 [SCORE] : 0.5755908330281575\n",
      "[473/1000]\n",
      "- [VAL] LOSS : 0.026645470410585403 [SCORE] : 1.0\n",
      "[474/1000]\n",
      "- [TRAIN] LOSS : 0.054255349763358635 [SCORE] : 0.5755908330281575\n",
      "[474/1000]\n",
      "- [VAL] LOSS : 0.0266362726688385 [SCORE] : 1.0\n",
      "[475/1000]\n",
      "- [TRAIN] LOSS : 0.054249914502725004 [SCORE] : 0.5755908330281575\n",
      "[475/1000]\n",
      "- [VAL] LOSS : 0.026627471670508385 [SCORE] : 1.0\n",
      "[476/1000]\n",
      "- [TRAIN] LOSS : 0.05424525408695142 [SCORE] : 0.5755908330281575\n",
      "[476/1000]\n",
      "- [VAL] LOSS : 0.02661992609500885 [SCORE] : 1.0\n",
      "[477/1000]\n",
      "- [TRAIN] LOSS : 0.05423840032890439 [SCORE] : 0.5755908330281575\n",
      "[477/1000]\n",
      "- [VAL] LOSS : 0.02661001868546009 [SCORE] : 1.0\n",
      "[478/1000]\n",
      "- [TRAIN] LOSS : 0.054233041141803065 [SCORE] : 0.5755908330281575\n",
      "[478/1000]\n",
      "- [VAL] LOSS : 0.02659982442855835 [SCORE] : 1.0\n",
      "[479/1000]\n",
      "- [TRAIN] LOSS : 0.05422819888529678 [SCORE] : 0.5755908330281575\n",
      "[479/1000]\n",
      "- [VAL] LOSS : 0.026590438559651375 [SCORE] : 1.0\n",
      "[480/1000]\n",
      "- [TRAIN] LOSS : 0.054223256496091686 [SCORE] : 0.5755908330281575\n",
      "[480/1000]\n",
      "- [VAL] LOSS : 0.02658211998641491 [SCORE] : 1.0\n",
      "[481/1000]\n",
      "- [TRAIN] LOSS : 0.054218786318476 [SCORE] : 0.5755908330281575\n",
      "[481/1000]\n",
      "- [VAL] LOSS : 0.026575174182653427 [SCORE] : 1.0\n",
      "[482/1000]\n",
      "- [TRAIN] LOSS : 0.05421194376734396 [SCORE] : 0.5755908330281575\n",
      "[482/1000]\n",
      "- [VAL] LOSS : 0.026565710082650185 [SCORE] : 1.0\n",
      "[483/1000]\n",
      "- [TRAIN] LOSS : 0.05420659673400223 [SCORE] : 0.5755908330281575\n",
      "[483/1000]\n",
      "- [VAL] LOSS : 0.02655583806335926 [SCORE] : 1.0\n",
      "[484/1000]\n",
      "- [TRAIN] LOSS : 0.054201911917577186 [SCORE] : 0.5755908330281575\n",
      "[484/1000]\n",
      "- [VAL] LOSS : 0.026546716690063477 [SCORE] : 1.0\n",
      "[485/1000]\n",
      "- [TRAIN] LOSS : 0.05419714595191181 [SCORE] : 0.5755908330281575\n",
      "[485/1000]\n",
      "- [VAL] LOSS : 0.026538310572504997 [SCORE] : 1.0\n",
      "[486/1000]\n",
      "- [TRAIN] LOSS : 0.05419201646000147 [SCORE] : 0.5755908330281575\n",
      "[486/1000]\n",
      "- [VAL] LOSS : 0.026530509814620018 [SCORE] : 1.0\n",
      "[487/1000]\n",
      "- [TRAIN] LOSS : 0.0541875205313166 [SCORE] : 0.5755908330281575\n",
      "[487/1000]\n",
      "- [VAL] LOSS : 0.026523763313889503 [SCORE] : 1.0\n",
      "[488/1000]\n",
      "- [TRAIN] LOSS : 0.05418088265384237 [SCORE] : 0.5755908330281575\n",
      "[488/1000]\n",
      "- [VAL] LOSS : 0.026514526456594467 [SCORE] : 1.0\n",
      "[489/1000]\n",
      "- [TRAIN] LOSS : 0.054175736755132674 [SCORE] : 0.5755908330281575\n",
      "[489/1000]\n",
      "- [VAL] LOSS : 0.026504982262849808 [SCORE] : 1.0\n",
      "[490/1000]\n",
      "- [TRAIN] LOSS : 0.05417121189335982 [SCORE] : 0.5755908330281575\n",
      "[490/1000]\n",
      "- [VAL] LOSS : 0.02649623341858387 [SCORE] : 1.0\n",
      "[491/1000]\n",
      "- [TRAIN] LOSS : 0.05416656162900229 [SCORE] : 0.5755908330281575\n",
      "[491/1000]\n",
      "- [VAL] LOSS : 0.02648826874792576 [SCORE] : 1.0\n",
      "[492/1000]\n",
      "- [TRAIN] LOSS : 0.05416161158743004 [SCORE] : 0.5755908330281575\n",
      "[492/1000]\n",
      "- [VAL] LOSS : 0.02648083306849003 [SCORE] : 1.0\n",
      "[493/1000]\n",
      "- [TRAIN] LOSS : 0.054157189931720494 [SCORE] : 0.5755908330281575\n",
      "[493/1000]\n",
      "- [VAL] LOSS : 0.02647443115711212 [SCORE] : 1.0\n",
      "[494/1000]\n",
      "- [TRAIN] LOSS : 0.0541507327153037 [SCORE] : 0.5755908330281575\n",
      "[494/1000]\n",
      "- [VAL] LOSS : 0.02646554633975029 [SCORE] : 1.0\n",
      "[495/1000]\n",
      "- [TRAIN] LOSS : 0.05414567803964019 [SCORE] : 0.5755908330281575\n",
      "[495/1000]\n",
      "- [VAL] LOSS : 0.026456374675035477 [SCORE] : 1.0\n",
      "[496/1000]\n",
      "- [TRAIN] LOSS : 0.054141289197529356 [SCORE] : 0.5755908330281575\n",
      "[496/1000]\n",
      "- [VAL] LOSS : 0.026447953656315804 [SCORE] : 1.0\n",
      "[497/1000]\n",
      "- [TRAIN] LOSS : 0.054136784514412284 [SCORE] : 0.5755908330281575\n",
      "[497/1000]\n",
      "- [VAL] LOSS : 0.02644030749797821 [SCORE] : 1.0\n",
      "[498/1000]\n",
      "- [TRAIN] LOSS : 0.05413197280528645 [SCORE] : 0.5755908330281575\n",
      "[498/1000]\n",
      "- [VAL] LOSS : 0.02643318474292755 [SCORE] : 1.0\n",
      "[499/1000]\n",
      "- [TRAIN] LOSS : 0.054127690847963095 [SCORE] : 0.5755908330281575\n",
      "[499/1000]\n",
      "- [VAL] LOSS : 0.026427151635289192 [SCORE] : 1.0\n",
      "[500/1000]\n",
      "- [TRAIN] LOSS : 0.054121334260950484 [SCORE] : 0.5755908330281575\n",
      "[500/1000]\n",
      "- [VAL] LOSS : 0.0264186579734087 [SCORE] : 1.0\n",
      "[501/1000]\n",
      "- [TRAIN] LOSS : 0.05411643997455637 [SCORE] : 0.5755908330281575\n",
      "[501/1000]\n",
      "- [VAL] LOSS : 0.02640981785953045 [SCORE] : 1.0\n",
      "[502/1000]\n",
      "- [TRAIN] LOSS : 0.05411216476932168 [SCORE] : 0.5755908330281575\n",
      "[502/1000]\n",
      "- [VAL] LOSS : 0.026401760056614876 [SCORE] : 1.0\n",
      "[503/1000]\n",
      "- [TRAIN] LOSS : 0.054107784588510795 [SCORE] : 0.5755908330281575\n",
      "[503/1000]\n",
      "- [VAL] LOSS : 0.02639438770711422 [SCORE] : 1.0\n",
      "[504/1000]\n",
      "- [TRAIN] LOSS : 0.054103091514358916 [SCORE] : 0.5755908330281575\n",
      "[504/1000]\n",
      "- [VAL] LOSS : 0.026387648656964302 [SCORE] : 1.0\n",
      "[505/1000]\n",
      "- [TRAIN] LOSS : 0.05409893387307723 [SCORE] : 0.5755908330281575\n",
      "[505/1000]\n",
      "- [VAL] LOSS : 0.02638191357254982 [SCORE] : 1.0\n",
      "[506/1000]\n",
      "- [TRAIN] LOSS : 0.05409274830793341 [SCORE] : 0.5755908330281575\n",
      "[506/1000]\n",
      "- [VAL] LOSS : 0.02637370117008686 [SCORE] : 1.0\n",
      "[507/1000]\n",
      "- [TRAIN] LOSS : 0.054087942186743024 [SCORE] : 0.5755908330281575\n",
      "[507/1000]\n",
      "- [VAL] LOSS : 0.026365214958786964 [SCORE] : 1.0\n",
      "[508/1000]\n",
      "- [TRAIN] LOSS : 0.05408380318743487 [SCORE] : 0.5755908330281575\n",
      "[508/1000]\n",
      "- [VAL] LOSS : 0.026357468217611313 [SCORE] : 1.0\n",
      "[509/1000]\n",
      "- [TRAIN] LOSS : 0.05407952526584268 [SCORE] : 0.5755908330281575\n",
      "[509/1000]\n",
      "- [VAL] LOSS : 0.026350440457463264 [SCORE] : 1.0\n",
      "[510/1000]\n",
      "- [TRAIN] LOSS : 0.054074923243994516 [SCORE] : 0.5755908330281575\n",
      "[510/1000]\n",
      "- [VAL] LOSS : 0.026343628764152527 [SCORE] : 1.0\n",
      "[511/1000]\n",
      "- [TRAIN] LOSS : 0.05407019921888908 [SCORE] : 0.5755908330281575\n",
      "[511/1000]\n",
      "- [VAL] LOSS : 0.026337072253227234 [SCORE] : 1.0\n",
      "[512/1000]\n",
      "- [TRAIN] LOSS : 0.05406620257223646 [SCORE] : 0.5755908330281575\n",
      "[512/1000]\n",
      "- [VAL] LOSS : 0.02633150853216648 [SCORE] : 1.0\n",
      "[513/1000]\n",
      "- [TRAIN] LOSS : 0.05406023945348958 [SCORE] : 0.5755908330281575\n",
      "[513/1000]\n",
      "- [VAL] LOSS : 0.026323648169636726 [SCORE] : 1.0\n",
      "[514/1000]\n",
      "- [TRAIN] LOSS : 0.054055638161177436 [SCORE] : 0.5755908330281575\n",
      "[514/1000]\n",
      "- [VAL] LOSS : 0.026315536350011826 [SCORE] : 1.0\n",
      "[515/1000]\n",
      "- [TRAIN] LOSS : 0.0540515899968644 [SCORE] : 0.5755908330281575\n",
      "[515/1000]\n",
      "- [VAL] LOSS : 0.026308204978704453 [SCORE] : 1.0\n",
      "[516/1000]\n",
      "- [TRAIN] LOSS : 0.054047431952009596 [SCORE] : 0.5755908330281575\n",
      "[516/1000]\n",
      "- [VAL] LOSS : 0.026301560923457146 [SCORE] : 1.0\n",
      "[517/1000]\n",
      "- [TRAIN] LOSS : 0.054042946392049394 [SCORE] : 0.5755908330281575\n",
      "[517/1000]\n",
      "- [VAL] LOSS : 0.02629539743065834 [SCORE] : 1.0\n",
      "[518/1000]\n",
      "- [TRAIN] LOSS : 0.05403904466268917 [SCORE] : 0.5755908330281575\n",
      "[518/1000]\n",
      "- [VAL] LOSS : 0.026290258392691612 [SCORE] : 1.0\n",
      "[519/1000]\n",
      "- [TRAIN] LOSS : 0.0540331538921843 [SCORE] : 0.5755908330281575\n",
      "[519/1000]\n",
      "- [VAL] LOSS : 0.02628275752067566 [SCORE] : 1.0\n",
      "[520/1000]\n",
      "- [TRAIN] LOSS : 0.054028590318436424 [SCORE] : 0.5755908330281575\n",
      "[520/1000]\n",
      "- [VAL] LOSS : 0.02627492882311344 [SCORE] : 1.0\n",
      "[521/1000]\n",
      "- [TRAIN] LOSS : 0.054024659438679616 [SCORE] : 0.5755908330281575\n",
      "[521/1000]\n",
      "- [VAL] LOSS : 0.026267852634191513 [SCORE] : 1.0\n",
      "[522/1000]\n",
      "- [TRAIN] LOSS : 0.05402058738594254 [SCORE] : 0.5755908330281575\n",
      "[522/1000]\n",
      "- [VAL] LOSS : 0.02626141905784607 [SCORE] : 1.0\n",
      "[523/1000]\n",
      "- [TRAIN] LOSS : 0.054016247283046444 [SCORE] : 0.5755908330281575\n",
      "[523/1000]\n",
      "- [VAL] LOSS : 0.026255188509821892 [SCORE] : 1.0\n",
      "[524/1000]\n",
      "- [TRAIN] LOSS : 0.0540117752738297 [SCORE] : 0.5755908330281575\n",
      "[524/1000]\n",
      "- [VAL] LOSS : 0.026248907670378685 [SCORE] : 1.0\n",
      "[525/1000]\n",
      "- [TRAIN] LOSS : 0.05400728743212919 [SCORE] : 0.5755908330281575\n",
      "[525/1000]\n",
      "- [VAL] LOSS : 0.02624286711215973 [SCORE] : 1.0\n",
      "[526/1000]\n",
      "- [TRAIN] LOSS : 0.05400360628652076 [SCORE] : 0.5755908330281575\n",
      "[526/1000]\n",
      "- [VAL] LOSS : 0.02623791992664337 [SCORE] : 1.0\n",
      "[527/1000]\n",
      "- [TRAIN] LOSS : 0.05399794747742514 [SCORE] : 0.5755908330281575\n",
      "[527/1000]\n",
      "- [VAL] LOSS : 0.026230761781334877 [SCORE] : 1.0\n",
      "[528/1000]\n",
      "- [TRAIN] LOSS : 0.053993538565312825 [SCORE] : 0.5755908330281575\n",
      "[528/1000]\n",
      "- [VAL] LOSS : 0.026223331689834595 [SCORE] : 1.0\n",
      "[529/1000]\n",
      "- [TRAIN] LOSS : 0.053989745552341144 [SCORE] : 0.5755908330281575\n",
      "[529/1000]\n",
      "- [VAL] LOSS : 0.026216654106974602 [SCORE] : 1.0\n",
      "[530/1000]\n",
      "- [TRAIN] LOSS : 0.05398583983381589 [SCORE] : 0.5755908330281575\n",
      "[530/1000]\n",
      "- [VAL] LOSS : 0.02621062658727169 [SCORE] : 1.0\n",
      "[531/1000]\n",
      "- [TRAIN] LOSS : 0.05398159100053211 [SCORE] : 0.5755908330281575\n",
      "[531/1000]\n",
      "- [VAL] LOSS : 0.02620476856827736 [SCORE] : 1.0\n",
      "[532/1000]\n",
      "- [TRAIN] LOSS : 0.05397724107218285 [SCORE] : 0.5755908330281575\n",
      "[532/1000]\n",
      "- [VAL] LOSS : 0.02619880437850952 [SCORE] : 1.0\n",
      "[533/1000]\n",
      "- [TRAIN] LOSS : 0.05397290658826629 [SCORE] : 0.5755908330281575\n",
      "[533/1000]\n",
      "- [VAL] LOSS : 0.02619309350848198 [SCORE] : 1.0\n",
      "[534/1000]\n",
      "- [TRAIN] LOSS : 0.053969373693689704 [SCORE] : 0.5755908330281575\n",
      "[534/1000]\n",
      "- [VAL] LOSS : 0.02618846856057644 [SCORE] : 1.0\n",
      "[535/1000]\n",
      "- [TRAIN] LOSS : 0.05396379912272096 [SCORE] : 0.5755908330281575\n",
      "[535/1000]\n",
      "- [VAL] LOSS : 0.026181669905781746 [SCORE] : 1.0\n",
      "[536/1000]\n",
      "- [TRAIN] LOSS : 0.053959560270110764 [SCORE] : 0.5755908330281575\n",
      "[536/1000]\n",
      "- [VAL] LOSS : 0.026174645870923996 [SCORE] : 1.0\n",
      "[537/1000]\n",
      "- [TRAIN] LOSS : 0.05395589079707861 [SCORE] : 0.5755908330281575\n",
      "[537/1000]\n",
      "- [VAL] LOSS : 0.026168331503868103 [SCORE] : 1.0\n",
      "[538/1000]\n",
      "- [TRAIN] LOSS : 0.05395208103582263 [SCORE] : 0.5755908330281575\n",
      "[538/1000]\n",
      "- [VAL] LOSS : 0.026162656024098396 [SCORE] : 1.0\n",
      "[539/1000]\n",
      "- [TRAIN] LOSS : 0.05394797762855887 [SCORE] : 0.5755908330281575\n",
      "[539/1000]\n",
      "- [VAL] LOSS : 0.026157107204198837 [SCORE] : 1.0\n",
      "[540/1000]\n",
      "- [TRAIN] LOSS : 0.05394371246608595 [SCORE] : 0.5755908330281575\n",
      "[540/1000]\n",
      "- [VAL] LOSS : 0.026151452213525772 [SCORE] : 1.0\n",
      "[541/1000]\n",
      "- [TRAIN] LOSS : 0.05393949688101808 [SCORE] : 0.5755908330281575\n",
      "[541/1000]\n",
      "- [VAL] LOSS : 0.026146071031689644 [SCORE] : 1.0\n",
      "[542/1000]\n",
      "- [TRAIN] LOSS : 0.053936074829349916 [SCORE] : 0.5755908330281575\n",
      "[542/1000]\n",
      "- [VAL] LOSS : 0.026141773909330368 [SCORE] : 1.0\n",
      "[543/1000]\n",
      "- [TRAIN] LOSS : 0.05393069041892886 [SCORE] : 0.5755908330281575\n",
      "[543/1000]\n",
      "- [VAL] LOSS : 0.026135336607694626 [SCORE] : 1.0\n",
      "[544/1000]\n",
      "- [TRAIN] LOSS : 0.05392653392627835 [SCORE] : 0.5755908330281575\n",
      "[544/1000]\n",
      "- [VAL] LOSS : 0.02612864039838314 [SCORE] : 1.0\n",
      "[545/1000]\n",
      "- [TRAIN] LOSS : 0.05392296073647837 [SCORE] : 0.5755908330281575\n",
      "[545/1000]\n",
      "- [VAL] LOSS : 0.026122597977519035 [SCORE] : 1.0\n",
      "[546/1000]\n",
      "- [TRAIN] LOSS : 0.05391927335100869 [SCORE] : 0.5755908330281575\n",
      "[546/1000]\n",
      "- [VAL] LOSS : 0.026117252185940742 [SCORE] : 1.0\n",
      "[547/1000]\n",
      "- [TRAIN] LOSS : 0.05391528132992486 [SCORE] : 0.5755908330281575\n",
      "[547/1000]\n",
      "- [VAL] LOSS : 0.02611199952661991 [SCORE] : 1.0\n",
      "[548/1000]\n",
      "- [TRAIN] LOSS : 0.05391117129474878 [SCORE] : 0.5755908330281575\n",
      "[548/1000]\n",
      "- [VAL] LOSS : 0.026106690987944603 [SCORE] : 1.0\n",
      "[549/1000]\n",
      "- [TRAIN] LOSS : 0.05390706605588396 [SCORE] : 0.5755908330281575\n",
      "[549/1000]\n",
      "- [VAL] LOSS : 0.026101278141140938 [SCORE] : 1.0\n",
      "[550/1000]\n",
      "- [TRAIN] LOSS : 0.05390304260266324 [SCORE] : 0.5755908330281575\n",
      "[550/1000]\n",
      "- [VAL] LOSS : 0.026096204295754433 [SCORE] : 1.0\n",
      "[551/1000]\n",
      "- [TRAIN] LOSS : 0.05389980950082342 [SCORE] : 0.5755908330281575\n",
      "[551/1000]\n",
      "- [VAL] LOSS : 0.02609224244952202 [SCORE] : 1.0\n",
      "[552/1000]\n",
      "- [TRAIN] LOSS : 0.0538945270391802 [SCORE] : 0.5755908330281575\n",
      "[552/1000]\n",
      "- [VAL] LOSS : 0.026086177676916122 [SCORE] : 1.0\n",
      "[553/1000]\n",
      "- [TRAIN] LOSS : 0.053890490795796116 [SCORE] : 0.5755908330281575\n",
      "[553/1000]\n",
      "- [VAL] LOSS : 0.026079874485731125 [SCORE] : 1.0\n",
      "[554/1000]\n",
      "- [TRAIN] LOSS : 0.053887016403799254 [SCORE] : 0.5755908330281575\n",
      "[554/1000]\n",
      "- [VAL] LOSS : 0.026074204593896866 [SCORE] : 1.0\n",
      "[555/1000]\n",
      "- [TRAIN] LOSS : 0.05388346348578731 [SCORE] : 0.5755908330281575\n",
      "[555/1000]\n",
      "- [VAL] LOSS : 0.02606910467147827 [SCORE] : 1.0\n",
      "[556/1000]\n",
      "- [TRAIN] LOSS : 0.053879566676914695 [SCORE] : 0.5755908330281575\n",
      "[556/1000]\n",
      "- [VAL] LOSS : 0.02606421522796154 [SCORE] : 1.0\n",
      "[557/1000]\n",
      "- [TRAIN] LOSS : 0.053875601710751654 [SCORE] : 0.5755908330281575\n",
      "[557/1000]\n",
      "- [VAL] LOSS : 0.02605925314128399 [SCORE] : 1.0\n",
      "[558/1000]\n",
      "- [TRAIN] LOSS : 0.05387160961205761 [SCORE] : 0.5755908330281575\n",
      "[558/1000]\n",
      "- [VAL] LOSS : 0.026054169982671738 [SCORE] : 1.0\n",
      "[559/1000]\n",
      "- [TRAIN] LOSS : 0.053867716295644644 [SCORE] : 0.5755908330281575\n",
      "[559/1000]\n",
      "- [VAL] LOSS : 0.026049092411994934 [SCORE] : 1.0\n",
      "[560/1000]\n",
      "- [TRAIN] LOSS : 0.05386385504777233 [SCORE] : 0.5755908330281575\n",
      "[560/1000]\n",
      "- [VAL] LOSS : 0.026044052094221115 [SCORE] : 1.0\n",
      "[561/1000]\n",
      "- [TRAIN] LOSS : 0.053860021429136394 [SCORE] : 0.5755908330281575\n",
      "[561/1000]\n",
      "- [VAL] LOSS : 0.026039421558380127 [SCORE] : 1.0\n",
      "[562/1000]\n",
      "- [TRAIN] LOSS : 0.05385688909639915 [SCORE] : 0.5755908330281575\n",
      "[562/1000]\n",
      "- [VAL] LOSS : 0.026035893708467484 [SCORE] : 1.0\n",
      "[563/1000]\n",
      "- [TRAIN] LOSS : 0.05385176489750544 [SCORE] : 0.5755908330281575\n",
      "[563/1000]\n",
      "- [VAL] LOSS : 0.026030289009213448 [SCORE] : 1.0\n",
      "[564/1000]\n",
      "- [TRAIN] LOSS : 0.05384787057215969 [SCORE] : 0.5755908330281575\n",
      "[564/1000]\n",
      "- [VAL] LOSS : 0.026024358347058296 [SCORE] : 1.0\n",
      "[565/1000]\n",
      "- [TRAIN] LOSS : 0.05384452577369909 [SCORE] : 0.5755908330281575\n",
      "[565/1000]\n",
      "- [VAL] LOSS : 0.026019113138318062 [SCORE] : 1.0\n",
      "[566/1000]\n",
      "- [TRAIN] LOSS : 0.05384107769156496 [SCORE] : 0.5755908330281575\n",
      "[566/1000]\n",
      "- [VAL] LOSS : 0.02601439878344536 [SCORE] : 1.0\n",
      "[567/1000]\n",
      "- [TRAIN] LOSS : 0.05383734063555797 [SCORE] : 0.5755908330281575\n",
      "[567/1000]\n",
      "- [VAL] LOSS : 0.026009848341345787 [SCORE] : 1.0\n",
      "[568/1000]\n",
      "- [TRAIN] LOSS : 0.05383349219337106 [SCORE] : 0.5755908330281575\n",
      "[568/1000]\n",
      "- [VAL] LOSS : 0.026005269959568977 [SCORE] : 1.0\n",
      "[569/1000]\n",
      "- [TRAIN] LOSS : 0.05382965300232172 [SCORE] : 0.5755908330281575\n",
      "[569/1000]\n",
      "- [VAL] LOSS : 0.026000602170825005 [SCORE] : 1.0\n",
      "[570/1000]\n",
      "- [TRAIN] LOSS : 0.05382590756441156 [SCORE] : 0.5755908330281575\n",
      "[570/1000]\n",
      "- [VAL] LOSS : 0.0259958915412426 [SCORE] : 1.0\n",
      "[571/1000]\n",
      "- [TRAIN] LOSS : 0.05382216532404224 [SCORE] : 0.5755908330281575\n",
      "[571/1000]\n",
      "- [VAL] LOSS : 0.02599121630191803 [SCORE] : 1.0\n",
      "[572/1000]\n",
      "- [TRAIN] LOSS : 0.05381848591690262 [SCORE] : 0.5755908330281575\n",
      "[572/1000]\n",
      "- [VAL] LOSS : 0.02598694898188114 [SCORE] : 1.0\n",
      "[573/1000]\n",
      "- [TRAIN] LOSS : 0.053815432172268626 [SCORE] : 0.5755908330281575\n",
      "[573/1000]\n",
      "- [VAL] LOSS : 0.02598370984196663 [SCORE] : 1.0\n",
      "[574/1000]\n",
      "- [TRAIN] LOSS : 0.05381048922426999 [SCORE] : 0.5755908330281575\n",
      "[574/1000]\n",
      "- [VAL] LOSS : 0.02597852237522602 [SCORE] : 1.0\n",
      "[575/1000]\n",
      "- [TRAIN] LOSS : 0.05380668730164568 [SCORE] : 0.5755908330281575\n",
      "[575/1000]\n",
      "- [VAL] LOSS : 0.025972997769713402 [SCORE] : 1.0\n",
      "[576/1000]\n",
      "- [TRAIN] LOSS : 0.05380347062212725 [SCORE] : 0.5755908330281575\n",
      "[576/1000]\n",
      "- [VAL] LOSS : 0.025968028232455254 [SCORE] : 1.0\n",
      "[577/1000]\n",
      "- [TRAIN] LOSS : 0.05380017065132658 [SCORE] : 0.5755908330281575\n",
      "[577/1000]\n",
      "- [VAL] LOSS : 0.025963695719838142 [SCORE] : 1.0\n",
      "[578/1000]\n",
      "- [TRAIN] LOSS : 0.05379757497770091 [SCORE] : 0.5755908330281575\n",
      "[578/1000]\n",
      "- [VAL] LOSS : 0.02596130780875683 [SCORE] : 1.0\n",
      "[579/1000]\n",
      "- [TRAIN] LOSS : 0.053793282542998595 [SCORE] : 0.5755908330281575\n",
      "[579/1000]\n",
      "- [VAL] LOSS : 0.025958191603422165 [SCORE] : 1.0\n",
      "[580/1000]\n",
      "- [TRAIN] LOSS : 0.05378882458123068 [SCORE] : 0.5755908330281575\n",
      "[580/1000]\n",
      "- [VAL] LOSS : 0.02595384232699871 [SCORE] : 1.0\n",
      "[581/1000]\n",
      "- [TRAIN] LOSS : 0.05378487212583423 [SCORE] : 0.5755908330281575\n",
      "[581/1000]\n",
      "- [VAL] LOSS : 0.02594888210296631 [SCORE] : 1.0\n",
      "[582/1000]\n",
      "- [TRAIN] LOSS : 0.0537813783933719 [SCORE] : 0.5755908330281575\n",
      "[582/1000]\n",
      "- [VAL] LOSS : 0.02594400942325592 [SCORE] : 1.0\n",
      "[583/1000]\n",
      "- [TRAIN] LOSS : 0.05377799617126584 [SCORE] : 0.5755908330281575\n",
      "[583/1000]\n",
      "- [VAL] LOSS : 0.025939475744962692 [SCORE] : 1.0\n",
      "[584/1000]\n",
      "- [TRAIN] LOSS : 0.0537745162534217 [SCORE] : 0.5755908330281575\n",
      "[584/1000]\n",
      "- [VAL] LOSS : 0.02593516930937767 [SCORE] : 1.0\n",
      "[585/1000]\n",
      "- [TRAIN] LOSS : 0.053770950017496946 [SCORE] : 0.5755908330281575\n",
      "[585/1000]\n",
      "- [VAL] LOSS : 0.025930849835276604 [SCORE] : 1.0\n",
      "[586/1000]\n",
      "- [TRAIN] LOSS : 0.053767352861662704 [SCORE] : 0.5755908330281575\n",
      "[586/1000]\n",
      "- [VAL] LOSS : 0.0259264949709177 [SCORE] : 1.0\n",
      "[587/1000]\n",
      "- [TRAIN] LOSS : 0.05376378732422988 [SCORE] : 0.5755908330281575\n",
      "[587/1000]\n",
      "- [VAL] LOSS : 0.025922158733010292 [SCORE] : 1.0\n",
      "[588/1000]\n",
      "- [TRAIN] LOSS : 0.05376021883760889 [SCORE] : 0.5755908330281575\n",
      "[588/1000]\n",
      "- [VAL] LOSS : 0.025917839258909225 [SCORE] : 1.0\n",
      "[589/1000]\n",
      "- [TRAIN] LOSS : 0.053756740782409905 [SCORE] : 0.5755908330281575\n",
      "[589/1000]\n",
      "- [VAL] LOSS : 0.025913557037711143 [SCORE] : 1.0\n",
      "[590/1000]\n",
      "- [TRAIN] LOSS : 0.053753210899109644 [SCORE] : 0.5755908330281575\n",
      "[590/1000]\n",
      "- [VAL] LOSS : 0.025909315794706345 [SCORE] : 1.0\n",
      "[591/1000]\n",
      "- [TRAIN] LOSS : 0.05374972332889835 [SCORE] : 0.5755908330281575\n",
      "[591/1000]\n",
      "- [VAL] LOSS : 0.02590508572757244 [SCORE] : 1.0\n",
      "[592/1000]\n",
      "- [TRAIN] LOSS : 0.05374621730297804 [SCORE] : 0.5755908330281575\n",
      "[592/1000]\n",
      "- [VAL] LOSS : 0.025900891050696373 [SCORE] : 1.0\n",
      "[593/1000]\n",
      "- [TRAIN] LOSS : 0.053742757486179474 [SCORE] : 0.5755908330281575\n",
      "[593/1000]\n",
      "- [VAL] LOSS : 0.025896774604916573 [SCORE] : 1.0\n",
      "[594/1000]\n",
      "- [TRAIN] LOSS : 0.05373929004805784 [SCORE] : 0.5755908330281575\n",
      "[594/1000]\n",
      "- [VAL] LOSS : 0.02589264139533043 [SCORE] : 1.0\n",
      "[595/1000]\n",
      "- [TRAIN] LOSS : 0.05373578870979448 [SCORE] : 0.5755908330281575\n",
      "[595/1000]\n",
      "- [VAL] LOSS : 0.02588854357600212 [SCORE] : 1.0\n",
      "[596/1000]\n",
      "- [TRAIN] LOSS : 0.05373232350684702 [SCORE] : 0.5755908330281575\n",
      "[596/1000]\n",
      "- [VAL] LOSS : 0.0258844792842865 [SCORE] : 1.0\n",
      "[597/1000]\n",
      "- [TRAIN] LOSS : 0.05372891441608469 [SCORE] : 0.5755908330281575\n",
      "[597/1000]\n",
      "- [VAL] LOSS : 0.02588042989373207 [SCORE] : 1.0\n",
      "[598/1000]\n",
      "- [TRAIN] LOSS : 0.05372546603903174 [SCORE] : 0.5755908330281575\n",
      "[598/1000]\n",
      "- [VAL] LOSS : 0.025876382365822792 [SCORE] : 1.0\n",
      "[599/1000]\n",
      "- [TRAIN] LOSS : 0.053722013471027216 [SCORE] : 0.5755908330281575\n",
      "[599/1000]\n",
      "- [VAL] LOSS : 0.025872403755784035 [SCORE] : 1.0\n",
      "[600/1000]\n",
      "- [TRAIN] LOSS : 0.05371862696483731 [SCORE] : 0.5755908330281575\n",
      "[600/1000]\n",
      "- [VAL] LOSS : 0.025868481025099754 [SCORE] : 1.0\n",
      "[601/1000]\n",
      "- [TRAIN] LOSS : 0.05371522335335612 [SCORE] : 0.5755908330281575\n",
      "[601/1000]\n",
      "- [VAL] LOSS : 0.025864532217383385 [SCORE] : 1.0\n",
      "[602/1000]\n",
      "- [TRAIN] LOSS : 0.05371182123199105 [SCORE] : 0.5755908330281575\n",
      "[602/1000]\n",
      "- [VAL] LOSS : 0.02586062252521515 [SCORE] : 1.0\n",
      "[603/1000]\n",
      "- [TRAIN] LOSS : 0.05370843300285439 [SCORE] : 0.5755908330281575\n",
      "[603/1000]\n",
      "- [VAL] LOSS : 0.02585677243769169 [SCORE] : 1.0\n",
      "[604/1000]\n",
      "- [TRAIN] LOSS : 0.05370506946928799 [SCORE] : 0.5755908330281575\n",
      "[604/1000]\n",
      "- [VAL] LOSS : 0.025852905586361885 [SCORE] : 1.0\n",
      "[605/1000]\n",
      "- [TRAIN] LOSS : 0.05370169910602272 [SCORE] : 0.5755908330281575\n",
      "[605/1000]\n",
      "- [VAL] LOSS : 0.02584906481206417 [SCORE] : 1.0\n",
      "[606/1000]\n",
      "- [TRAIN] LOSS : 0.053698298055678606 [SCORE] : 0.5755908330281575\n",
      "[606/1000]\n",
      "- [VAL] LOSS : 0.02584526315331459 [SCORE] : 1.0\n",
      "[607/1000]\n",
      "- [TRAIN] LOSS : 0.05369497574865818 [SCORE] : 0.5755908330281575\n",
      "[607/1000]\n",
      "- [VAL] LOSS : 0.02584150619804859 [SCORE] : 1.0\n",
      "[608/1000]\n",
      "- [TRAIN] LOSS : 0.05369165254135926 [SCORE] : 0.5755908330281575\n",
      "[608/1000]\n",
      "- [VAL] LOSS : 0.02583775855600834 [SCORE] : 1.0\n",
      "[609/1000]\n",
      "- [TRAIN] LOSS : 0.053688300168141724 [SCORE] : 0.5755908330281575\n",
      "[609/1000]\n",
      "- [VAL] LOSS : 0.025834040716290474 [SCORE] : 1.0\n",
      "[610/1000]\n",
      "- [TRAIN] LOSS : 0.053684941958636045 [SCORE] : 0.5755908330281575\n",
      "[610/1000]\n",
      "- [VAL] LOSS : 0.02583034709095955 [SCORE] : 1.0\n",
      "[611/1000]\n",
      "- [TRAIN] LOSS : 0.05368163672586282 [SCORE] : 0.5755908330281575\n",
      "[611/1000]\n",
      "- [VAL] LOSS : 0.025826672092080116 [SCORE] : 1.0\n",
      "[612/1000]\n",
      "- [TRAIN] LOSS : 0.05367832990984122 [SCORE] : 0.5755908330281575\n",
      "[612/1000]\n",
      "- [VAL] LOSS : 0.02582305483520031 [SCORE] : 1.0\n",
      "[613/1000]\n",
      "- [TRAIN] LOSS : 0.05367504168922702 [SCORE] : 0.5755908330281575\n",
      "[613/1000]\n",
      "- [VAL] LOSS : 0.025819405913352966 [SCORE] : 1.0\n",
      "[614/1000]\n",
      "- [TRAIN] LOSS : 0.0536717422461758 [SCORE] : 0.5755908330281575\n",
      "[614/1000]\n",
      "- [VAL] LOSS : 0.02581579051911831 [SCORE] : 1.0\n",
      "[615/1000]\n",
      "- [TRAIN] LOSS : 0.053668464068323374 [SCORE] : 0.5755908330281575\n",
      "[615/1000]\n",
      "- [VAL] LOSS : 0.025812223553657532 [SCORE] : 1.0\n",
      "[616/1000]\n",
      "- [TRAIN] LOSS : 0.05366520777655145 [SCORE] : 0.5755908330281575\n",
      "[616/1000]\n",
      "- [VAL] LOSS : 0.025808677077293396 [SCORE] : 1.0\n",
      "[617/1000]\n",
      "- [TRAIN] LOSS : 0.05366197858626644 [SCORE] : 0.5755908330281575\n",
      "[617/1000]\n",
      "- [VAL] LOSS : 0.025805162265896797 [SCORE] : 1.0\n",
      "[618/1000]\n",
      "- [TRAIN] LOSS : 0.05365865623267988 [SCORE] : 0.5755908330281575\n",
      "[618/1000]\n",
      "- [VAL] LOSS : 0.02580164186656475 [SCORE] : 1.0\n",
      "[619/1000]\n",
      "- [TRAIN] LOSS : 0.05365544420977433 [SCORE] : 0.5755908330281575\n",
      "[619/1000]\n",
      "- [VAL] LOSS : 0.025798212736845016 [SCORE] : 1.0\n",
      "[620/1000]\n",
      "- [TRAIN] LOSS : 0.053652175888419154 [SCORE] : 0.5755908330281575\n",
      "[620/1000]\n",
      "- [VAL] LOSS : 0.025794733315706253 [SCORE] : 1.0\n",
      "[621/1000]\n",
      "- [TRAIN] LOSS : 0.05364895372961958 [SCORE] : 0.5755908330281575\n",
      "[621/1000]\n",
      "- [VAL] LOSS : 0.025791307911276817 [SCORE] : 1.0\n",
      "[622/1000]\n",
      "- [TRAIN] LOSS : 0.05364570658033093 [SCORE] : 0.5755908330281575\n",
      "[622/1000]\n",
      "- [VAL] LOSS : 0.02578790672123432 [SCORE] : 1.0\n",
      "[623/1000]\n",
      "- [TRAIN] LOSS : 0.05364251531039675 [SCORE] : 0.5755908330281575\n",
      "[623/1000]\n",
      "- [VAL] LOSS : 0.025784533470869064 [SCORE] : 1.0\n",
      "[624/1000]\n",
      "- [TRAIN] LOSS : 0.05363931297324598 [SCORE] : 0.5755908330281575\n",
      "[624/1000]\n",
      "- [VAL] LOSS : 0.025781145319342613 [SCORE] : 1.0\n",
      "[625/1000]\n",
      "- [TRAIN] LOSS : 0.05363610323208074 [SCORE] : 0.5755908330281575\n",
      "[625/1000]\n",
      "- [VAL] LOSS : 0.025777842849493027 [SCORE] : 1.0\n",
      "[626/1000]\n",
      "- [TRAIN] LOSS : 0.053632887670149405 [SCORE] : 0.5755908330281575\n",
      "[626/1000]\n",
      "- [VAL] LOSS : 0.02577451802790165 [SCORE] : 1.0\n",
      "[627/1000]\n",
      "- [TRAIN] LOSS : 0.053629711363464595 [SCORE] : 0.5755908330281575\n",
      "[627/1000]\n",
      "- [VAL] LOSS : 0.0257712434977293 [SCORE] : 1.0\n",
      "[628/1000]\n",
      "- [TRAIN] LOSS : 0.053626588095600405 [SCORE] : 0.5755908330281575\n",
      "[628/1000]\n",
      "- [VAL] LOSS : 0.025767961516976357 [SCORE] : 1.0\n",
      "[629/1000]\n",
      "- [TRAIN] LOSS : 0.05362334903329611 [SCORE] : 0.5755908330281575\n",
      "[629/1000]\n",
      "- [VAL] LOSS : 0.025764692574739456 [SCORE] : 1.0\n",
      "[630/1000]\n",
      "- [TRAIN] LOSS : 0.05362022477202118 [SCORE] : 0.5755908330281575\n",
      "[630/1000]\n",
      "- [VAL] LOSS : 0.0257614366710186 [SCORE] : 1.0\n",
      "[631/1000]\n",
      "- [TRAIN] LOSS : 0.0536171012558043 [SCORE] : 0.5755908330281575\n",
      "[631/1000]\n",
      "- [VAL] LOSS : 0.02575826644897461 [SCORE] : 1.0\n",
      "[632/1000]\n",
      "- [TRAIN] LOSS : 0.05361395166255534 [SCORE] : 0.5755908330281575\n",
      "[632/1000]\n",
      "- [VAL] LOSS : 0.02575509436428547 [SCORE] : 1.0\n",
      "[633/1000]\n",
      "- [TRAIN] LOSS : 0.053610819236685835 [SCORE] : 0.5755908330281575\n",
      "[633/1000]\n",
      "- [VAL] LOSS : 0.02575189806520939 [SCORE] : 1.0\n",
      "[634/1000]\n",
      "- [TRAIN] LOSS : 0.053607674594968555 [SCORE] : 0.5755908330281575\n",
      "[634/1000]\n",
      "- [VAL] LOSS : 0.025748806074261665 [SCORE] : 1.0\n",
      "[635/1000]\n",
      "- [TRAIN] LOSS : 0.053604577714577314 [SCORE] : 0.5755908330281575\n",
      "[635/1000]\n",
      "- [VAL] LOSS : 0.025745661929249763 [SCORE] : 1.0\n",
      "[636/1000]\n",
      "- [TRAIN] LOSS : 0.05360146670912703 [SCORE] : 0.5755908330281575\n",
      "[636/1000]\n",
      "- [VAL] LOSS : 0.0257425494492054 [SCORE] : 1.0\n",
      "[637/1000]\n",
      "- [TRAIN] LOSS : 0.05359834445019563 [SCORE] : 0.5755908330281575\n",
      "[637/1000]\n",
      "- [VAL] LOSS : 0.025739500299096107 [SCORE] : 1.0\n",
      "[638/1000]\n",
      "- [TRAIN] LOSS : 0.053595258357624215 [SCORE] : 0.5755908330281575\n",
      "[638/1000]\n",
      "- [VAL] LOSS : 0.025736480951309204 [SCORE] : 1.0\n",
      "[639/1000]\n",
      "- [TRAIN] LOSS : 0.05359216509386897 [SCORE] : 0.5755908330281575\n",
      "[639/1000]\n",
      "- [VAL] LOSS : 0.025733429938554764 [SCORE] : 1.0\n",
      "[640/1000]\n",
      "- [TRAIN] LOSS : 0.053589056540901465 [SCORE] : 0.5755908330281575\n",
      "[640/1000]\n",
      "- [VAL] LOSS : 0.02573038637638092 [SCORE] : 1.0\n",
      "[641/1000]\n",
      "- [TRAIN] LOSS : 0.053586034833764035 [SCORE] : 0.5755908330281575\n",
      "[641/1000]\n",
      "- [VAL] LOSS : 0.02572740614414215 [SCORE] : 1.0\n",
      "[642/1000]\n",
      "- [TRAIN] LOSS : 0.053582922120889026 [SCORE] : 0.5755908330281575\n",
      "[642/1000]\n",
      "- [VAL] LOSS : 0.02572442777454853 [SCORE] : 1.0\n",
      "[643/1000]\n",
      "- [TRAIN] LOSS : 0.05357988455022375 [SCORE] : 0.5755908330281575\n",
      "[643/1000]\n",
      "- [VAL] LOSS : 0.025721492245793343 [SCORE] : 1.0\n",
      "[644/1000]\n",
      "- [TRAIN] LOSS : 0.05357678748356799 [SCORE] : 0.5755908330281575\n",
      "[644/1000]\n",
      "- [VAL] LOSS : 0.025718558579683304 [SCORE] : 1.0\n",
      "[645/1000]\n",
      "- [TRAIN] LOSS : 0.053573772435386975 [SCORE] : 0.5755908330281575\n",
      "[645/1000]\n",
      "- [VAL] LOSS : 0.02571563795208931 [SCORE] : 1.0\n",
      "[646/1000]\n",
      "- [TRAIN] LOSS : 0.0535707114264369 [SCORE] : 0.5755908330281575\n",
      "[646/1000]\n",
      "- [VAL] LOSS : 0.02571277692914009 [SCORE] : 1.0\n",
      "[647/1000]\n",
      "- [TRAIN] LOSS : 0.05356769599020481 [SCORE] : 0.5755908330281575\n",
      "[647/1000]\n",
      "- [VAL] LOSS : 0.025709891691803932 [SCORE] : 1.0\n",
      "[648/1000]\n",
      "- [TRAIN] LOSS : 0.05356466897452871 [SCORE] : 0.5755908330281575\n",
      "[648/1000]\n",
      "- [VAL] LOSS : 0.025707051157951355 [SCORE] : 1.0\n",
      "[649/1000]\n",
      "- [TRAIN] LOSS : 0.05356162258734306 [SCORE] : 0.5755908330281575\n",
      "[649/1000]\n",
      "- [VAL] LOSS : 0.025704210624098778 [SCORE] : 1.0\n",
      "[650/1000]\n",
      "- [TRAIN] LOSS : 0.05355860386043787 [SCORE] : 0.5755908330281575\n",
      "[650/1000]\n",
      "- [VAL] LOSS : 0.025701414793729782 [SCORE] : 1.0\n",
      "[651/1000]\n",
      "- [TRAIN] LOSS : 0.05355559239784877 [SCORE] : 0.5755908330281575\n",
      "[651/1000]\n",
      "- [VAL] LOSS : 0.02569858357310295 [SCORE] : 1.0\n",
      "[652/1000]\n",
      "- [TRAIN] LOSS : 0.053552603845794994 [SCORE] : 0.5755908330281575\n",
      "[652/1000]\n",
      "- [VAL] LOSS : 0.025695834308862686 [SCORE] : 1.0\n",
      "[653/1000]\n",
      "- [TRAIN] LOSS : 0.05354958969789247 [SCORE] : 0.5755908330281575\n",
      "[653/1000]\n",
      "- [VAL] LOSS : 0.025693079456686974 [SCORE] : 1.0\n",
      "[654/1000]\n",
      "- [TRAIN] LOSS : 0.053546622923264904 [SCORE] : 0.5755908330281575\n",
      "[654/1000]\n",
      "- [VAL] LOSS : 0.025690371170639992 [SCORE] : 1.0\n",
      "[655/1000]\n",
      "- [TRAIN] LOSS : 0.053543626920630534 [SCORE] : 0.5755908330281575\n",
      "[655/1000]\n",
      "- [VAL] LOSS : 0.025687692686915398 [SCORE] : 1.0\n",
      "[656/1000]\n",
      "- [TRAIN] LOSS : 0.05354063315317035 [SCORE] : 0.5755908330281575\n",
      "[656/1000]\n",
      "- [VAL] LOSS : 0.02568492665886879 [SCORE] : 1.0\n",
      "[657/1000]\n",
      "- [TRAIN] LOSS : 0.053537680425991614 [SCORE] : 0.5755908330281575\n",
      "[657/1000]\n",
      "- [VAL] LOSS : 0.025682279840111732 [SCORE] : 1.0\n",
      "[658/1000]\n",
      "- [TRAIN] LOSS : 0.0535347137444963 [SCORE] : 0.5755908330281575\n",
      "[658/1000]\n",
      "- [VAL] LOSS : 0.025679634883999825 [SCORE] : 1.0\n",
      "[659/1000]\n",
      "- [TRAIN] LOSS : 0.05353173715993762 [SCORE] : 0.5755908330281575\n",
      "[659/1000]\n",
      "- [VAL] LOSS : 0.025676975026726723 [SCORE] : 1.0\n",
      "[660/1000]\n",
      "- [TRAIN] LOSS : 0.05352879014487068 [SCORE] : 0.5755908330281575\n",
      "[660/1000]\n",
      "- [VAL] LOSS : 0.025674348697066307 [SCORE] : 1.0\n",
      "[661/1000]\n",
      "- [TRAIN] LOSS : 0.05352585068903863 [SCORE] : 0.5755908330281575\n",
      "[661/1000]\n",
      "- [VAL] LOSS : 0.025671767070889473 [SCORE] : 1.0\n",
      "[662/1000]\n",
      "- [TRAIN] LOSS : 0.05352293257601559 [SCORE] : 0.5755908330281575\n",
      "[662/1000]\n",
      "- [VAL] LOSS : 0.02566918358206749 [SCORE] : 1.0\n",
      "[663/1000]\n",
      "- [TRAIN] LOSS : 0.053519981602827706 [SCORE] : 0.5755908330281575\n",
      "[663/1000]\n",
      "- [VAL] LOSS : 0.02566658891737461 [SCORE] : 1.0\n",
      "[664/1000]\n",
      "- [TRAIN] LOSS : 0.0535170482005924 [SCORE] : 0.5755908330281575\n",
      "[664/1000]\n",
      "- [VAL] LOSS : 0.025664066895842552 [SCORE] : 1.0\n",
      "[665/1000]\n",
      "- [TRAIN] LOSS : 0.05351412895446022 [SCORE] : 0.5755908330281575\n",
      "[665/1000]\n",
      "- [VAL] LOSS : 0.02566153183579445 [SCORE] : 1.0\n",
      "[666/1000]\n",
      "- [TRAIN] LOSS : 0.05351122963863115 [SCORE] : 0.5755908330281575\n",
      "[666/1000]\n",
      "- [VAL] LOSS : 0.02565903402864933 [SCORE] : 1.0\n",
      "[667/1000]\n",
      "- [TRAIN] LOSS : 0.053508287854492666 [SCORE] : 0.5755908330281575\n",
      "[667/1000]\n",
      "- [VAL] LOSS : 0.02565651759505272 [SCORE] : 1.0\n",
      "[668/1000]\n",
      "- [TRAIN] LOSS : 0.0535053841304034 [SCORE] : 0.5755908330281575\n",
      "[668/1000]\n",
      "- [VAL] LOSS : 0.025654032826423645 [SCORE] : 1.0\n",
      "[669/1000]\n",
      "- [TRAIN] LOSS : 0.05350250440339247 [SCORE] : 0.5755908330281575\n",
      "[669/1000]\n",
      "- [VAL] LOSS : 0.02565157599747181 [SCORE] : 1.0\n",
      "[670/1000]\n",
      "- [TRAIN] LOSS : 0.05349961686879397 [SCORE] : 0.5755908330281575\n",
      "[670/1000]\n",
      "- [VAL] LOSS : 0.025649143382906914 [SCORE] : 1.0\n",
      "[671/1000]\n",
      "- [TRAIN] LOSS : 0.05349672219405572 [SCORE] : 0.5755908330281575\n",
      "[671/1000]\n",
      "- [VAL] LOSS : 0.025646697729825974 [SCORE] : 1.0\n",
      "[672/1000]\n",
      "- [TRAIN] LOSS : 0.05349382068961859 [SCORE] : 0.5755908330281575\n",
      "[672/1000]\n",
      "- [VAL] LOSS : 0.02564428746700287 [SCORE] : 1.0\n",
      "[673/1000]\n",
      "- [TRAIN] LOSS : 0.05349094951525331 [SCORE] : 0.5755908330281575\n",
      "[673/1000]\n",
      "- [VAL] LOSS : 0.02564193122088909 [SCORE] : 1.0\n",
      "[674/1000]\n",
      "- [TRAIN] LOSS : 0.053488097013905646 [SCORE] : 0.5755908330281575\n",
      "[674/1000]\n",
      "- [VAL] LOSS : 0.025639543309807777 [SCORE] : 1.0\n",
      "[675/1000]\n",
      "- [TRAIN] LOSS : 0.05348522930095593 [SCORE] : 0.5755908330281575\n",
      "[675/1000]\n",
      "- [VAL] LOSS : 0.02563716657459736 [SCORE] : 1.0\n",
      "[676/1000]\n",
      "- [TRAIN] LOSS : 0.053482346795499326 [SCORE] : 0.5755908330281575\n",
      "[676/1000]\n",
      "- [VAL] LOSS : 0.025634827092289925 [SCORE] : 1.0\n",
      "[677/1000]\n",
      "- [TRAIN] LOSS : 0.053479497258861856 [SCORE] : 0.5755908330281575\n",
      "[677/1000]\n",
      "- [VAL] LOSS : 0.025632476434111595 [SCORE] : 1.0\n",
      "[678/1000]\n",
      "- [TRAIN] LOSS : 0.05347665183556576 [SCORE] : 0.5755908330281575\n",
      "[678/1000]\n",
      "- [VAL] LOSS : 0.0256301611661911 [SCORE] : 1.0\n",
      "[679/1000]\n",
      "- [TRAIN] LOSS : 0.05347384326159954 [SCORE] : 0.5755908330281575\n",
      "[679/1000]\n",
      "- [VAL] LOSS : 0.025627845898270607 [SCORE] : 1.0\n",
      "[680/1000]\n",
      "- [TRAIN] LOSS : 0.053471022347609205 [SCORE] : 0.5755908330281575\n",
      "[680/1000]\n",
      "- [VAL] LOSS : 0.02562560699880123 [SCORE] : 1.0\n",
      "[681/1000]\n",
      "- [TRAIN] LOSS : 0.053468161076307294 [SCORE] : 0.5755908330281575\n",
      "[681/1000]\n",
      "- [VAL] LOSS : 0.025623325258493423 [SCORE] : 1.0\n",
      "[682/1000]\n",
      "- [TRAIN] LOSS : 0.053465303219854834 [SCORE] : 0.5755908330281575\n",
      "[682/1000]\n",
      "- [VAL] LOSS : 0.02562110312283039 [SCORE] : 1.0\n",
      "[683/1000]\n",
      "- [TRAIN] LOSS : 0.05346245807595551 [SCORE] : 0.5755908330281575\n",
      "[683/1000]\n",
      "- [VAL] LOSS : 0.025618860498070717 [SCORE] : 1.0\n",
      "[684/1000]\n",
      "- [TRAIN] LOSS : 0.05345968458180626 [SCORE] : 0.5755908330281575\n",
      "[684/1000]\n",
      "- [VAL] LOSS : 0.025616595521569252 [SCORE] : 1.0\n",
      "[685/1000]\n",
      "- [TRAIN] LOSS : 0.05345689823540548 [SCORE] : 0.5755908330281575\n",
      "[685/1000]\n",
      "- [VAL] LOSS : 0.02561444416642189 [SCORE] : 1.0\n",
      "[686/1000]\n",
      "- [TRAIN] LOSS : 0.05345411131468912 [SCORE] : 0.5755908330281575\n",
      "[686/1000]\n",
      "- [VAL] LOSS : 0.025612248107790947 [SCORE] : 1.0\n",
      "[687/1000]\n",
      "- [TRAIN] LOSS : 0.05345130511559546 [SCORE] : 0.5755908330281575\n",
      "[687/1000]\n",
      "- [VAL] LOSS : 0.025610068812966347 [SCORE] : 1.0\n",
      "[688/1000]\n",
      "- [TRAIN] LOSS : 0.053448490845039484 [SCORE] : 0.5755908330281575\n",
      "[688/1000]\n",
      "- [VAL] LOSS : 0.025607898831367493 [SCORE] : 1.0\n",
      "[689/1000]\n",
      "- [TRAIN] LOSS : 0.05344569004761676 [SCORE] : 0.5755908330281575\n",
      "[689/1000]\n",
      "- [VAL] LOSS : 0.02560577169060707 [SCORE] : 1.0\n",
      "[690/1000]\n",
      "- [TRAIN] LOSS : 0.053442925789083044 [SCORE] : 0.5755908330281575\n",
      "[690/1000]\n",
      "- [VAL] LOSS : 0.0256036426872015 [SCORE] : 1.0\n",
      "[691/1000]\n",
      "- [TRAIN] LOSS : 0.05344018225247661 [SCORE] : 0.5755908330281575\n",
      "[691/1000]\n",
      "- [VAL] LOSS : 0.025601567700505257 [SCORE] : 1.0\n",
      "[692/1000]\n",
      "- [TRAIN] LOSS : 0.05343739603025218 [SCORE] : 0.5755908330281575\n",
      "[692/1000]\n",
      "- [VAL] LOSS : 0.025599459186196327 [SCORE] : 1.0\n",
      "[693/1000]\n",
      "- [TRAIN] LOSS : 0.053434605508421856 [SCORE] : 0.5755908330281575\n",
      "[693/1000]\n",
      "- [VAL] LOSS : 0.02559739537537098 [SCORE] : 1.0\n",
      "[694/1000]\n",
      "- [TRAIN] LOSS : 0.0534318537140886 [SCORE] : 0.5755908330281575\n",
      "[694/1000]\n",
      "- [VAL] LOSS : 0.025595298036932945 [SCORE] : 1.0\n",
      "[695/1000]\n",
      "- [TRAIN] LOSS : 0.05342908498520652 [SCORE] : 0.5755908330281575\n",
      "[695/1000]\n",
      "- [VAL] LOSS : 0.025593256577849388 [SCORE] : 1.0\n",
      "[696/1000]\n",
      "- [TRAIN] LOSS : 0.053426367696374655 [SCORE] : 0.5755908330281575\n",
      "[696/1000]\n",
      "- [VAL] LOSS : 0.025590943172574043 [SCORE] : 1.0\n",
      "[697/1000]\n",
      "- [TRAIN] LOSS : 0.05342303514480591 [SCORE] : 0.5755908330281575\n",
      "[697/1000]\n",
      "- [VAL] LOSS : 0.025587690994143486 [SCORE] : 1.0\n",
      "[698/1000]\n",
      "- [TRAIN] LOSS : 0.053421387417862815 [SCORE] : 0.5755908330281575\n",
      "[698/1000]\n",
      "- [VAL] LOSS : 0.02558622881770134 [SCORE] : 1.0\n",
      "[699/1000]\n",
      "- [TRAIN] LOSS : 0.05341872240727146 [SCORE] : 0.5755908330281575\n",
      "[699/1000]\n",
      "- [VAL] LOSS : 0.025585055351257324 [SCORE] : 1.0\n",
      "[700/1000]\n",
      "- [TRAIN] LOSS : 0.05341562141353885 [SCORE] : 0.5755908330281575\n",
      "[700/1000]\n",
      "- [VAL] LOSS : 0.025583211332559586 [SCORE] : 1.0\n",
      "[701/1000]\n",
      "- [TRAIN] LOSS : 0.05341199510730803 [SCORE] : 0.5755908330281575\n",
      "[701/1000]\n",
      "- [VAL] LOSS : 0.025579988956451416 [SCORE] : 1.0\n",
      "[702/1000]\n",
      "- [TRAIN] LOSS : 0.053410324081778524 [SCORE] : 0.5755908330281575\n",
      "[702/1000]\n",
      "- [VAL] LOSS : 0.025578420609235764 [SCORE] : 1.0\n",
      "[703/1000]\n",
      "- [TRAIN] LOSS : 0.053407780298342304 [SCORE] : 0.5755908330281575\n",
      "[703/1000]\n",
      "- [VAL] LOSS : 0.025576960295438766 [SCORE] : 1.0\n",
      "[704/1000]\n",
      "- [TRAIN] LOSS : 0.053404173254966734 [SCORE] : 0.5755908330281575\n",
      "[704/1000]\n",
      "- [VAL] LOSS : 0.025574253872036934 [SCORE] : 1.0\n",
      "[705/1000]\n",
      "- [TRAIN] LOSS : 0.05340228386533757 [SCORE] : 0.5755908330281575\n",
      "[705/1000]\n",
      "- [VAL] LOSS : 0.02557291090488434 [SCORE] : 1.0\n",
      "[706/1000]\n",
      "- [TRAIN] LOSS : 0.05339961472588281 [SCORE] : 0.5755908330281575\n",
      "[706/1000]\n",
      "- [VAL] LOSS : 0.025571415200829506 [SCORE] : 1.0\n",
      "[707/1000]\n",
      "- [TRAIN] LOSS : 0.053395996273805695 [SCORE] : 0.5755908330281575\n",
      "[707/1000]\n",
      "- [VAL] LOSS : 0.02556864731013775 [SCORE] : 1.0\n",
      "[708/1000]\n",
      "- [TRAIN] LOSS : 0.05339420566645761 [SCORE] : 0.5755908330281575\n",
      "[708/1000]\n",
      "- [VAL] LOSS : 0.025567231699824333 [SCORE] : 1.0\n",
      "[709/1000]\n",
      "- [TRAIN] LOSS : 0.05339155790085594 [SCORE] : 0.5755908330281575\n",
      "[709/1000]\n",
      "- [VAL] LOSS : 0.0255661029368639 [SCORE] : 1.0\n",
      "[710/1000]\n",
      "- [TRAIN] LOSS : 0.05338853808740775 [SCORE] : 0.5755908330281575\n",
      "[710/1000]\n",
      "- [VAL] LOSS : 0.02556431293487549 [SCORE] : 1.0\n",
      "[711/1000]\n",
      "- [TRAIN] LOSS : 0.05338504910469055 [SCORE] : 0.5755908330281575\n",
      "[711/1000]\n",
      "- [VAL] LOSS : 0.025561271235346794 [SCORE] : 1.0\n",
      "[712/1000]\n",
      "- [TRAIN] LOSS : 0.05338341350046297 [SCORE] : 0.5755908330281575\n",
      "[712/1000]\n",
      "- [VAL] LOSS : 0.025559870526194572 [SCORE] : 1.0\n",
      "[713/1000]\n",
      "- [TRAIN] LOSS : 0.053380890066425005 [SCORE] : 0.5755908330281575\n",
      "[713/1000]\n",
      "- [VAL] LOSS : 0.02555854432284832 [SCORE] : 1.0\n",
      "[714/1000]\n",
      "- [TRAIN] LOSS : 0.0533772886885951 [SCORE] : 0.5755908330281575\n",
      "[714/1000]\n",
      "- [VAL] LOSS : 0.025555938482284546 [SCORE] : 1.0\n",
      "[715/1000]\n",
      "- [TRAIN] LOSS : 0.05337547929957509 [SCORE] : 0.5755908330281575\n",
      "[715/1000]\n",
      "- [VAL] LOSS : 0.02555469237267971 [SCORE] : 1.0\n",
      "[716/1000]\n",
      "- [TRAIN] LOSS : 0.05337284179404378 [SCORE] : 0.5755908330281575\n",
      "[716/1000]\n",
      "- [VAL] LOSS : 0.025553328916430473 [SCORE] : 1.0\n",
      "[717/1000]\n",
      "- [TRAIN] LOSS : 0.05336930365301669 [SCORE] : 0.5755908330281575\n",
      "[717/1000]\n",
      "- [VAL] LOSS : 0.0255507193505764 [SCORE] : 1.0\n",
      "[718/1000]\n",
      "- [TRAIN] LOSS : 0.05336748769817253 [SCORE] : 0.5755908330281575\n",
      "[718/1000]\n",
      "- [VAL] LOSS : 0.02554943971335888 [SCORE] : 1.0\n",
      "[719/1000]\n",
      "- [TRAIN] LOSS : 0.05336489761248231 [SCORE] : 0.5755908330281575\n",
      "[719/1000]\n",
      "- [VAL] LOSS : 0.025548147037625313 [SCORE] : 1.0\n",
      "[720/1000]\n",
      "- [TRAIN] LOSS : 0.05336137901370724 [SCORE] : 0.5755908330281575\n",
      "[720/1000]\n",
      "- [VAL] LOSS : 0.025545570999383926 [SCORE] : 1.0\n",
      "[721/1000]\n",
      "- [TRAIN] LOSS : 0.05335958859262367 [SCORE] : 0.5755908330281575\n",
      "[721/1000]\n",
      "- [VAL] LOSS : 0.025544369593262672 [SCORE] : 1.0\n",
      "[722/1000]\n",
      "- [TRAIN] LOSS : 0.05335701814231773 [SCORE] : 0.5755908330281575\n",
      "[722/1000]\n",
      "- [VAL] LOSS : 0.025543387979269028 [SCORE] : 1.0\n",
      "[723/1000]\n",
      "- [TRAIN] LOSS : 0.05335405869409442 [SCORE] : 0.5755908330281575\n",
      "[723/1000]\n",
      "- [VAL] LOSS : 0.025541724637150764 [SCORE] : 1.0\n",
      "[724/1000]\n",
      "- [TRAIN] LOSS : 0.05335063021630049 [SCORE] : 0.5755908330281575\n",
      "[724/1000]\n",
      "- [VAL] LOSS : 0.025538882240653038 [SCORE] : 1.0\n",
      "[725/1000]\n",
      "- [TRAIN] LOSS : 0.053349057833353676 [SCORE] : 0.5755908330281575\n",
      "[725/1000]\n",
      "- [VAL] LOSS : 0.025537613779306412 [SCORE] : 1.0\n",
      "[726/1000]\n",
      "- [TRAIN] LOSS : 0.05334660168737173 [SCORE] : 0.5755908330281575\n",
      "[726/1000]\n",
      "- [VAL] LOSS : 0.025536416098475456 [SCORE] : 1.0\n",
      "[727/1000]\n",
      "- [TRAIN] LOSS : 0.053343078742424645 [SCORE] : 0.5755908330281575\n",
      "[727/1000]\n",
      "- [VAL] LOSS : 0.025534044951200485 [SCORE] : 1.0\n",
      "[728/1000]\n",
      "- [TRAIN] LOSS : 0.05334131832545002 [SCORE] : 0.5755908330281575\n",
      "[728/1000]\n",
      "- [VAL] LOSS : 0.025532949715852737 [SCORE] : 1.0\n",
      "[729/1000]\n",
      "- [TRAIN] LOSS : 0.05333870931838949 [SCORE] : 0.5755908330281575\n",
      "[729/1000]\n",
      "- [VAL] LOSS : 0.025531768798828125 [SCORE] : 1.0\n",
      "[730/1000]\n",
      "- [TRAIN] LOSS : 0.05333520164713264 [SCORE] : 0.5755908330281575\n",
      "[730/1000]\n",
      "- [VAL] LOSS : 0.025529339909553528 [SCORE] : 1.0\n",
      "[731/1000]\n",
      "- [TRAIN] LOSS : 0.053333499717215696 [SCORE] : 0.5755908330281575\n",
      "[731/1000]\n",
      "- [VAL] LOSS : 0.025528201833367348 [SCORE] : 1.0\n",
      "[732/1000]\n",
      "- [TRAIN] LOSS : 0.053330917935818435 [SCORE] : 0.5755908330281575\n",
      "[732/1000]\n",
      "- [VAL] LOSS : 0.02552703395485878 [SCORE] : 1.0\n",
      "[733/1000]\n",
      "- [TRAIN] LOSS : 0.053327472896004716 [SCORE] : 0.5755908330281575\n",
      "[733/1000]\n",
      "- [VAL] LOSS : 0.025524629279971123 [SCORE] : 1.0\n",
      "[734/1000]\n",
      "- [TRAIN] LOSS : 0.053325748272861045 [SCORE] : 0.5755908330281575\n",
      "[734/1000]\n",
      "- [VAL] LOSS : 0.025523612275719643 [SCORE] : 1.0\n",
      "[735/1000]\n",
      "- [TRAIN] LOSS : 0.05332319991042216 [SCORE] : 0.5755908330281575\n",
      "[735/1000]\n",
      "- [VAL] LOSS : 0.025522468611598015 [SCORE] : 1.0\n",
      "[736/1000]\n",
      "- [TRAIN] LOSS : 0.05331971341123184 [SCORE] : 0.5755908330281575\n",
      "[736/1000]\n",
      "- [VAL] LOSS : 0.02552010677754879 [SCORE] : 1.0\n",
      "[737/1000]\n",
      "- [TRAIN] LOSS : 0.053318038846676546 [SCORE] : 0.5755908330281575\n",
      "[737/1000]\n",
      "- [VAL] LOSS : 0.025519106537103653 [SCORE] : 1.0\n",
      "[738/1000]\n",
      "- [TRAIN] LOSS : 0.05331548446168502 [SCORE] : 0.5755908330281575\n",
      "[738/1000]\n",
      "- [VAL] LOSS : 0.025518028065562248 [SCORE] : 1.0\n",
      "[739/1000]\n",
      "- [TRAIN] LOSS : 0.05331206200644374 [SCORE] : 0.5755908330281575\n",
      "[739/1000]\n",
      "- [VAL] LOSS : 0.025515643879771233 [SCORE] : 1.0\n",
      "[740/1000]\n",
      "- [TRAIN] LOSS : 0.05331037746121486 [SCORE] : 0.5755908330281575\n",
      "[740/1000]\n",
      "- [VAL] LOSS : 0.025514725595712662 [SCORE] : 1.0\n",
      "[741/1000]\n",
      "- [TRAIN] LOSS : 0.05330782647555073 [SCORE] : 0.5755908330281575\n",
      "[741/1000]\n",
      "- [VAL] LOSS : 0.025513632223010063 [SCORE] : 1.0\n",
      "[742/1000]\n",
      "- [TRAIN] LOSS : 0.05330438744276762 [SCORE] : 0.5755908330281575\n",
      "[742/1000]\n",
      "- [VAL] LOSS : 0.025511344894766808 [SCORE] : 1.0\n",
      "[743/1000]\n",
      "- [TRAIN] LOSS : 0.0533027007865409 [SCORE] : 0.5755908330281575\n",
      "[743/1000]\n",
      "- [VAL] LOSS : 0.025510361418128014 [SCORE] : 1.0\n",
      "[744/1000]\n",
      "- [TRAIN] LOSS : 0.053300204764430724 [SCORE] : 0.5755908330281575\n",
      "[744/1000]\n",
      "- [VAL] LOSS : 0.025509359315037727 [SCORE] : 1.0\n",
      "[745/1000]\n",
      "- [TRAIN] LOSS : 0.05329677515352766 [SCORE] : 0.5755908330281575\n",
      "[745/1000]\n",
      "- [VAL] LOSS : 0.025507105514407158 [SCORE] : 1.0\n",
      "[746/1000]\n",
      "- [TRAIN] LOSS : 0.05329510602168739 [SCORE] : 0.5755908330281575\n",
      "[746/1000]\n",
      "- [VAL] LOSS : 0.02550615929067135 [SCORE] : 1.0\n",
      "[747/1000]\n",
      "- [TRAIN] LOSS : 0.05329261398874223 [SCORE] : 0.5755908330281575\n",
      "[747/1000]\n",
      "- [VAL] LOSS : 0.02550547942519188 [SCORE] : 1.0\n",
      "[748/1000]\n",
      "- [TRAIN] LOSS : 0.05328980617535611 [SCORE] : 0.5755908330281575\n",
      "[748/1000]\n",
      "- [VAL] LOSS : 0.025504088029265404 [SCORE] : 1.0\n",
      "[749/1000]\n",
      "- [TRAIN] LOSS : 0.05328641108547648 [SCORE] : 0.5755908330281575\n",
      "[749/1000]\n",
      "- [VAL] LOSS : 0.025501567870378494 [SCORE] : 1.0\n",
      "[750/1000]\n",
      "- [TRAIN] LOSS : 0.053284961140404145 [SCORE] : 0.5755908330281575\n",
      "[750/1000]\n",
      "- [VAL] LOSS : 0.025500593706965446 [SCORE] : 1.0\n",
      "[751/1000]\n",
      "- [TRAIN] LOSS : 0.05328257029565672 [SCORE] : 0.5755908330281575\n",
      "[751/1000]\n",
      "- [VAL] LOSS : 0.025499699637293816 [SCORE] : 1.0\n",
      "[752/1000]\n",
      "- [TRAIN] LOSS : 0.05327916295888523 [SCORE] : 0.5755908330281575\n",
      "[752/1000]\n",
      "- [VAL] LOSS : 0.025497566908597946 [SCORE] : 1.0\n",
      "[753/1000]\n",
      "- [TRAIN] LOSS : 0.05327749407539765 [SCORE] : 0.5755908330281575\n",
      "[753/1000]\n",
      "- [VAL] LOSS : 0.025496764108538628 [SCORE] : 1.0\n",
      "[754/1000]\n",
      "- [TRAIN] LOSS : 0.05327501092106104 [SCORE] : 0.5755908330281575\n",
      "[754/1000]\n",
      "- [VAL] LOSS : 0.025495845824480057 [SCORE] : 1.0\n",
      "[755/1000]\n",
      "- [TRAIN] LOSS : 0.053271600728233656 [SCORE] : 0.5755908330281575\n",
      "[755/1000]\n",
      "- [VAL] LOSS : 0.025493668392300606 [SCORE] : 1.0\n",
      "[756/1000]\n",
      "- [TRAIN] LOSS : 0.053269951200733585 [SCORE] : 0.5755908330281575\n",
      "[756/1000]\n",
      "- [VAL] LOSS : 0.02549286186695099 [SCORE] : 1.0\n",
      "[757/1000]\n",
      "- [TRAIN] LOSS : 0.05326752377053102 [SCORE] : 0.5755908330281575\n",
      "[757/1000]\n",
      "- [VAL] LOSS : 0.025491978973150253 [SCORE] : 1.0\n",
      "[758/1000]\n",
      "- [TRAIN] LOSS : 0.05326412838573257 [SCORE] : 0.5755908330281575\n",
      "[758/1000]\n",
      "- [VAL] LOSS : 0.02548982761800289 [SCORE] : 1.0\n",
      "[759/1000]\n",
      "- [TRAIN] LOSS : 0.0532625015048931 [SCORE] : 0.5755908330281575\n",
      "[759/1000]\n",
      "- [VAL] LOSS : 0.025489045307040215 [SCORE] : 1.0\n",
      "[760/1000]\n",
      "- [TRAIN] LOSS : 0.05326004382222891 [SCORE] : 0.5755908330281575\n",
      "[760/1000]\n",
      "- [VAL] LOSS : 0.025488149374723434 [SCORE] : 1.0\n",
      "[761/1000]\n",
      "- [TRAIN] LOSS : 0.05325670232996345 [SCORE] : 0.5755908330281575\n",
      "[761/1000]\n",
      "- [VAL] LOSS : 0.025486066937446594 [SCORE] : 1.0\n",
      "[762/1000]\n",
      "- [TRAIN] LOSS : 0.0532550612774988 [SCORE] : 0.5755908330281575\n",
      "[762/1000]\n",
      "- [VAL] LOSS : 0.025485273450613022 [SCORE] : 1.0\n",
      "[763/1000]\n",
      "- [TRAIN] LOSS : 0.05325264424706499 [SCORE] : 0.5755908330281575\n",
      "[763/1000]\n",
      "- [VAL] LOSS : 0.0254844818264246 [SCORE] : 1.0\n",
      "[764/1000]\n",
      "- [TRAIN] LOSS : 0.053249296204497414 [SCORE] : 0.5755908330281575\n",
      "[764/1000]\n",
      "- [VAL] LOSS : 0.025482382625341415 [SCORE] : 1.0\n",
      "[765/1000]\n",
      "- [TRAIN] LOSS : 0.05324768129115303 [SCORE] : 0.5755908330281575\n",
      "[765/1000]\n",
      "- [VAL] LOSS : 0.025481639429926872 [SCORE] : 1.0\n",
      "[766/1000]\n",
      "- [TRAIN] LOSS : 0.053245254109303156 [SCORE] : 0.5755908330281575\n",
      "[766/1000]\n",
      "- [VAL] LOSS : 0.025480836629867554 [SCORE] : 1.0\n",
      "[767/1000]\n",
      "- [TRAIN] LOSS : 0.05324191246181727 [SCORE] : 0.5755908330281575\n",
      "[767/1000]\n",
      "- [VAL] LOSS : 0.025478797033429146 [SCORE] : 1.0\n",
      "[768/1000]\n",
      "- [TRAIN] LOSS : 0.05324032474309206 [SCORE] : 0.5755908330281575\n",
      "[768/1000]\n",
      "- [VAL] LOSS : 0.02547808177769184 [SCORE] : 1.0\n",
      "[769/1000]\n",
      "- [TRAIN] LOSS : 0.05323789827525616 [SCORE] : 0.5755908330281575\n",
      "[769/1000]\n",
      "- [VAL] LOSS : 0.02547728456556797 [SCORE] : 1.0\n",
      "[770/1000]\n",
      "- [TRAIN] LOSS : 0.05323453309635321 [SCORE] : 0.5755908330281575\n",
      "[770/1000]\n",
      "- [VAL] LOSS : 0.025475334376096725 [SCORE] : 1.0\n",
      "[771/1000]\n",
      "- [TRAIN] LOSS : 0.05323296627029776 [SCORE] : 0.5755908330281575\n",
      "[771/1000]\n",
      "- [VAL] LOSS : 0.02547430992126465 [SCORE] : 1.0\n",
      "[772/1000]\n",
      "- [TRAIN] LOSS : 0.053229993643860024 [SCORE] : 0.5755908330281575\n",
      "[772/1000]\n",
      "- [VAL] LOSS : 0.025472717359662056 [SCORE] : 1.0\n",
      "[773/1000]\n",
      "- [TRAIN] LOSS : 0.05322837638668716 [SCORE] : 0.5755908330281575\n",
      "[773/1000]\n",
      "- [VAL] LOSS : 0.025472372770309448 [SCORE] : 1.0\n",
      "[774/1000]\n",
      "- [TRAIN] LOSS : 0.053225748644520836 [SCORE] : 0.5755908330281575\n",
      "[774/1000]\n",
      "- [VAL] LOSS : 0.02547174133360386 [SCORE] : 1.0\n",
      "[775/1000]\n",
      "- [TRAIN] LOSS : 0.053222362681602436 [SCORE] : 0.5755908330281575\n",
      "[775/1000]\n",
      "- [VAL] LOSS : 0.025469720363616943 [SCORE] : 1.0\n",
      "[776/1000]\n",
      "- [TRAIN] LOSS : 0.05322075641403596 [SCORE] : 0.5755908330281575\n",
      "[776/1000]\n",
      "- [VAL] LOSS : 0.02546900510787964 [SCORE] : 1.0\n",
      "[777/1000]\n",
      "- [TRAIN] LOSS : 0.05321839380388459 [SCORE] : 0.5755908330281575\n",
      "[777/1000]\n",
      "- [VAL] LOSS : 0.02546824887394905 [SCORE] : 1.0\n",
      "[778/1000]\n",
      "- [TRAIN] LOSS : 0.05321514586297174 [SCORE] : 0.5755908330281575\n",
      "[778/1000]\n",
      "- [VAL] LOSS : 0.0254663098603487 [SCORE] : 1.0\n",
      "[779/1000]\n",
      "- [TRAIN] LOSS : 0.05321357478387654 [SCORE] : 0.5755908330281575\n",
      "[779/1000]\n",
      "- [VAL] LOSS : 0.02546575665473938 [SCORE] : 1.0\n",
      "[780/1000]\n",
      "- [TRAIN] LOSS : 0.05321119477351507 [SCORE] : 0.5755908330281575\n",
      "[780/1000]\n",
      "- [VAL] LOSS : 0.025465020909905434 [SCORE] : 1.0\n",
      "[781/1000]\n",
      "- [TRAIN] LOSS : 0.053207922431950765 [SCORE] : 0.5755908330281575\n",
      "[781/1000]\n",
      "- [VAL] LOSS : 0.02546314336359501 [SCORE] : 1.0\n",
      "[782/1000]\n",
      "- [TRAIN] LOSS : 0.05320632032429178 [SCORE] : 0.5755908330281575\n",
      "[782/1000]\n",
      "- [VAL] LOSS : 0.025462573394179344 [SCORE] : 1.0\n",
      "[783/1000]\n",
      "- [TRAIN] LOSS : 0.05320398040736715 [SCORE] : 0.5755908330281575\n",
      "[783/1000]\n",
      "- [VAL] LOSS : 0.025461871176958084 [SCORE] : 1.0\n",
      "[784/1000]\n",
      "- [TRAIN] LOSS : 0.05320070840728779 [SCORE] : 0.5755908330281575\n",
      "[784/1000]\n",
      "- [VAL] LOSS : 0.02546003647148609 [SCORE] : 1.0\n",
      "[785/1000]\n",
      "- [TRAIN] LOSS : 0.05319911250844598 [SCORE] : 0.5755908330281575\n",
      "[785/1000]\n",
      "- [VAL] LOSS : 0.02545945905148983 [SCORE] : 1.0\n",
      "[786/1000]\n",
      "- [TRAIN] LOSS : 0.05319677395746112 [SCORE] : 0.5755908330281575\n",
      "[786/1000]\n",
      "- [VAL] LOSS : 0.025458795949816704 [SCORE] : 1.0\n",
      "[787/1000]\n",
      "- [TRAIN] LOSS : 0.053193516299749416 [SCORE] : 0.5755908330281575\n",
      "[787/1000]\n",
      "- [VAL] LOSS : 0.025456979870796204 [SCORE] : 1.0\n",
      "[788/1000]\n",
      "- [TRAIN] LOSS : 0.05319195419239501 [SCORE] : 0.5755908330281575\n",
      "[788/1000]\n",
      "- [VAL] LOSS : 0.025456445291638374 [SCORE] : 1.0\n",
      "[789/1000]\n",
      "- [TRAIN] LOSS : 0.053189594919482865 [SCORE] : 0.5755908330281575\n",
      "[789/1000]\n",
      "- [VAL] LOSS : 0.02545580081641674 [SCORE] : 1.0\n",
      "[790/1000]\n",
      "- [TRAIN] LOSS : 0.05318634940000872 [SCORE] : 0.5755908330281575\n",
      "[790/1000]\n",
      "- [VAL] LOSS : 0.025454027578234673 [SCORE] : 1.0\n",
      "[791/1000]\n",
      "- [TRAIN] LOSS : 0.05318481492189069 [SCORE] : 0.5755908330281575\n",
      "[791/1000]\n",
      "- [VAL] LOSS : 0.025453507900238037 [SCORE] : 1.0\n",
      "[792/1000]\n",
      "- [TRAIN] LOSS : 0.05318248874197404 [SCORE] : 0.5755908330281575\n",
      "[792/1000]\n",
      "- [VAL] LOSS : 0.02545292116701603 [SCORE] : 1.0\n",
      "[793/1000]\n",
      "- [TRAIN] LOSS : 0.0531792515112708 [SCORE] : 0.5755908330281575\n",
      "[793/1000]\n",
      "- [VAL] LOSS : 0.02545112557709217 [SCORE] : 1.0\n",
      "[794/1000]\n",
      "- [TRAIN] LOSS : 0.053177702644219 [SCORE] : 0.5755908330281575\n",
      "[794/1000]\n",
      "- [VAL] LOSS : 0.025450656190514565 [SCORE] : 1.0\n",
      "[795/1000]\n",
      "- [TRAIN] LOSS : 0.053175367104510464 [SCORE] : 0.5755908330281575\n",
      "[795/1000]\n",
      "- [VAL] LOSS : 0.02545003779232502 [SCORE] : 1.0\n",
      "[796/1000]\n",
      "- [TRAIN] LOSS : 0.053172143766035636 [SCORE] : 0.5755908330281575\n",
      "[796/1000]\n",
      "- [VAL] LOSS : 0.02544831670820713 [SCORE] : 1.0\n",
      "[797/1000]\n",
      "- [TRAIN] LOSS : 0.05317062797645728 [SCORE] : 0.5755908330281575\n",
      "[797/1000]\n",
      "- [VAL] LOSS : 0.025447582826018333 [SCORE] : 1.0\n",
      "[798/1000]\n",
      "- [TRAIN] LOSS : 0.05316768125630915 [SCORE] : 0.5755908330281575\n",
      "[798/1000]\n",
      "- [VAL] LOSS : 0.025446193292737007 [SCORE] : 1.0\n",
      "[799/1000]\n",
      "- [TRAIN] LOSS : 0.053166096952433385 [SCORE] : 0.5755908330281575\n",
      "[799/1000]\n",
      "- [VAL] LOSS : 0.025446053594350815 [SCORE] : 1.0\n",
      "[800/1000]\n",
      "- [TRAIN] LOSS : 0.05316363770204286 [SCORE] : 0.5755908330281575\n",
      "[800/1000]\n",
      "- [VAL] LOSS : 0.025445638224482536 [SCORE] : 1.0\n",
      "[801/1000]\n",
      "- [TRAIN] LOSS : 0.053160323016345504 [SCORE] : 0.5755908330281575\n",
      "[801/1000]\n",
      "- [VAL] LOSS : 0.025443831458687782 [SCORE] : 1.0\n",
      "[802/1000]\n",
      "- [TRAIN] LOSS : 0.05315879195307692 [SCORE] : 0.5755908330281575\n",
      "[802/1000]\n",
      "- [VAL] LOSS : 0.025443339720368385 [SCORE] : 1.0\n",
      "[803/1000]\n",
      "- [TRAIN] LOSS : 0.05315652256831527 [SCORE] : 0.5755908330281575\n",
      "[803/1000]\n",
      "- [VAL] LOSS : 0.02544279396533966 [SCORE] : 1.0\n",
      "[804/1000]\n",
      "- [TRAIN] LOSS : 0.053153327712789175 [SCORE] : 0.5755908330281575\n",
      "[804/1000]\n",
      "- [VAL] LOSS : 0.025441106408834457 [SCORE] : 1.0\n",
      "[805/1000]\n",
      "- [TRAIN] LOSS : 0.053151827103768784 [SCORE] : 0.5755908330281575\n",
      "[805/1000]\n",
      "- [VAL] LOSS : 0.02544068731367588 [SCORE] : 1.0\n",
      "[806/1000]\n",
      "- [TRAIN] LOSS : 0.053149543205897015 [SCORE] : 0.5755908330281575\n",
      "[806/1000]\n",
      "- [VAL] LOSS : 0.025440244004130363 [SCORE] : 1.0\n",
      "[807/1000]\n",
      "- [TRAIN] LOSS : 0.05314634471821288 [SCORE] : 0.5755908330281575\n",
      "[807/1000]\n",
      "- [VAL] LOSS : 0.02543857879936695 [SCORE] : 1.0\n",
      "[808/1000]\n",
      "- [TRAIN] LOSS : 0.0531448299686114 [SCORE] : 0.5755908330281575\n",
      "[808/1000]\n",
      "- [VAL] LOSS : 0.02543818950653076 [SCORE] : 1.0\n",
      "[809/1000]\n",
      "- [TRAIN] LOSS : 0.05314252266349892 [SCORE] : 0.5755908330281575\n",
      "[809/1000]\n",
      "- [VAL] LOSS : 0.0254377331584692 [SCORE] : 1.0\n",
      "[810/1000]\n",
      "- [TRAIN] LOSS : 0.0531393283046782 [SCORE] : 0.5755908330281575\n",
      "[810/1000]\n",
      "- [VAL] LOSS : 0.025436094030737877 [SCORE] : 1.0\n",
      "[811/1000]\n",
      "- [TRAIN] LOSS : 0.05313786522795757 [SCORE] : 0.5755908330281575\n",
      "[811/1000]\n",
      "- [VAL] LOSS : 0.025435464456677437 [SCORE] : 1.0\n",
      "[812/1000]\n",
      "- [TRAIN] LOSS : 0.053134981480737524 [SCORE] : 0.5755908330281575\n",
      "[812/1000]\n",
      "- [VAL] LOSS : 0.025434156879782677 [SCORE] : 1.0\n",
      "[813/1000]\n",
      "- [TRAIN] LOSS : 0.053133422539879877 [SCORE] : 0.5755908330281575\n",
      "[813/1000]\n",
      "- [VAL] LOSS : 0.025434112176299095 [SCORE] : 1.0\n",
      "[814/1000]\n",
      "- [TRAIN] LOSS : 0.05313099914540847 [SCORE] : 0.5755908330281575\n",
      "[814/1000]\n",
      "- [VAL] LOSS : 0.025433827191591263 [SCORE] : 1.0\n",
      "[815/1000]\n",
      "- [TRAIN] LOSS : 0.05312774167396128 [SCORE] : 0.5755908330281575\n",
      "[815/1000]\n",
      "- [VAL] LOSS : 0.025432197377085686 [SCORE] : 1.0\n",
      "[816/1000]\n",
      "- [TRAIN] LOSS : 0.05312625969139238 [SCORE] : 0.5755908330281575\n",
      "[816/1000]\n",
      "- [VAL] LOSS : 0.02543174847960472 [SCORE] : 1.0\n",
      "[817/1000]\n",
      "- [TRAIN] LOSS : 0.05312401563860476 [SCORE] : 0.5755908330281575\n",
      "[817/1000]\n",
      "- [VAL] LOSS : 0.02543129399418831 [SCORE] : 1.0\n",
      "[818/1000]\n",
      "- [TRAIN] LOSS : 0.05312084057368338 [SCORE] : 0.5755908330281575\n",
      "[818/1000]\n",
      "- [VAL] LOSS : 0.025429727509617805 [SCORE] : 1.0\n",
      "[819/1000]\n",
      "- [TRAIN] LOSS : 0.053119403092811504 [SCORE] : 0.5755908330281575\n",
      "[819/1000]\n",
      "- [VAL] LOSS : 0.02542913891375065 [SCORE] : 1.0\n",
      "[820/1000]\n",
      "- [TRAIN] LOSS : 0.05311654792167246 [SCORE] : 0.5755908330281575\n",
      "[820/1000]\n",
      "- [VAL] LOSS : 0.025427961722016335 [SCORE] : 1.0\n",
      "[821/1000]\n",
      "- [TRAIN] LOSS : 0.053115047281607985 [SCORE] : 0.5755908330281575\n",
      "[821/1000]\n",
      "- [VAL] LOSS : 0.025427956134080887 [SCORE] : 1.0\n",
      "[822/1000]\n",
      "- [TRAIN] LOSS : 0.05311262509785593 [SCORE] : 0.5755908330281575\n",
      "[822/1000]\n",
      "- [VAL] LOSS : 0.02542770840227604 [SCORE] : 1.0\n",
      "[823/1000]\n",
      "- [TRAIN] LOSS : 0.053109319088980554 [SCORE] : 0.5755908330281575\n",
      "[823/1000]\n",
      "- [VAL] LOSS : 0.025426117703318596 [SCORE] : 1.0\n",
      "[824/1000]\n",
      "- [TRAIN] LOSS : 0.05310788561279575 [SCORE] : 0.5755908330281575\n",
      "[824/1000]\n",
      "- [VAL] LOSS : 0.025425760075449944 [SCORE] : 1.0\n",
      "[825/1000]\n",
      "- [TRAIN] LOSS : 0.053105633690332374 [SCORE] : 0.5755908330281575\n",
      "[825/1000]\n",
      "- [VAL] LOSS : 0.02542533352971077 [SCORE] : 1.0\n",
      "[826/1000]\n",
      "- [TRAIN] LOSS : 0.053102549724280836 [SCORE] : 0.5755908330281575\n",
      "[826/1000]\n",
      "- [VAL] LOSS : 0.025423819199204445 [SCORE] : 1.0\n",
      "[827/1000]\n",
      "- [TRAIN] LOSS : 0.0531011210133632 [SCORE] : 0.5755908330281575\n",
      "[827/1000]\n",
      "- [VAL] LOSS : 0.025423627346754074 [SCORE] : 1.0\n",
      "[828/1000]\n",
      "- [TRAIN] LOSS : 0.053098848167185984 [SCORE] : 0.5755908330281575\n",
      "[828/1000]\n",
      "- [VAL] LOSS : 0.025423303246498108 [SCORE] : 1.0\n",
      "[829/1000]\n",
      "- [TRAIN] LOSS : 0.05309572033584118 [SCORE] : 0.5755908330281575\n",
      "[829/1000]\n",
      "- [VAL] LOSS : 0.025421781465411186 [SCORE] : 1.0\n",
      "[830/1000]\n",
      "- [TRAIN] LOSS : 0.053094294387847185 [SCORE] : 0.5755908330281575\n",
      "[830/1000]\n",
      "- [VAL] LOSS : 0.025421300902962685 [SCORE] : 1.0\n",
      "[831/1000]\n",
      "- [TRAIN] LOSS : 0.0530914736756434 [SCORE] : 0.5755908330281575\n",
      "[831/1000]\n",
      "- [VAL] LOSS : 0.025420133024454117 [SCORE] : 1.0\n",
      "[832/1000]\n",
      "- [TRAIN] LOSS : 0.0530899739669015 [SCORE] : 0.5755908330281575\n",
      "[832/1000]\n",
      "- [VAL] LOSS : 0.0254201702773571 [SCORE] : 1.0\n",
      "[833/1000]\n",
      "- [TRAIN] LOSS : 0.05308760615686576 [SCORE] : 0.5755908330281575\n",
      "[833/1000]\n",
      "- [VAL] LOSS : 0.025419997051358223 [SCORE] : 1.0\n",
      "[834/1000]\n",
      "- [TRAIN] LOSS : 0.053084331254164376 [SCORE] : 0.5755908330281575\n",
      "[834/1000]\n",
      "- [VAL] LOSS : 0.025418493896722794 [SCORE] : 1.0\n",
      "[835/1000]\n",
      "- [TRAIN] LOSS : 0.05308291219795744 [SCORE] : 0.5755908330281575\n",
      "[835/1000]\n",
      "- [VAL] LOSS : 0.025418218225240707 [SCORE] : 1.0\n",
      "[836/1000]\n",
      "- [TRAIN] LOSS : 0.05308071837450067 [SCORE] : 0.5755908330281575\n",
      "[836/1000]\n",
      "- [VAL] LOSS : 0.025417905300855637 [SCORE] : 1.0\n",
      "[837/1000]\n",
      "- [TRAIN] LOSS : 0.05307763090046744 [SCORE] : 0.5755908330281575\n",
      "[837/1000]\n",
      "- [VAL] LOSS : 0.025416461750864983 [SCORE] : 1.0\n",
      "[838/1000]\n",
      "- [TRAIN] LOSS : 0.053076236074169475 [SCORE] : 0.5755908330281575\n",
      "[838/1000]\n",
      "- [VAL] LOSS : 0.025416018441319466 [SCORE] : 1.0\n",
      "[839/1000]\n",
      "- [TRAIN] LOSS : 0.053073426537836595 [SCORE] : 0.5755908330281575\n",
      "[839/1000]\n",
      "- [VAL] LOSS : 0.025414956733584404 [SCORE] : 1.0\n",
      "[840/1000]\n",
      "- [TRAIN] LOSS : 0.053071933519095185 [SCORE] : 0.5755908330281575\n",
      "[840/1000]\n",
      "- [VAL] LOSS : 0.025415053591132164 [SCORE] : 1.0\n",
      "[841/1000]\n",
      "- [TRAIN] LOSS : 0.05306957216622929 [SCORE] : 0.5755908330281575\n",
      "[841/1000]\n",
      "- [VAL] LOSS : 0.025414908304810524 [SCORE] : 1.0\n",
      "[842/1000]\n",
      "- [TRAIN] LOSS : 0.05306637086905539 [SCORE] : 0.5755908330281575\n",
      "[842/1000]\n",
      "- [VAL] LOSS : 0.025413470342755318 [SCORE] : 1.0\n",
      "[843/1000]\n",
      "- [TRAIN] LOSS : 0.053064906504005194 [SCORE] : 0.5755908330281575\n",
      "[843/1000]\n",
      "- [VAL] LOSS : 0.025412943214178085 [SCORE] : 1.0\n",
      "[844/1000]\n",
      "- [TRAIN] LOSS : 0.053062206227332355 [SCORE] : 0.5755908330281575\n",
      "[844/1000]\n",
      "- [VAL] LOSS : 0.02541187033057213 [SCORE] : 1.0\n",
      "[845/1000]\n",
      "- [TRAIN] LOSS : 0.05306076992613574 [SCORE] : 0.5755908330281575\n",
      "[845/1000]\n",
      "- [VAL] LOSS : 0.025412019342184067 [SCORE] : 1.0\n",
      "[846/1000]\n",
      "- [TRAIN] LOSS : 0.053058406027654804 [SCORE] : 0.5755908330281575\n",
      "[846/1000]\n",
      "- [VAL] LOSS : 0.025411901995539665 [SCORE] : 1.0\n",
      "[847/1000]\n",
      "- [TRAIN] LOSS : 0.053055217551688356 [SCORE] : 0.5755908330281575\n",
      "[847/1000]\n",
      "- [VAL] LOSS : 0.025410516187548637 [SCORE] : 1.0\n",
      "[848/1000]\n",
      "- [TRAIN] LOSS : 0.053053786031280956 [SCORE] : 0.5755908330281575\n",
      "[848/1000]\n",
      "- [VAL] LOSS : 0.02541026845574379 [SCORE] : 1.0\n",
      "[849/1000]\n",
      "- [TRAIN] LOSS : 0.05305163897573948 [SCORE] : 0.5755908330281575\n",
      "[849/1000]\n",
      "- [VAL] LOSS : 0.02541000209748745 [SCORE] : 1.0\n",
      "[850/1000]\n",
      "- [TRAIN] LOSS : 0.05304859008950492 [SCORE] : 0.5755908330281575\n",
      "[850/1000]\n",
      "- [VAL] LOSS : 0.025408681482076645 [SCORE] : 1.0\n",
      "[851/1000]\n",
      "- [TRAIN] LOSS : 0.05304719628766179 [SCORE] : 0.5755908330281575\n",
      "[851/1000]\n",
      "- [VAL] LOSS : 0.025408346205949783 [SCORE] : 1.0\n",
      "[852/1000]\n",
      "- [TRAIN] LOSS : 0.05304443674782912 [SCORE] : 0.5755908330281575\n",
      "[852/1000]\n",
      "- [VAL] LOSS : 0.02540736272931099 [SCORE] : 1.0\n",
      "[853/1000]\n",
      "- [TRAIN] LOSS : 0.05304296558412413 [SCORE] : 0.5755908330281575\n",
      "[853/1000]\n",
      "- [VAL] LOSS : 0.025407548993825912 [SCORE] : 1.0\n",
      "[854/1000]\n",
      "- [TRAIN] LOSS : 0.05304062310606241 [SCORE] : 0.5755908330281575\n",
      "[854/1000]\n",
      "- [VAL] LOSS : 0.02540745958685875 [SCORE] : 1.0\n",
      "[855/1000]\n",
      "- [TRAIN] LOSS : 0.053037438976267975 [SCORE] : 0.5755908330281575\n",
      "[855/1000]\n",
      "- [VAL] LOSS : 0.025406116619706154 [SCORE] : 1.0\n",
      "[856/1000]\n",
      "- [TRAIN] LOSS : 0.05303605208173394 [SCORE] : 0.5755908330281575\n",
      "[856/1000]\n",
      "- [VAL] LOSS : 0.025405708700418472 [SCORE] : 1.0\n",
      "[857/1000]\n",
      "- [TRAIN] LOSS : 0.053033348033204676 [SCORE] : 0.5755908330281575\n",
      "[857/1000]\n",
      "- [VAL] LOSS : 0.025404715910553932 [SCORE] : 1.0\n",
      "[858/1000]\n",
      "- [TRAIN] LOSS : 0.0530319057404995 [SCORE] : 0.5755908330281575\n",
      "[858/1000]\n",
      "- [VAL] LOSS : 0.025404900312423706 [SCORE] : 1.0\n",
      "[859/1000]\n",
      "- [TRAIN] LOSS : 0.05302960869545738 [SCORE] : 0.5755908330281575\n",
      "[859/1000]\n",
      "- [VAL] LOSS : 0.025404853746294975 [SCORE] : 1.0\n",
      "[860/1000]\n",
      "- [TRAIN] LOSS : 0.05302643810088436 [SCORE] : 0.5755908330281575\n",
      "[860/1000]\n",
      "- [VAL] LOSS : 0.025403542444109917 [SCORE] : 1.0\n",
      "[861/1000]\n",
      "- [TRAIN] LOSS : 0.05302504914191862 [SCORE] : 0.5755908330281575\n",
      "[861/1000]\n",
      "- [VAL] LOSS : 0.025403406471014023 [SCORE] : 1.0\n",
      "[862/1000]\n",
      "- [TRAIN] LOSS : 0.05302295113603274 [SCORE] : 0.5755908330281575\n",
      "[862/1000]\n",
      "- [VAL] LOSS : 0.025403203442692757 [SCORE] : 1.0\n",
      "[863/1000]\n",
      "- [TRAIN] LOSS : 0.0530199038485686 [SCORE] : 0.5755908330281575\n",
      "[863/1000]\n",
      "- [VAL] LOSS : 0.025401931256055832 [SCORE] : 1.0\n",
      "[864/1000]\n",
      "- [TRAIN] LOSS : 0.05301855301174025 [SCORE] : 0.5755908330281575\n",
      "[864/1000]\n",
      "- [VAL] LOSS : 0.025401663035154343 [SCORE] : 1.0\n",
      "[865/1000]\n",
      "- [TRAIN] LOSS : 0.053015831780309476 [SCORE] : 0.5755908330281575\n",
      "[865/1000]\n",
      "- [VAL] LOSS : 0.025400739163160324 [SCORE] : 1.0\n",
      "[866/1000]\n",
      "- [TRAIN] LOSS : 0.053014422518511614 [SCORE] : 0.5755908330281575\n",
      "[866/1000]\n",
      "- [VAL] LOSS : 0.025401027873158455 [SCORE] : 1.0\n",
      "[867/1000]\n",
      "- [TRAIN] LOSS : 0.05301204801847537 [SCORE] : 0.5755908330281575\n",
      "[867/1000]\n",
      "- [VAL] LOSS : 0.02540099248290062 [SCORE] : 1.0\n",
      "[868/1000]\n",
      "- [TRAIN] LOSS : 0.053008949011564256 [SCORE] : 0.5755908330281575\n",
      "[868/1000]\n",
      "- [VAL] LOSS : 0.02539973519742489 [SCORE] : 1.0\n",
      "[869/1000]\n",
      "- [TRAIN] LOSS : 0.05300757858591775 [SCORE] : 0.5755908330281575\n",
      "[869/1000]\n",
      "- [VAL] LOSS : 0.02539932169020176 [SCORE] : 1.0\n",
      "[870/1000]\n",
      "- [TRAIN] LOSS : 0.05300489598885179 [SCORE] : 0.5755908330281575\n",
      "[870/1000]\n",
      "- [VAL] LOSS : 0.025398407131433487 [SCORE] : 1.0\n",
      "[871/1000]\n",
      "- [TRAIN] LOSS : 0.05300348823269208 [SCORE] : 0.5755908330281575\n",
      "[871/1000]\n",
      "- [VAL] LOSS : 0.02539871446788311 [SCORE] : 1.0\n",
      "[872/1000]\n",
      "- [TRAIN] LOSS : 0.05300120012834668 [SCORE] : 0.5755908330281575\n",
      "[872/1000]\n",
      "- [VAL] LOSS : 0.025398751720786095 [SCORE] : 1.0\n",
      "[873/1000]\n",
      "- [TRAIN] LOSS : 0.052998091792687774 [SCORE] : 0.5755908330281575\n",
      "[873/1000]\n",
      "- [VAL] LOSS : 0.02539750561118126 [SCORE] : 1.0\n",
      "[874/1000]\n",
      "- [TRAIN] LOSS : 0.05299669434316456 [SCORE] : 0.5755908330281575\n",
      "[874/1000]\n",
      "- [VAL] LOSS : 0.025397175922989845 [SCORE] : 1.0\n",
      "[875/1000]\n",
      "- [TRAIN] LOSS : 0.05299404881273707 [SCORE] : 0.5755908330281575\n",
      "[875/1000]\n",
      "- [VAL] LOSS : 0.02539629116654396 [SCORE] : 1.0\n",
      "[876/1000]\n",
      "- [TRAIN] LOSS : 0.05299269041667382 [SCORE] : 0.5755908330281575\n",
      "[876/1000]\n",
      "- [VAL] LOSS : 0.025396589189767838 [SCORE] : 1.0\n",
      "[877/1000]\n",
      "- [TRAIN] LOSS : 0.05299039181942741 [SCORE] : 0.5755908330281575\n",
      "[877/1000]\n",
      "- [VAL] LOSS : 0.025396589189767838 [SCORE] : 1.0\n",
      "[878/1000]\n",
      "- [TRAIN] LOSS : 0.05298724148112039 [SCORE] : 0.5755908330281575\n",
      "[878/1000]\n",
      "- [VAL] LOSS : 0.025395425036549568 [SCORE] : 1.0\n",
      "[879/1000]\n",
      "- [TRAIN] LOSS : 0.05298592249552409 [SCORE] : 0.5755908330281575\n",
      "[879/1000]\n",
      "- [VAL] LOSS : 0.025395052507519722 [SCORE] : 1.0\n",
      "[880/1000]\n",
      "- [TRAIN] LOSS : 0.05298323504005869 [SCORE] : 0.5755908330281575\n",
      "[880/1000]\n",
      "- [VAL] LOSS : 0.025394225493073463 [SCORE] : 1.0\n",
      "[881/1000]\n",
      "- [TRAIN] LOSS : 0.052981921518221495 [SCORE] : 0.5755908330281575\n",
      "[881/1000]\n",
      "- [VAL] LOSS : 0.025394560769200325 [SCORE] : 1.0\n",
      "성능 및 손실 개선이 없어서 학습\n"
     ]
    }
   ],
   "source": [
    "# 학습의 효과 확인 손실값과 성능평가값 저장 필요\n",
    "LOSS_HISTORY, SCORE_HISTROY=[[],[]], [[],[]]\n",
    "\n",
    "f1_score_metric = MulticlassF1Score(num_classes=3)\n",
    "# 중단 기준\n",
    "BREAK_CNT =0\n",
    "for epoch in range(1,EPOCH+1):\n",
    "    # 학습 모드로 모델 설정\n",
    "    model.train()\n",
    "\n",
    "    # 배치크기 만큼 데이터 로딩해서 학습 진행\n",
    "    loss_total, score_total=0, 0\n",
    "    for featureTS, targetTS in trainDL:\n",
    "\n",
    "        # 학습 진행\n",
    "        pre_y=model(featureTS)\n",
    "\n",
    "        # 손실 계산 : nn.CrossEntropyLoss 요구사항 : 정답/타겟은 0D 또는 1D, 타입은 long\n",
    "        loss=crossLoss(pre_y, targetTS.reshape(-1).long())\n",
    "        loss_total += loss.item()\n",
    "\n",
    "        # 성능평가 계산\n",
    "        score=f1_score_metric(pre_y, targetTS.reshape(-1))\n",
    "        score_total += score.item()\n",
    "\n",
    "        # 최적화 진행\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 에포크 당 검증기능\n",
    "    # 모델 검증 모드 설정\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 검증 데이터셋\n",
    "        val_featureTS=torch.FloatTensor(valDS.featureDF.values)\n",
    "        val_targetTS=torch.FloatTensor(valDS.targetDF.values)\n",
    "\n",
    "        # 추론/평가\n",
    "        pre_val=model(val_featureTS)\n",
    "\n",
    "        # 손실\n",
    "        loss_val=crossLoss(pre_val, val_targetTS.reshape(-1).long())\n",
    "\n",
    "        # 성능평가\n",
    "        score_val=MulticlassF1Score(num_classes=3)(pre_val, val_targetTS.reshape(-1))\n",
    "    \n",
    "    # 에포크 당 손실값과 성능평가값 저장\n",
    "    LOSS_HISTORY[0].append(loss_total/BATCH_CNT)\n",
    "    SCORE_HISTROY[0].append(score_total/BATCH_CNT)\n",
    "\n",
    "    LOSS_HISTORY[1].append(loss_val)\n",
    "    SCORE_HISTROY[1].append(score_val)\n",
    "\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- [TRAIN] LOSS : {LOSS_HISTORY[0][-1]} [SCORE] : {SCORE_HISTROY[0][-1]}')\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- [VAL] LOSS : {LOSS_HISTORY[1][-1]} [SCORE] : {SCORE_HISTROY[1][-1]}')\n",
    "\n",
    "\n",
    "    # 학습이 잘안되면 중단\n",
    "    # LOSS를 기준으로 중단\n",
    "\n",
    "    if len(LOSS_HISTORY[1]) >=2:\n",
    "        if LOSS_HISTORY[1][-1] >= LOSS_HISTORY[1][-2]:\n",
    "            BREAK_CNT +=1\n",
    "\n",
    "    if len(LOSS_HISTORY[1]) ==1:\n",
    "        # 첫번째라서 무조건 저장\n",
    "        torch.save(model.state_dict(), SAVE_FILE)\n",
    "        # 모델 저장\n",
    "        torch.save(model, SAVE_MODEL)\n",
    "    else:\n",
    "        if LOSS_HISTORY[1][-1] < min(LOSS_HISTORY[1]):      # 현재 에포크를 포함하여 전체 검증 손실에서 최소값 찾음음\n",
    "            torch.save(model.state_dict(), SAVE_FILE)\n",
    "            torch.save(model, SAVE_MODEL)\n",
    "            \n",
    "    # 학습 중단 여부 설정\n",
    "    if BREAK_CNT >10 :\n",
    "        print(\"성능 및 손실 개선이 없어서 학습\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3340e+01,  1.0752e+00, -1.7030e+01],\n",
      "        [-3.4867e+00,  1.9029e+00, -5.6810e+00],\n",
      "        [-8.8647e+00,  2.6510e+00, -4.5921e+00],\n",
      "        [ 1.2178e+01,  1.0020e+00, -1.5562e+01],\n",
      "        [-2.1906e+01,  5.4833e-02,  1.8794e+00],\n",
      "        [-1.2635e+01,  1.6654e+00, -2.1426e+00],\n",
      "        [-1.9420e+01, -3.3531e-01,  2.1551e+00],\n",
      "        [ 8.9118e+00,  1.1320e+00, -1.3198e+01],\n",
      "        [ 9.6848e+00,  9.2122e-01, -1.2813e+01],\n",
      "        [-2.3739e+01, -1.4731e+00,  4.8034e+00],\n",
      "        [-1.0614e+01,  1.6544e+00, -2.2942e+00],\n",
      "        [ 1.0914e+01,  1.0301e+00, -1.4531e+01],\n",
      "        [-2.3714e+01, -1.3736e+00,  4.6799e+00],\n",
      "        [-1.0322e+01,  2.3512e+00, -3.6686e+00],\n",
      "        [-1.2493e+01,  1.3667e+00, -1.5887e+00],\n",
      "        [ 1.0168e+01,  8.7539e-01, -1.3023e+01],\n",
      "        [-8.0315e+00,  2.4027e+00, -4.0656e+00],\n",
      "        [-1.3530e+01,  9.1984e-01, -6.3006e-01],\n",
      "        [ 1.1115e+01,  9.3500e-01, -1.4218e+01],\n",
      "        [ 1.1177e+01,  9.3895e-01, -1.4298e+01],\n",
      "        [-1.2030e+01,  1.2014e+00, -1.2484e+00],\n",
      "        [-1.4191e+01,  6.2920e-01, -1.0604e-02],\n",
      "        [-1.5200e+01,  9.5661e-01, -5.5576e-01],\n",
      "        [ 1.1253e+01,  9.4375e-01, -1.4394e+01],\n",
      "        [-2.1385e+01, -4.1907e-01,  2.7363e+00],\n",
      "        [-9.2333e+00,  2.1461e+00, -3.3724e+00],\n",
      "        [ 1.2955e+01,  1.0509e+00, -1.6544e+01],\n",
      "        [ 1.1420e+01,  9.5424e-01, -1.4604e+01],\n",
      "        [-1.1782e+01,  1.7061e+00, -2.2031e+00],\n",
      "        [-1.9405e+01, -7.9833e-01,  3.1257e+00],\n",
      "        [-1.1914e+01,  1.6853e+00, -2.2110e+00],\n",
      "        [-2.4733e+01, -1.4274e+00,  4.8386e+00],\n",
      "        [-7.5623e+00,  2.3226e+00, -4.0825e+00],\n",
      "        [-2.3527e+01, -1.7078e+00,  5.3474e+00],\n",
      "        [-2.2322e+01, -1.6203e+00,  5.1125e+00],\n",
      "        [ 1.1789e+01,  9.7750e-01, -1.5071e+01],\n",
      "        [-1.0545e+01,  1.3955e+00, -1.8393e+00],\n",
      "        [ 1.1429e+01,  9.5481e-01, -1.4616e+01]])\n"
     ]
    }
   ],
   "source": [
    "# 모델 검증 모드 설정\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 검증 데이터셋\n",
    "    test_featureTS=torch.FloatTensor(testDS.featureDF.values)\n",
    "    test_targetTS=torch.FloatTensor(testDS.targetDF.values)\n",
    "\n",
    "    # 추론/평가\n",
    "    pre_val=model(test_featureTS)\n",
    "    print(pre_val)\n",
    "\n",
    "    # 손실\n",
    "    loss_test=crossLoss(pre_val, test_targetTS.reshape(-1).long())\n",
    "\n",
    "    # 성능평가\n",
    "    score_test=MulticlassF1Score(num_classes=3)(pre_val, test_targetTS.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 후 loss 시각화\n",
    "\n",
    "THRESHOLD=len(LOSS_HISTORY[1])\n",
    "fg, axes=plt.subplots(1,2,figsize=(10,5))\n",
    "axes[0].plot(range(1, THRESHOLD+1), LOSS_HISTORY[0][:THRESHOLD], label='Train')\n",
    "axes[0].plot(range(1, THRESHOLD+1), LOSS_HISTORY[1][:THRESHOLD], label='Val')\n",
    "axes[0].grid()\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Epoch&Loss')\n",
    "\n",
    "axes[1].plot(range(1, THRESHOLD+1), SCORE_HISTROY[0][:THRESHOLD], label='Train')\n",
    "axes[1].plot(range(1, THRESHOLD+1), SCORE_HISTROY[1][:THRESHOLD], label='Val')\n",
    "axes[1].grid()\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Epoch&Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
