{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 정의 모델 클래스\n",
    "- 부모클래스 : nn.Module  \n",
    "- 필수오버라이딩  \n",
    "    * _ _init_ _() : 모델 층 구성 즉, 설계  \n",
    "    * forward() : 순방향 학습 진행 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈로딩 \n",
    "import torch                                # 텐서 관련 모듈\n",
    "import torch.nn as nn                       # 인공신경망 관련 모듈\n",
    "import torch.nn.functional as F             # 인공신경망 관련 함수들 모듈 ( 손실함수, 활성화함수 등등)\n",
    "import torch.optim as optim                 # 최적화 관련 모듈 (가중치, 절편 빠르게 찾아주는 알고리즘)\n",
    "from torchinfo import summary               # 모델 구조 및 정보 관련모듈\n",
    "from torchmetrics.regression import *       # 회귀 성능 지표 관련 모듈\n",
    "from torchmetrics.classification import *   # 분류 성능 지표 관련 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 텐서 저장 및 실행 위치 설정\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기본] 신경망클래스 <hr>\n",
    "    * 입력층 - 입력 피쳐 고정\n",
    "    * 출력층 - 출력 타겟 고정\n",
    "    * 은닉층 - 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "# 데이터셋 : 피쳐 4개, 타겟 1개, 회귀\n",
    "# 입력층 :  입력     4개      출력  20개   AF   ReLU\n",
    "# 은닉층 :  입력    20개      출력 100개   AF   ReLU\n",
    "# 출력층 :  입력   100개      출력  1개    AF   X,  Sigmoid & Softmax (애넨 분류일때)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐가 4개 고정인 정적인 함수수\n",
    "class Mymodel(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 callback func)\n",
    "    def __init__(self):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(4,20)\n",
    "        self.hidden_layer=nn.Linear(20, 100)\n",
    "        self.output_layer=nn.Linear(100,1)\n",
    "\n",
    "    \n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 callback func: 시스템에서 호출되는 함수)\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y= self.input_layer(x)   # y= x1W1+x2W2+x3W3+x4W4+b\n",
    "        y=F.relu(y)              # 0<=y     ---------------> 죽은 relue ==> 해결하기위해 leakyReLu\n",
    "\n",
    "        y=self.hidden_layer(y)   # y= x1W1+x2W2+x3W3+x4W4......X20W20+b\n",
    "        y=F.relu(y)\n",
    "\n",
    "        return self.output_layer(y)     # 1개 퍼셉트론 : x1W1+x2W2+x3W3+x4W4......X100W100+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력 피쳐가 동적인 모델델\n",
    "class Mymodel2(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 callback func)\n",
    "    def __init__(self, in_feature):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_feature,20)\n",
    "        self.hidden_layer=nn.Linear(20, 100)\n",
    "        self.output_layer=nn.Linear(100,1)\n",
    "\n",
    "    \n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 callback func: 시스템에서 호출되는 함수)\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y= self.input_layer(x)   # y= x1W1+x2W2+x3W3+x4W4+b\n",
    "        y=F.relu(y)              # 0<=y     ---------------> 죽은 relue ==> 해결하기위해 leakyReLu\n",
    "\n",
    "        y=self.hidden_layer(y)   # y= x1W1+x2W2+x3W3+x4W4......X20W20+b\n",
    "        y=F.relu(y)\n",
    "\n",
    "        return self.output_layer(y)     # 1개 퍼셉트론 : x1W1+x2W2+x3W3+x4W4......X100W100+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력 피쳐, 은닉층 퍼셉트론수가 동적인 모델델\n",
    "class Mymodel3(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 callback func)\n",
    "    def __init__(self, in_feature, in_out, h_out):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_feature,in_out)\n",
    "        self.hidden_layer=nn.Linear(in_out, h_out)\n",
    "        self.output_layer=nn.Linear(h_out,1)\n",
    "\n",
    "    \n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 callback func: 시스템에서 호출되는 함수)\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y= self.input_layer(x)   # y= x1W1+x2W2+x3W3+x4W4+b\n",
    "        y=F.relu(y)              # 0<=y     ---------------> 죽은 relue ==> 해결하기위해 leakyReLu\n",
    "\n",
    "        y=self.hidden_layer(y)   # y= x1W1+x2W2+x3W3+x4W4......X20W20+b\n",
    "        y=F.relu(y)\n",
    "\n",
    "        return self.output_layer(y)     # 1개 퍼셉트론 : x1W1+x2W2+x3W3+x4W4......X100W100+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델\n",
    "class Mymodel4(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 callback func)\n",
    "    def __init__(self, in_feature, h_range ,in_out, h_out):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer = nn.Linear(in_feature,in_out)\n",
    "        for _ in range(h_range):\n",
    "            self.hidden_layers.append(nn.Linear(in_out, h_out))\n",
    "            in_out = h_out\n",
    "        self.output_layer=nn.Linear(h_out,1)\n",
    "\n",
    "    \n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 callback func: 시스템에서 호출되는 함수)\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y= self.input_layer(x)   # y= x1W1+x2W2+x3W3+x4W4+b\n",
    "        y=F.relu(y)              # 0<=y     ---------------> 죽은 relue ==> 해결하기위해 leakyReLu\n",
    "\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            y=hidden_layer(y)   # y= x1W1+x2W2+x3W3+x4W4......X20W20+b\n",
    "            y=F.relu(y)\n",
    "\n",
    "        return self.output_layer(y)     # 1개 퍼셉트론 : x1W1+x2W2+x3W3+x4W4......X100W100+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mymodel4(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=100, out_features=70, bias=True)\n",
       "    (1-4): 4 x Linear(in_features=70, out_features=70, bias=True)\n",
       "  )\n",
       "  (input_layer): Linear(in_features=3, out_features=100, bias=True)\n",
       "  (output_layer): Linear(in_features=70, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 모델 인스턴스 생성\n",
    "m1 = Mymodel4(3,5,100,70)\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hidden_layers.0.weight', Parameter containing:\n",
      "tensor([[-5.0983e-02,  7.0246e-02,  6.4331e-04,  ..., -4.9937e-02,\n",
      "         -6.7589e-02,  3.7483e-02],\n",
      "        [-7.9416e-02, -8.3106e-02, -8.9932e-02,  ..., -2.5801e-02,\n",
      "         -1.4218e-03,  9.8305e-02],\n",
      "        [ 6.7165e-02, -7.4106e-03,  9.8037e-02,  ..., -3.8047e-02,\n",
      "          7.0047e-02,  7.0392e-02],\n",
      "        ...,\n",
      "        [ 2.5147e-03,  1.6005e-02, -7.4095e-02,  ...,  2.7428e-02,\n",
      "         -9.5036e-02, -4.7379e-02],\n",
      "        [-1.0175e-02,  6.7984e-02, -8.7000e-05,  ..., -4.0919e-02,\n",
      "          9.7483e-02,  7.1616e-02],\n",
      "        [-1.7923e-03, -8.5366e-02,  3.0353e-02,  ..., -1.0551e-02,\n",
      "         -2.5179e-02,  2.2959e-02]], requires_grad=True))\n",
      "('hidden_layers.0.bias', Parameter containing:\n",
      "tensor([ 0.0512,  0.0184, -0.0734,  0.0909,  0.0447,  0.0348, -0.0207,  0.0914,\n",
      "         0.0019, -0.0023,  0.0433, -0.0067,  0.0177,  0.0003,  0.0237,  0.0021,\n",
      "        -0.0635, -0.0293,  0.0873,  0.0653, -0.0105,  0.0080, -0.0652,  0.0730,\n",
      "        -0.0960,  0.0497,  0.0720,  0.0224,  0.0450, -0.0911, -0.0324, -0.0469,\n",
      "         0.0416, -0.0877, -0.0623,  0.0391,  0.0246, -0.0944,  0.0729,  0.0613,\n",
      "         0.0179,  0.0066, -0.0801, -0.0987, -0.0768, -0.0718, -0.0653, -0.0523,\n",
      "         0.0942, -0.0986,  0.0863,  0.0857,  0.0146,  0.0049,  0.0333,  0.0115,\n",
      "        -0.0778, -0.0807,  0.0946, -0.0352,  0.0531,  0.0372, -0.0504, -0.0086,\n",
      "        -0.0189,  0.0939,  0.0091,  0.0320, -0.0607, -0.0511],\n",
      "       requires_grad=True))\n",
      "('hidden_layers.1.weight', Parameter containing:\n",
      "tensor([[-0.0895,  0.0545, -0.0978,  ..., -0.0254,  0.0383,  0.0214],\n",
      "        [-0.1041,  0.0738, -0.0860,  ..., -0.0946, -0.0360, -0.0568],\n",
      "        [-0.0044,  0.0473,  0.0564,  ...,  0.1072,  0.1049, -0.0152],\n",
      "        ...,\n",
      "        [-0.1067, -0.0412,  0.0972,  ..., -0.0666, -0.0035, -0.0986],\n",
      "        [-0.0901,  0.0240, -0.0219,  ...,  0.1013,  0.0816,  0.0538],\n",
      "        [-0.0793, -0.0114,  0.0804,  ...,  0.0768, -0.0659, -0.0403]],\n",
      "       requires_grad=True))\n",
      "('hidden_layers.1.bias', Parameter containing:\n",
      "tensor([-0.0428, -0.0524,  0.0930,  0.0871, -0.0056, -0.1151,  0.0409,  0.0056,\n",
      "        -0.0976, -0.0249, -0.0527,  0.0407,  0.0656, -0.0089, -0.1179,  0.0784,\n",
      "        -0.0287,  0.0722, -0.0028,  0.0217, -0.1084, -0.1008,  0.0153, -0.0442,\n",
      "         0.0315, -0.0108, -0.0880, -0.0154, -0.0467,  0.0610,  0.0464, -0.0254,\n",
      "         0.0739, -0.0101,  0.1185,  0.0565,  0.0434,  0.0481,  0.0658, -0.0994,\n",
      "         0.1127,  0.0751,  0.0138,  0.0311, -0.0178,  0.0076, -0.0100, -0.0425,\n",
      "        -0.0041,  0.0895, -0.0871,  0.0973, -0.1066,  0.1083, -0.0560,  0.0218,\n",
      "         0.0650,  0.1162,  0.0445, -0.0717, -0.0416,  0.0363,  0.0444, -0.1003,\n",
      "         0.0698,  0.0544,  0.0025, -0.0902, -0.1149, -0.0329],\n",
      "       requires_grad=True))\n",
      "('hidden_layers.2.weight', Parameter containing:\n",
      "tensor([[ 0.1067,  0.1155, -0.0231,  ..., -0.1103,  0.0269,  0.0929],\n",
      "        [ 0.0798,  0.0913, -0.0545,  ...,  0.0325, -0.0284,  0.0642],\n",
      "        [ 0.0383, -0.0131, -0.1096,  ...,  0.0857,  0.0934, -0.0229],\n",
      "        ...,\n",
      "        [ 0.0215, -0.0465, -0.0141,  ...,  0.0540,  0.0513, -0.0207],\n",
      "        [-0.0688, -0.0601,  0.0688,  ...,  0.0923,  0.0626, -0.0787],\n",
      "        [ 0.0801, -0.0773,  0.0555,  ..., -0.1171, -0.0362,  0.0139]],\n",
      "       requires_grad=True))\n",
      "('hidden_layers.2.bias', Parameter containing:\n",
      "tensor([-0.0459, -0.0977, -0.0374, -0.0691,  0.1148,  0.1180, -0.0031, -0.0479,\n",
      "         0.0688, -0.0217,  0.0953,  0.0715,  0.0895,  0.1106, -0.0307, -0.1083,\n",
      "        -0.0302, -0.1142,  0.0599, -0.0266,  0.0749, -0.0879,  0.0393,  0.0327,\n",
      "         0.1048, -0.0847,  0.0289, -0.0641, -0.0281,  0.0931, -0.0460,  0.0629,\n",
      "         0.0275,  0.0019,  0.1000,  0.0175, -0.0828,  0.1094,  0.0826, -0.0459,\n",
      "         0.0813, -0.0203,  0.0355,  0.0091, -0.0996,  0.1148,  0.0894, -0.1063,\n",
      "        -0.1164,  0.1156,  0.0494, -0.0184, -0.0413,  0.0948,  0.0772, -0.1184,\n",
      "        -0.0461, -0.0596, -0.1091, -0.0175,  0.0452, -0.0286, -0.0830, -0.0234,\n",
      "        -0.0271, -0.0711,  0.0295, -0.0996,  0.0384, -0.0171],\n",
      "       requires_grad=True))\n",
      "('hidden_layers.3.weight', Parameter containing:\n",
      "tensor([[-0.0414,  0.0513,  0.0735,  ..., -0.0168,  0.0091, -0.0504],\n",
      "        [ 0.0530,  0.0420, -0.0438,  ...,  0.0657,  0.0173, -0.0923],\n",
      "        [-0.0379,  0.0204, -0.0453,  ...,  0.0027,  0.0491, -0.0905],\n",
      "        ...,\n",
      "        [-0.0062,  0.0936,  0.0323,  ..., -0.0592,  0.0976,  0.0385],\n",
      "        [ 0.0439, -0.0027, -0.1087,  ...,  0.0851, -0.0152,  0.0214],\n",
      "        [-0.0574,  0.0948, -0.0090,  ...,  0.0344,  0.0530, -0.0118]],\n",
      "       requires_grad=True))\n",
      "('hidden_layers.3.bias', Parameter containing:\n",
      "tensor([ 0.0305,  0.0557, -0.0169, -0.0125, -0.0080, -0.0610,  0.0825, -0.1042,\n",
      "         0.0243, -0.0511,  0.0235, -0.0020, -0.0336, -0.0444, -0.0868,  0.0100,\n",
      "         0.0152,  0.0632, -0.0304, -0.0811,  0.0217, -0.0520, -0.1157,  0.0153,\n",
      "         0.0059, -0.0340,  0.0322, -0.0115,  0.0576,  0.0480,  0.0440, -0.1108,\n",
      "        -0.0392, -0.0300,  0.0267,  0.0818, -0.0157, -0.0489, -0.0929, -0.0732,\n",
      "        -0.0193,  0.0168,  0.0386,  0.0907,  0.1072, -0.0135, -0.0190,  0.0250,\n",
      "        -0.0390,  0.0099,  0.0428,  0.1009,  0.0373,  0.1163, -0.0261, -0.0789,\n",
      "        -0.0482,  0.0757,  0.0693, -0.0873, -0.0394,  0.0606, -0.0270,  0.0763,\n",
      "        -0.0132,  0.0398, -0.0727, -0.0304,  0.1135,  0.1037],\n",
      "       requires_grad=True))\n",
      "('hidden_layers.4.weight', Parameter containing:\n",
      "tensor([[-0.0370, -0.0669,  0.0208,  ..., -0.0976, -0.0740,  0.0682],\n",
      "        [-0.0630, -0.0477, -0.0275,  ..., -0.0897, -0.0071,  0.0934],\n",
      "        [-0.0446,  0.0989,  0.0812,  ..., -0.0558, -0.0477, -0.0528],\n",
      "        ...,\n",
      "        [-0.0844, -0.0626, -0.1012,  ...,  0.0310,  0.0946, -0.0114],\n",
      "        [ 0.0300, -0.0422, -0.0623,  ...,  0.0983, -0.0861, -0.0987],\n",
      "        [ 0.1061, -0.0653,  0.0710,  ..., -0.0654, -0.1188,  0.1121]],\n",
      "       requires_grad=True))\n",
      "('hidden_layers.4.bias', Parameter containing:\n",
      "tensor([ 0.0531, -0.0598,  0.1174, -0.0209, -0.0228, -0.0969,  0.0615, -0.0446,\n",
      "         0.0248, -0.0065,  0.0408,  0.0106,  0.0755, -0.0461,  0.0528, -0.0092,\n",
      "        -0.0855, -0.0616, -0.0847, -0.0902,  0.0425,  0.0352, -0.0149,  0.0895,\n",
      "         0.0262, -0.0212,  0.0893,  0.1119,  0.1038,  0.0832, -0.0957, -0.1075,\n",
      "         0.0162,  0.0815, -0.0745,  0.0441,  0.1057,  0.0553, -0.0804,  0.0671,\n",
      "         0.0216,  0.1103,  0.0881,  0.0710,  0.1177, -0.0009,  0.0426, -0.0945,\n",
      "         0.0842, -0.1116, -0.0277, -0.0036,  0.0220, -0.0892,  0.0697,  0.0696,\n",
      "         0.0321, -0.0695, -0.0828, -0.1000, -0.0999, -0.0858, -0.0252,  0.0213,\n",
      "         0.1160,  0.0404, -0.0949, -0.0761,  0.1126, -0.0316],\n",
      "       requires_grad=True))\n",
      "('input_layer.weight', Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119],\n",
      "        [ 0.2710, -0.5435,  0.3462],\n",
      "        [-0.1188,  0.2937,  0.0803],\n",
      "        [-0.0707,  0.1601,  0.0285],\n",
      "        [ 0.2109, -0.2250, -0.0421],\n",
      "        [-0.0520,  0.0837, -0.0023],\n",
      "        [ 0.5047,  0.1797, -0.2150],\n",
      "        [-0.3487, -0.0968, -0.2490],\n",
      "        [-0.1850,  0.0276,  0.3442],\n",
      "        [ 0.3138, -0.5644,  0.3579],\n",
      "        [ 0.1613,  0.5476,  0.3811],\n",
      "        [-0.5260, -0.5489, -0.2785],\n",
      "        [ 0.5070, -0.0962,  0.2471],\n",
      "        [-0.2683,  0.5665, -0.2443],\n",
      "        [ 0.4330,  0.0068, -0.3042],\n",
      "        [ 0.2968, -0.3065,  0.1698],\n",
      "        [-0.1667, -0.0633, -0.5551],\n",
      "        [-0.2753,  0.3133, -0.1403],\n",
      "        [ 0.5751,  0.4628, -0.0270],\n",
      "        [-0.3854,  0.3516,  0.1792],\n",
      "        [-0.3732,  0.3750,  0.3505],\n",
      "        [ 0.5120, -0.3236, -0.0950],\n",
      "        [-0.0112,  0.0843, -0.4382],\n",
      "        [-0.4097,  0.3141, -0.1354],\n",
      "        [ 0.2820,  0.0329,  0.1896],\n",
      "        [ 0.1270,  0.2099,  0.2862],\n",
      "        [-0.5347,  0.2906, -0.4059],\n",
      "        [-0.4356,  0.0351, -0.0984],\n",
      "        [ 0.3391, -0.3344, -0.5133],\n",
      "        [ 0.4202, -0.0856,  0.3247],\n",
      "        [ 0.1856, -0.4329,  0.1160],\n",
      "        [ 0.1387, -0.3866, -0.2739],\n",
      "        [ 0.1969,  0.1034, -0.2456],\n",
      "        [-0.1748,  0.5288, -0.1068],\n",
      "        [ 0.3255,  0.2500, -0.3732],\n",
      "        [-0.4910,  0.5542,  0.0301],\n",
      "        [ 0.3957,  0.1196,  0.1857],\n",
      "        [ 0.4313,  0.5475, -0.3831],\n",
      "        [ 0.0722,  0.4309,  0.4183],\n",
      "        [ 0.3587, -0.4178, -0.4158],\n",
      "        [-0.3492,  0.0725,  0.5754],\n",
      "        [-0.3647,  0.3077, -0.3196],\n",
      "        [-0.5428, -0.1227,  0.3327],\n",
      "        [ 0.5360, -0.3586,  0.1253],\n",
      "        [ 0.4982,  0.3826,  0.3598],\n",
      "        [ 0.4103,  0.3652,  0.1491],\n",
      "        [-0.3948, -0.4848, -0.2646],\n",
      "        [-0.0672, -0.3539,  0.2112],\n",
      "        [ 0.1787, -0.1307,  0.2219],\n",
      "        [ 0.1866,  0.3525,  0.3888],\n",
      "        [-0.1955,  0.5641, -0.0667],\n",
      "        [-0.0198, -0.5449, -0.3716],\n",
      "        [-0.3373, -0.2469,  0.4105],\n",
      "        [-0.1887, -0.4314,  0.2221],\n",
      "        [ 0.1848,  0.3739, -0.2988],\n",
      "        [ 0.1252, -0.2102, -0.1297],\n",
      "        [-0.4601, -0.2631, -0.1768],\n",
      "        [ 0.2469,  0.1055,  0.1426],\n",
      "        [ 0.5763,  0.5627,  0.3938],\n",
      "        [ 0.0184, -0.3994,  0.4512],\n",
      "        [-0.1444, -0.0467, -0.4974],\n",
      "        [-0.1140, -0.3724,  0.5305],\n",
      "        [-0.4991, -0.4500, -0.0196],\n",
      "        [-0.3122,  0.2066, -0.2222],\n",
      "        [-0.2712,  0.0327,  0.4179],\n",
      "        [-0.4061,  0.2711,  0.3709],\n",
      "        [ 0.5648, -0.4041,  0.1398],\n",
      "        [-0.4269,  0.4929, -0.2240],\n",
      "        [ 0.3478,  0.0172, -0.0450],\n",
      "        [-0.0184,  0.0981,  0.2722],\n",
      "        [ 0.0926,  0.1761, -0.5193],\n",
      "        [ 0.4206,  0.5034,  0.4772],\n",
      "        [ 0.4268, -0.4166, -0.2140],\n",
      "        [ 0.5091, -0.4397,  0.5238],\n",
      "        [-0.4541, -0.4067,  0.2823],\n",
      "        [-0.4148, -0.1323,  0.4200],\n",
      "        [ 0.4573,  0.5460, -0.1172],\n",
      "        [-0.4488,  0.5685, -0.1230],\n",
      "        [-0.2375,  0.1407, -0.4038],\n",
      "        [ 0.3795,  0.3618, -0.4581],\n",
      "        [-0.4742, -0.0506,  0.2425],\n",
      "        [-0.0167, -0.2928,  0.0132],\n",
      "        [-0.5427, -0.4080, -0.3843],\n",
      "        [ 0.4755,  0.5089, -0.1961],\n",
      "        [ 0.0259,  0.2575,  0.0692],\n",
      "        [-0.2891,  0.3330,  0.3549],\n",
      "        [-0.0335, -0.0711,  0.5247],\n",
      "        [ 0.5047, -0.3273,  0.5649],\n",
      "        [ 0.1429, -0.3835,  0.3160],\n",
      "        [-0.4310,  0.5334, -0.3712],\n",
      "        [ 0.1633,  0.1758,  0.1373],\n",
      "        [ 0.4789, -0.2398, -0.2437],\n",
      "        [-0.5003, -0.0237, -0.2735],\n",
      "        [ 0.0231, -0.1184,  0.1916],\n",
      "        [ 0.4995,  0.1704, -0.1860],\n",
      "        [-0.2833, -0.5035,  0.4858],\n",
      "        [-0.1094,  0.1165,  0.0214],\n",
      "        [-0.3679,  0.3252,  0.3236],\n",
      "        [ 0.0291, -0.3281, -0.2453],\n",
      "        [ 0.0080, -0.3299, -0.3221]], requires_grad=True))\n",
      "('input_layer.bias', Parameter containing:\n",
      "tensor([-0.0861, -0.1662,  0.1417, -0.1518, -0.0765, -0.2204, -0.5276,  0.5035,\n",
      "         0.1101,  0.5163, -0.0930, -0.1451, -0.5428, -0.2192,  0.3786, -0.5655,\n",
      "        -0.2019,  0.3518,  0.3328,  0.2132, -0.1143, -0.0437, -0.1096, -0.2128,\n",
      "         0.2409, -0.1966, -0.3243,  0.1310, -0.4173, -0.1232,  0.2950,  0.2158,\n",
      "         0.4915, -0.3938,  0.0050, -0.1705, -0.3229, -0.3248,  0.1455,  0.0604,\n",
      "        -0.2947, -0.4070, -0.4784,  0.5658, -0.0984, -0.1335, -0.2123, -0.2888,\n",
      "        -0.5280, -0.3389,  0.3529,  0.1265, -0.2037, -0.2182,  0.3688,  0.4162,\n",
      "         0.5562,  0.1684,  0.2789, -0.0464, -0.3375, -0.5656,  0.3517, -0.0839,\n",
      "         0.2372,  0.0284, -0.5389, -0.2832,  0.1624, -0.1169, -0.5575,  0.2025,\n",
      "         0.0221, -0.1855,  0.2332,  0.1219, -0.2129,  0.5187,  0.2896,  0.0725,\n",
      "        -0.1109, -0.4890,  0.4458, -0.1490, -0.3731,  0.3702,  0.2050,  0.1429,\n",
      "        -0.1145, -0.1802,  0.0980,  0.4468,  0.0827, -0.2730,  0.5735,  0.0123,\n",
      "         0.4564,  0.1407,  0.5175,  0.3331], requires_grad=True))\n",
      "('output_layer.weight', Parameter containing:\n",
      "tensor([[ 0.0011, -0.0327, -0.0951, -0.0137,  0.0140, -0.0087, -0.0983, -0.0923,\n",
      "         -0.0114, -0.0232, -0.0912, -0.0175,  0.0130,  0.0665, -0.0957,  0.0740,\n",
      "         -0.0860,  0.0405,  0.0557,  0.0186, -0.1120, -0.0530, -0.0537, -0.0017,\n",
      "          0.0750,  0.0549,  0.0434,  0.0953,  0.0736,  0.1160,  0.0515, -0.0508,\n",
      "          0.0070, -0.0980, -0.0769, -0.0913,  0.0828, -0.0365,  0.1093, -0.0401,\n",
      "         -0.1017, -0.0505,  0.1056,  0.0969, -0.0948, -0.1015,  0.0968,  0.0299,\n",
      "         -0.0010, -0.1166, -0.0098, -0.0383,  0.0122,  0.0515, -0.0033,  0.0244,\n",
      "          0.0837, -0.0235,  0.0742, -0.0572, -0.0538,  0.0466,  0.0584,  0.1105,\n",
      "         -0.0841, -0.0690,  0.1032,  0.0013,  0.0283, -0.0345]],\n",
      "       requires_grad=True))\n",
      "('output_layer.bias', Parameter containing:\n",
      "tensor([0.0600], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# 모델 파라미터 즉, w와 b 확인\n",
    "for m in m1.named_parameters(): print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling forward()\n",
      "tensor([[0.0325],\n",
      "        [0.0310]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행 ==> 모델인스턴스명(데이터)\n",
    "# 임의의 데이터\n",
    "dataTS=torch.FloatTensor([[1,3,5], [2,4,6]])  # 입력층 4개라했으니까 4개 넣어줘야함함\n",
    "targetTS=torch.FloatTensor([[4],[5]])\n",
    "\n",
    "# 학습\n",
    "pre_y = m1(dataTS)\n",
    "\n",
    "print(pre_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
